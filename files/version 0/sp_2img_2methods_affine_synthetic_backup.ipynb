{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import FundamentalMatrixTransform, AffineTransform\n",
    "# Suppress the specific warning\n",
    "import warnings\n",
    "import csv\n",
    "import sys\n",
    "from IPython.utils.capture import capture_output\n",
    "from datetime import datetime\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3: # pragma: no cover\n",
    "  print('Warning: OpenCV 3 is not installed')\n",
    "\n",
    "# Jet colormap for visualization.\n",
    "myjet = np.array([[0.        , 0.        , 0.5       ],\n",
    "                  [0.        , 0.        , 0.99910873],\n",
    "                  [0.        , 0.37843137, 1.        ],\n",
    "                  [0.        , 0.83333333, 1.        ],\n",
    "                  [0.30044276, 1.        , 0.66729918],\n",
    "                  [0.66729918, 1.        , 0.30044276],\n",
    "                  [1.        , 0.90123457, 0.        ],\n",
    "                  [1.        , 0.48002905, 0.        ],\n",
    "                  [0.99910873, 0.07334786, 0.        ],\n",
    "                  [0.5       , 0.        , 0.        ]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperPointNet(torch.nn.Module):\n",
    "  \"\"\" Pytorch definition of SuperPoint Network. \"\"\"\n",
    "  def __init__(self):\n",
    "    super(SuperPointNet, self).__init__()\n",
    "    self.relu = torch.nn.ReLU(inplace=True)\n",
    "    self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    c1, c2, c3, c4, c5, d1 = 64, 64, 128, 128, 256, 256\n",
    "    # Shared Encoder.\n",
    "    self.conv1a = torch.nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv1b = torch.nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2a = torch.nn.Conv2d(c1, c2, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2b = torch.nn.Conv2d(c2, c2, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3a = torch.nn.Conv2d(c2, c3, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3b = torch.nn.Conv2d(c3, c3, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv4a = torch.nn.Conv2d(c3, c4, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv4b = torch.nn.Conv2d(c4, c4, kernel_size=3, stride=1, padding=1)\n",
    "    # Detector Head.\n",
    "    self.convPa = torch.nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "    self.convPb = torch.nn.Conv2d(c5, 65, kernel_size=1, stride=1, padding=0)\n",
    "    # Descriptor Head.\n",
    "    self.convDa = torch.nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "    self.convDb = torch.nn.Conv2d(c5, d1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" Forward pass that jointly computes unprocessed point and descriptor\n",
    "    tensors.\n",
    "    Input\n",
    "      x: Image pytorch tensor shaped N x 1 x H x W.\n",
    "    Output\n",
    "      semi: Output point pytorch tensor shaped N x 65 x H/8 x W/8.\n",
    "      desc: Output descriptor pytorch tensor shaped N x 256 x H/8 x W/8.\n",
    "    \"\"\"\n",
    "    # Shared Encoder.\n",
    "    x = self.relu(self.conv1a(x))\n",
    "    x = self.relu(self.conv1b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv2a(x))\n",
    "    x = self.relu(self.conv2b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv3a(x))\n",
    "    x = self.relu(self.conv3b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv4a(x))\n",
    "    x = self.relu(self.conv4b(x))\n",
    "    # Detector Head.\n",
    "    cPa = self.relu(self.convPa(x))\n",
    "    semi = self.convPb(cPa)\n",
    "    # Descriptor Head.\n",
    "    cDa = self.relu(self.convDa(x))\n",
    "    desc = self.convDb(cDa)\n",
    "    dn = torch.norm(desc, p=2, dim=1) # Compute the norm.\n",
    "    desc = desc.div(torch.unsqueeze(dn, 1)) # Divide by norm to normalize.\n",
    "    return semi, desc\n",
    "\n",
    "\n",
    "class SuperPointFrontend(object):\n",
    "  \"\"\" Wrapper around pytorch net to help with pre and post image processing. \"\"\"\n",
    "  def __init__(self, weights_path, nms_dist, conf_thresh, nn_thresh,\n",
    "               cuda=False):\n",
    "    self.name = 'SuperPoint'\n",
    "    self.cuda = cuda\n",
    "    self.nms_dist = nms_dist\n",
    "    self.conf_thresh = conf_thresh\n",
    "    self.nn_thresh = nn_thresh # L2 descriptor distance for good match.\n",
    "    self.cell = 8 # Size of each output cell. Keep this fixed.\n",
    "    self.border_remove = 4 # Remove points this close to the border.\n",
    "\n",
    "    # Load the network in inference mode.\n",
    "    self.net = SuperPointNet()\n",
    "    if cuda:\n",
    "      # Train on GPU, deploy on GPU.\n",
    "      self.net.load_state_dict(torch.load(weights_path))\n",
    "      self.net = self.net.cuda()\n",
    "    else:\n",
    "      # Train on GPU, deploy on CPU.\n",
    "      self.net.load_state_dict(torch.load(weights_path,\n",
    "                               map_location=lambda storage, loc: storage))\n",
    "    self.net.eval()\n",
    "\n",
    "  def nms_fast(self, in_corners, H, W, dist_thresh):\n",
    "    \"\"\"\n",
    "    Run a faster approximate Non-Max-Suppression on numpy corners shaped:\n",
    "      3xN [x_i,y_i,conf_i]^T\n",
    "  \n",
    "    Algo summary: Create a grid sized HxW. Assign each corner location a 1, rest\n",
    "    are zeros. Iterate through all the 1's and convert them either to -1 or 0.\n",
    "    Suppress points by setting nearby values to 0.\n",
    "  \n",
    "    Grid Value Legend:\n",
    "    -1 : Kept.\n",
    "     0 : Empty or suppressed.\n",
    "     1 : To be processed (converted to either kept or supressed).\n",
    "  \n",
    "    NOTE: The NMS first rounds points to integers, so NMS distance might not\n",
    "    be exactly dist_thresh. It also assumes points are within image boundaries.\n",
    "  \n",
    "    Inputs\n",
    "      in_corners - 3xN numpy array with corners [x_i, y_i, confidence_i]^T.\n",
    "      H - Image height.\n",
    "      W - Image width.\n",
    "      dist_thresh - Distance to suppress, measured as an infinty norm distance.\n",
    "    Returns\n",
    "      nmsed_corners - 3xN numpy matrix with surviving corners.\n",
    "      nmsed_inds - N length numpy vector with surviving corner indices.\n",
    "    \"\"\"\n",
    "    grid = np.zeros((H, W)).astype(int) # Track NMS data.\n",
    "    inds = np.zeros((H, W)).astype(int) # Store indices of points.\n",
    "    # Sort by confidence and round to nearest int.\n",
    "    inds1 = np.argsort(-in_corners[2,:])\n",
    "    corners = in_corners[:,inds1]\n",
    "    rcorners = corners[:2,:].round().astype(int) # Rounded corners.\n",
    "    # Check for edge case of 0 or 1 corners.\n",
    "    if rcorners.shape[1] == 0:\n",
    "      return np.zeros((3,0)).astype(int), np.zeros(0).astype(int)\n",
    "    if rcorners.shape[1] == 1:\n",
    "      out = np.vstack((rcorners, in_corners[2])).reshape(3,1)\n",
    "      return out, np.zeros((1)).astype(int)\n",
    "    # Initialize the grid.\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "      grid[rcorners[1,i], rcorners[0,i]] = 1\n",
    "      inds[rcorners[1,i], rcorners[0,i]] = i\n",
    "    # Pad the border of the grid, so that we can NMS points near the border.\n",
    "    pad = dist_thresh\n",
    "    grid = np.pad(grid, ((pad,pad), (pad,pad)), mode='constant')\n",
    "    # Iterate through points, highest to lowest conf, suppress neighborhood.\n",
    "    count = 0\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "      # Account for top and left padding.\n",
    "      pt = (rc[0]+pad, rc[1]+pad)\n",
    "      if grid[pt[1], pt[0]] == 1: # If not yet suppressed.\n",
    "        grid[pt[1]-pad:pt[1]+pad+1, pt[0]-pad:pt[0]+pad+1] = 0\n",
    "        grid[pt[1], pt[0]] = -1\n",
    "        count += 1\n",
    "    # Get all surviving -1's and return sorted array of remaining corners.\n",
    "    keepy, keepx = np.where(grid==-1)\n",
    "    keepy, keepx = keepy - pad, keepx - pad\n",
    "    inds_keep = inds[keepy, keepx]\n",
    "    out = corners[:, inds_keep]\n",
    "    values = out[-1, :]\n",
    "    inds2 = np.argsort(-values)\n",
    "    out = out[:, inds2]\n",
    "    out_inds = inds1[inds_keep[inds2]]\n",
    "    return out, out_inds\n",
    "\n",
    "  def run(self, img):\n",
    "    \"\"\" Process a numpy image to extract points and descriptors.\n",
    "    Input\n",
    "      img - HxW numpy float32 input image in range [0,1].\n",
    "    Output\n",
    "      corners - 3xN numpy array with corners [x_i, y_i, confidence_i]^T.\n",
    "      desc - 256xN numpy array of corresponding unit normalized descriptors.\n",
    "      heatmap - HxW numpy heatmap in range [0,1] of point confidences.\n",
    "      \"\"\"\n",
    "    assert img.ndim == 2, 'Image must be grayscale.'\n",
    "    assert img.dtype == np.float32, 'Image must be float32.'\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    inp = img.copy()\n",
    "    inp = (inp.reshape(1, H, W))\n",
    "    inp = torch.from_numpy(inp)\n",
    "    inp = torch.autograd.Variable(inp).view(1, 1, H, W)\n",
    "    if self.cuda:\n",
    "      inp = inp.cuda()\n",
    "    # Forward pass of network.\n",
    "    outs = self.net.forward(inp)\n",
    "    semi, coarse_desc = outs[0], outs[1]\n",
    "    # Convert pytorch -> numpy.\n",
    "    semi = semi.data.cpu().numpy().squeeze()\n",
    "    # --- Process points.\n",
    "    dense = np.exp(semi) # Softmax.\n",
    "    dense = dense / (np.sum(dense, axis=0)+.00001) # Should sum to 1.\n",
    "    # Remove dustbin.\n",
    "    nodust = dense[:-1, :, :]\n",
    "    # Reshape to get full resolution heatmap.\n",
    "    Hc = int(H / self.cell)\n",
    "    Wc = int(W / self.cell)\n",
    "    nodust = nodust.transpose(1, 2, 0)\n",
    "    heatmap = np.reshape(nodust, [Hc, Wc, self.cell, self.cell])\n",
    "    heatmap = np.transpose(heatmap, [0, 2, 1, 3])\n",
    "    heatmap = np.reshape(heatmap, [Hc*self.cell, Wc*self.cell])\n",
    "    xs, ys = np.where(heatmap >= self.conf_thresh) # Confidence threshold.\n",
    "    if len(xs) == 0:\n",
    "      return np.zeros((3, 0)), None, None\n",
    "    pts = np.zeros((3, len(xs))) # Populate point data sized 3xN.\n",
    "    pts[0, :] = ys\n",
    "    pts[1, :] = xs\n",
    "    pts[2, :] = heatmap[xs, ys]\n",
    "    pts, _ = self.nms_fast(pts, H, W, dist_thresh=self.nms_dist) # Apply NMS.\n",
    "    inds = np.argsort(pts[2,:])\n",
    "    pts = pts[:,inds[::-1]] # Sort by confidence.\n",
    "    # Remove points along border.\n",
    "    bord = self.border_remove\n",
    "    toremoveW = np.logical_or(pts[0, :] < bord, pts[0, :] >= (W-bord))\n",
    "    toremoveH = np.logical_or(pts[1, :] < bord, pts[1, :] >= (H-bord))\n",
    "    toremove = np.logical_or(toremoveW, toremoveH)\n",
    "    pts = pts[:, ~toremove]\n",
    "    # --- Process descriptor.\n",
    "    D = coarse_desc.shape[1]\n",
    "    if pts.shape[1] == 0:\n",
    "      desc = np.zeros((D, 0))\n",
    "    else:\n",
    "      # Interpolate into descriptor map using 2D point locations.\n",
    "      samp_pts = torch.from_numpy(pts[:2, :].copy())\n",
    "      samp_pts[0, :] = (samp_pts[0, :] / (float(W)/2.)) - 1.\n",
    "      samp_pts[1, :] = (samp_pts[1, :] / (float(H)/2.)) - 1.\n",
    "      samp_pts = samp_pts.transpose(0, 1).contiguous()\n",
    "      samp_pts = samp_pts.view(1, 1, -1, 2)\n",
    "      samp_pts = samp_pts.float()\n",
    "      if self.cuda:\n",
    "        samp_pts = samp_pts.cuda()\n",
    "      desc = torch.nn.functional.grid_sample(coarse_desc, samp_pts)\n",
    "      desc = desc.data.cpu().numpy().reshape(D, -1)\n",
    "      desc /= np.linalg.norm(desc, axis=0)[np.newaxis, :]\n",
    "    return pts, desc, heatmap\n",
    "\n",
    "\n",
    "class PointTracker(object):\n",
    "  \"\"\" Class to manage a fixed memory of points and descriptors that enables\n",
    "  sparse optical flow point tracking.\n",
    "\n",
    "  Internally, the tracker stores a 'tracks' matrix sized M x (2+L), of M\n",
    "  tracks with maximum length L, where each row corresponds to:\n",
    "  row_m = [track_id_m, avg_desc_score_m, point_id_0_m, ..., point_id_L-1_m].\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, max_length, nn_thresh):\n",
    "    if max_length < 2:\n",
    "      raise ValueError('max_length must be greater than or equal to 2.')\n",
    "    self.maxl = max_length\n",
    "    self.nn_thresh = nn_thresh\n",
    "    self.all_pts = []\n",
    "    for n in range(self.maxl):\n",
    "      self.all_pts.append(np.zeros((2, 0)))\n",
    "    self.last_desc = None\n",
    "    self.tracks = np.zeros((0, self.maxl+2))\n",
    "    self.track_count = 0\n",
    "    self.max_score = 9999\n",
    "\n",
    "  def nn_match_two_way(self, desc1, desc2, nn_thresh):\n",
    "    \"\"\"\n",
    "    Performs two-way nearest neighbor matching of two sets of descriptors, such\n",
    "    that the NN match from descriptor A->B must equal the NN match from B->A.\n",
    "\n",
    "    Inputs:\n",
    "      desc1 - NxM numpy matrix of N corresponding M-dimensional descriptors.\n",
    "      desc2 - NxM numpy matrix of N corresponding M-dimensional descriptors.\n",
    "      nn_thresh - Optional descriptor distance below which is a good match.\n",
    "\n",
    "    Returns:\n",
    "      matches - 3xL numpy array, of L matches, where L <= N and each column i is\n",
    "                a match of two descriptors, d_i in image 1 and d_j' in image 2:\n",
    "                [d_i index, d_j' index, match_score]^T\n",
    "    \"\"\"\n",
    "    assert desc1.shape[0] == desc2.shape[0]\n",
    "    if desc1.shape[1] == 0 or desc2.shape[1] == 0:\n",
    "      return np.zeros((3, 0))\n",
    "    if nn_thresh < 0.0:\n",
    "      raise ValueError('\\'nn_thresh\\' should be non-negative')\n",
    "    # Compute L2 distance. Easy since vectors are unit normalized.\n",
    "    dmat = np.dot(desc1.T, desc2)\n",
    "    dmat = np.sqrt(2-2*np.clip(dmat, -1, 1))\n",
    "    # Get NN indices and scores.\n",
    "    idx = np.argmin(dmat, axis=1)\n",
    "    scores = dmat[np.arange(dmat.shape[0]), idx]\n",
    "    # Threshold the NN matches.  <<< ======================= this threshold is not good\n",
    "    keep = scores < nn_thresh\n",
    "    # Check if nearest neighbor goes both directions and keep those.\n",
    "    idx2 = np.argmin(dmat, axis=0)\n",
    "    keep_bi = np.arange(len(idx)) == idx2[idx]\n",
    "    keep = np.logical_and(keep, keep_bi)\n",
    "    idx = idx[keep]\n",
    "    scores = scores[keep]\n",
    "    # Get the surviving point indices.\n",
    "    m_idx1 = np.arange(desc1.shape[1])[keep]\n",
    "    m_idx2 = idx\n",
    "    # Populate the final 3xN match data structure.\n",
    "    matches = np.zeros((3, int(keep.sum())))\n",
    "    matches[0, :] = m_idx1\n",
    "    matches[1, :] = m_idx2\n",
    "    matches[2, :] = scores\n",
    "    return matches\n",
    "  \n",
    "  def nn_match_two_way_with_ransac(self, points1, points2, matches, max_reproj_error=5.0):  # <<=========================================== TODO Here is the RANSAC\n",
    "    '''find matching points between two images using RANSAC'''\n",
    "    \n",
    "    # estimate affine transform model using all coordinates\n",
    "    model = AffineTransform()\n",
    "    model.estimate(points1, points2)\n",
    "\n",
    "    # try min_samples = 3, if fail, try min_samples = 2\n",
    "    try:\n",
    "      # Find the best fundamental matrix using RANSAC\n",
    "      best_model, best_inliers = ransac((points1, points2),\n",
    "                                        AffineTransform, min_samples=3,\n",
    "                                        residual_threshold=max_reproj_error, max_trials=100)\n",
    "    except:\n",
    "      try:\n",
    "        best_model, best_inliers = ransac((points1, points2),\n",
    "                                          AffineTransform, min_samples=2,\n",
    "                                          residual_threshold=max_reproj_error, max_trials=100)\n",
    "        \n",
    "      except:\n",
    "        # if ransac failed, return an array of TRUE with the original matches shape\n",
    "        print('ransac failed')\n",
    "        print('matches shape: ', matches.shape)\n",
    "        return np.ones((matches.shape[1])).astype(bool)\n",
    "    \n",
    "    # the inliners are the matching points\n",
    "    matches = np.array(best_inliers).T\n",
    "\n",
    "    # print(f'matches: {matches.shape}')\n",
    "    return matches\n",
    "\n",
    "  def get_offsets(self):\n",
    "    \"\"\" Iterate through list of points and accumulate an offset value. Used to\n",
    "    index the global point IDs into the list of points.\n",
    "\n",
    "    Returns\n",
    "      offsets - N length array with integer offset locations.\n",
    "    \"\"\"\n",
    "    # Compute id offsets.\n",
    "    offsets = []\n",
    "    offsets.append(0)\n",
    "    for i in range(len(self.all_pts)-1): # Skip last camera size, not needed.\n",
    "      offsets.append(self.all_pts[i].shape[1])\n",
    "    offsets = np.array(offsets)\n",
    "    offsets = np.cumsum(offsets)\n",
    "    return offsets\n",
    "\n",
    "  def update(self, pts, desc):\n",
    "    \"\"\" Add a new set of point and descriptor observations to the tracker.\n",
    "\n",
    "    Inputs\n",
    "      pts - 3xN numpy array of 2D point observations.\n",
    "      desc - DxN numpy array of corresponding D dimensional descriptors.\n",
    "    \"\"\"\n",
    "    if pts is None or desc is None:\n",
    "      print('PointTracker: Warning, no points were added to tracker.')\n",
    "      return\n",
    "    assert pts.shape[1] == desc.shape[1]\n",
    "    # Initialize last_desc.\n",
    "    if self.last_desc is None:\n",
    "      self.last_desc = np.zeros((desc.shape[0], 0))\n",
    "    # Remove oldest points, store its size to update ids later.\n",
    "    remove_size = self.all_pts[0].shape[1]\n",
    "    self.all_pts.pop(0)\n",
    "    self.all_pts.append(pts)\n",
    "    # Remove oldest point in track.\n",
    "    self.tracks = np.delete(self.tracks, 2, axis=1)\n",
    "    # Update track offsets.\n",
    "    for i in range(2, self.tracks.shape[1]):\n",
    "      self.tracks[:, i] -= remove_size\n",
    "    self.tracks[:, 2:][self.tracks[:, 2:] < -1] = -1\n",
    "    offsets = self.get_offsets()\n",
    "    # Add a new -1 column.\n",
    "    self.tracks = np.hstack((self.tracks, -1*np.ones((self.tracks.shape[0], 1))))\n",
    "    # Try to append to existing tracks.\n",
    "    matched = np.zeros((pts.shape[1])).astype(bool)\n",
    "    matches = self.nn_match_two_way(self.last_desc, desc, self.nn_thresh)\n",
    "    for match in matches.T:\n",
    "      # Add a new point to it's matched track.\n",
    "      id1 = int(match[0]) + offsets[-2]\n",
    "      id2 = int(match[1]) + offsets[-1]\n",
    "      found = np.argwhere(self.tracks[:, -2] == id1)\n",
    "      if found.shape[0] > 0:\n",
    "        matched[int(match[1])] = True\n",
    "        row = int(found)\n",
    "        self.tracks[row, -1] = id2\n",
    "        if self.tracks[row, 1] == self.max_score:\n",
    "          # Initialize track score.\n",
    "          self.tracks[row, 1] = match[2]\n",
    "        else:\n",
    "          # Update track score with running average.\n",
    "          # NOTE(dd): this running average can contain scores from old matches\n",
    "          #           not contained in last max_length track points.\n",
    "          track_len = (self.tracks[row, 2:] != -1).sum() - 1.\n",
    "          frac = 1. / float(track_len)\n",
    "          self.tracks[row, 1] = (1.-frac)*self.tracks[row, 1] + frac*match[2]\n",
    "    # Add unmatched tracks.\n",
    "    new_ids = np.arange(pts.shape[1]) + offsets[-1]\n",
    "    new_ids = new_ids[~matched]\n",
    "    new_tracks = -1*np.ones((new_ids.shape[0], self.maxl + 2))\n",
    "    new_tracks[:, -1] = new_ids\n",
    "    new_num = new_ids.shape[0]\n",
    "    new_trackids = self.track_count + np.arange(new_num)\n",
    "    new_tracks[:, 0] = new_trackids\n",
    "    new_tracks[:, 1] = self.max_score*np.ones(new_ids.shape[0])\n",
    "    self.tracks = np.vstack((self.tracks, new_tracks))\n",
    "    self.track_count += new_num # Update the track count.\n",
    "    # Remove empty tracks.\n",
    "    keep_rows = np.any(self.tracks[:, 2:] >= 0, axis=1)\n",
    "    self.tracks = self.tracks[keep_rows, :]\n",
    "    # Store the last descriptors.\n",
    "    self.last_desc = desc.copy()\n",
    "    return\n",
    "\n",
    "  def get_tracks(self, min_length):\n",
    "    \"\"\" Retrieve point tracks of a given minimum length.\n",
    "    Input\n",
    "      min_length - integer >= 1 with minimum track length\n",
    "    Output\n",
    "      returned_tracks - M x (2+L) sized matrix storing track indices, where\n",
    "        M is the number of tracks and L is the maximum track length.\n",
    "    \"\"\"\n",
    "    if min_length < 1:\n",
    "      raise ValueError('\\'min_length\\' too small.')\n",
    "    valid = np.ones((self.tracks.shape[0])).astype(bool)\n",
    "    good_len = np.sum(self.tracks[:, 2:] != -1, axis=1) >= min_length\n",
    "    # Remove tracks which do not have an observation in most recent frame.\n",
    "    not_headless = (self.tracks[:, -1] != -1)\n",
    "    keepers = np.logical_and.reduce((valid, good_len, not_headless))\n",
    "    returned_tracks = self.tracks[keepers, :].copy()\n",
    "    return returned_tracks\n",
    "\n",
    "  def draw_tracks(self, out, tracks):\n",
    "    \"\"\" Visualize tracks all overlayed on a single image.\n",
    "    Inputs\n",
    "      out - numpy uint8 image sized HxWx3 upon which tracks are overlayed.\n",
    "      tracks - M x (2+L) sized matrix storing track info.\n",
    "    \"\"\"\n",
    "    # Store the number of points per camera.\n",
    "    pts_mem = self.all_pts\n",
    "    N = len(pts_mem) # Number of cameras/images.\n",
    "    # Get offset ids needed to reference into pts_mem.\n",
    "    offsets = self.get_offsets()\n",
    "    # Width of track and point circles to be drawn.\n",
    "    stroke = 1\n",
    "    # Iterate through each track and draw it.\n",
    "    for track in tracks:\n",
    "      clr = myjet[int(np.clip(np.floor(track[1]*10), 0, 9)), :]*255\n",
    "      for i in range(N-1):\n",
    "        if track[i+2] == -1 or track[i+3] == -1:\n",
    "          continue\n",
    "        offset1 = offsets[i]\n",
    "        offset2 = offsets[i+1]\n",
    "        idx1 = int(track[i+2]-offset1)\n",
    "        idx2 = int(track[i+3]-offset2)\n",
    "        pt1 = pts_mem[i][:2, idx1]\n",
    "        pt2 = pts_mem[i+1][:2, idx2]\n",
    "        p1 = (int(round(pt1[0])), int(round(pt1[1])))\n",
    "        p2 = (int(round(pt2[0])), int(round(pt2[1])))\n",
    "        cv2.line(out, p1, p2, clr, thickness=stroke, lineType=16)\n",
    "        # Draw end points of each track.\n",
    "        if i == N-2:\n",
    "          clr2 = (255, 0, 0)\n",
    "          cv2.circle(out, p2, stroke, clr2, -1, lineType=16)\n",
    "\n",
    "  def read_image(self, impath, img_size):\n",
    "    \"\"\" Read image as grayscale and resize to img_size.\n",
    "    Inputs\n",
    "      impath: Path to input image.\n",
    "      img_size: (W, H) tuple specifying resize size.\n",
    "    Returns\n",
    "      grayim: float32 numpy array sized H x W with values in range [0, 1].\n",
    "    \"\"\"\n",
    "    grayim = cv2.imread(impath, 0)\n",
    "    if grayim is None:\n",
    "      raise Exception('Error reading image %s' % impath)\n",
    "    # Image is resized via opencv.\n",
    "    interp = cv2.INTER_AREA\n",
    "    grayim = cv2.resize(grayim, (img_size[1], img_size[0]), interpolation=interp)\n",
    "    grayim = (grayim.astype('float32') / 255.)\n",
    "    return grayim\n",
    "\n",
    "  def next_frame(self):\n",
    "    \"\"\" Return the next frame, and increment internal counter.\n",
    "    Returns\n",
    "       image: Next H x W image.\n",
    "       status: True or False depending whether image was loaded.\n",
    "    \"\"\"\n",
    "    if self.i == self.maxlen:\n",
    "      return (None, False)\n",
    "    if self.camera:\n",
    "      ret, input_image = self.cap.read()\n",
    "      if ret is False:\n",
    "        print('VideoStreamer: Cannot get image from camera (maybe bad --camid?)')\n",
    "        return (None, False)\n",
    "      if self.video_file:\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.listing[self.i])\n",
    "      input_image = cv2.resize(input_image, (self.sizer[1], self.sizer[0]),\n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "      input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)\n",
    "      input_image = input_image.astype('float')/255.0\n",
    "    else:\n",
    "      image_file = self.listing[self.i]\n",
    "      input_image = self.read_image(image_file, self.sizer)\n",
    "    # Increment internal counter.\n",
    "    self.i = self.i + 1\n",
    "    input_image = input_image.astype('float32')\n",
    "    return (input_image, True)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified code to load and feed 2 images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to overlay points on the image\n",
    "def overlay_points(image, points, color=(0, 255, 0), radius=5):\n",
    "    for point in points.T:\n",
    "        x, y = int(point[0]), int(point[1])\n",
    "        cv2.circle(image, (x, y), radius, color, -1)\n",
    "    return image\n",
    "\n",
    "# Predefined bright colors\n",
    "bright_colors = [\n",
    "    (255, 0, 0),    # Red\n",
    "    (0, 255, 0),    # Green\n",
    "    (0, 0, 255),    # Blue\n",
    "    (255, 255, 0),  # Yellow\n",
    "    (0, 255, 255),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (255, 128, 0),  # Orange\n",
    "    (0, 255, 128),  # Spring Green\n",
    "    (128, 0, 255),  # Purple\n",
    "    (255, 0, 128),  # Rose\n",
    "]\n",
    "\n",
    "# Function to draw lines connecting points from image 1 to image 2 with random bright colors\n",
    "def draw_lines(image1, image2, points1, points2, match, line_thickness=1, opacity=0.5):\n",
    "    # Convert grayscale images to 3-channel with repeated intensity value\n",
    "    if len(image1.shape) == 2:\n",
    "        image1 = cv2.cvtColor(image1, cv2.COLOR_GRAY2BGR)\n",
    "    if len(image2.shape) == 2:\n",
    "        image2 = cv2.cvtColor(image2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    combined_image = np.concatenate((image1, image2), axis=1)\n",
    "    for pt1, pt2, value in zip(points1.T, points2.T, match):\n",
    "        x1, y1 = int(pt1[0]), int(pt1[1])\n",
    "        x2, y2 = int(pt2[0] + image1.shape[1]), int(pt2[1])\n",
    "\n",
    "        # Randomly choose a color from the predefined bright colors\n",
    "        line_color = random.choice(bright_colors)\n",
    "\n",
    "        # Draw the line with the chosen color and specified thickness\n",
    "        cv2.line(combined_image, (x1, y1), (x2, y2), line_color, line_thickness)\n",
    "\n",
    "        # Add lines with the chosen color and opacity on top of the combined image\n",
    "        overlay = combined_image.copy()\n",
    "        cv2.line(overlay, (x1, y1), (x2, y2), line_color, line_thickness)\n",
    "        cv2.addWeighted(overlay, opacity, combined_image, 1 - opacity, 0, combined_image)\n",
    "\n",
    "        # Add value from 'match' array as text around the beginning of the line on the left image\n",
    "        value_text = f\"{value:.2f}\"\n",
    "        text_x = x1 - 50 if x1 > 50 else x1 + 10\n",
    "        text_y = y1 + 15\n",
    "        cv2.putText(combined_image, value_text, (text_x, text_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, line_color, 1)\n",
    "\n",
    "    return combined_image\n",
    "\n",
    "# function to draw lines connecting original points and the transformed points with random bright colors on one image\n",
    "def draw_lines_one_image(image, points1, points2, line_thickness=1, opacity=0.5, line_color=None):\n",
    "    # Convert grayscale images to 3-channel with repeated intensity value\n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    for pt1, pt2 in zip(points1.T, points2.T):\n",
    "        x1, y1 = int(pt1[0]), int(pt1[1])\n",
    "        x2, y2 = int(pt2[0]), int(pt2[1])\n",
    "\n",
    "        # If not specified, randomly choose a color from the predefined bright colors\n",
    "        if line_color is None:\n",
    "            line_color = random.choice(bright_colors)\n",
    "\n",
    "        # Draw the line with the chosen color and specified thickness\n",
    "        cv2.line(image, (x1, y1), (x2, y2), line_color, line_thickness)\n",
    "\n",
    "        # Add lines with the chosen color and opacity on top of the combined image\n",
    "        overlay = image.copy()\n",
    "        cv2.line(overlay, (x1, y1), (x2, y2), line_color, line_thickness)\n",
    "        cv2.addWeighted(overlay, opacity, image, 1 - opacity, 0, image)\n",
    "\n",
    "        # show the distance between the two points (MSE)\n",
    "        '''value = np.mean((pt1 - pt2)**2)\n",
    "        value_text = f\"{value:.2f}\"\n",
    "        text_x = x1 - 50 if x1 > 50 else x1 + 10\n",
    "        text_y = y1 + 15\n",
    "        cv2.putText(image, value_text, (text_x, text_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, line_color, 1)'''\n",
    "        \n",
    "    return image\n",
    "\n",
    "# Function to create a heatmap from keypoints\n",
    "def create_heatmap(image, keypoints, size=5, sigma=1):\n",
    "    heatmap = np.zeros_like(image)\n",
    "    for kp in keypoints.T:\n",
    "        x, y = int(kp[0]), int(kp[1])\n",
    "        heatmap[y - size:y + size + 1, x - size:x + size + 1] = 1\n",
    "    heatmap = cv2.GaussianBlur(heatmap, (0, 0), sigmaX=sigma, sigmaY=sigma)\n",
    "    return heatmap\n",
    "\n",
    "# Function to create a checkerboard image of image1 and image2 with 10x10 squares\n",
    "def create_checkerboard(image1, image2):\n",
    "    checkerboard = np.zeros((512, 512))\n",
    "    # Create the checkerboard pattern\n",
    "    width = 512\n",
    "\n",
    "    tile_size = width / 10\n",
    "    for i in range(checkerboard.shape[0]):\n",
    "        for j in range(checkerboard.shape[1]):\n",
    "            num = (math.floor(i / tile_size) + math.floor(j / tile_size)) % 2\n",
    "            if num == 0:\n",
    "                checkerboard[i, j] = image1[i, j]\n",
    "            else:\n",
    "                checkerboard[i, j] = image2[i, j]\n",
    "\n",
    "    return checkerboard\n",
    "\n",
    "def check_location(points):\n",
    "    # check if the location of the points in array is within the image\n",
    "    # return the number of points that are outside the image\n",
    "    # points is a 2D array of shape (2, N)\n",
    "    # where N is the number of points\n",
    "    # the first row is the x coordinate\n",
    "    # the second row is the y coordinate\n",
    "    # the image size is 512 x 512\n",
    "    # the points are assumed to be in the range of 0 to 511\n",
    "    # return the number of points that are outside the image\n",
    "    number_outside = np.sum((points < 0) | (points > 511))\n",
    "    \n",
    "    # print if there are points outside the image\n",
    "    if number_outside > 0:\n",
    "        print(f'Number of points outside the image: {number_outside}')\n",
    "    # return number_outside\n",
    "\n",
    "def load_images_from_folder(folder, img_size=(512, 512)):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        grayim = cv2.imread(img_path, 0)\n",
    "        interp = cv2.INTER_AREA\n",
    "        grayim = cv2.resize(grayim, (img_size[1], img_size[0]), interpolation=interp)\n",
    "        image = (grayim.astype('float32') / 255.)\n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "    return images\n",
    "\n",
    "\n",
    "def affine_and_plot(name, dir_name, image1_name, image1, image2, points1, \\\n",
    "    points2, desc1, desc2, affine_GT, plot=True):  \n",
    "    print('Normal')\n",
    "\n",
    "    # Part 1 - points matching\n",
    "    # match the points between the two images\n",
    "    tracker = PointTracker(5, nn_thresh=0.7)\n",
    "    matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=0.7)\n",
    "\n",
    "    # take the elements from points1 and points2 using the matches as indices\n",
    "    matches1 = points1[:2, matches[0, :].astype(int)]\n",
    "    matches2 = points2[:2, matches[1, :].astype(int)]\n",
    "    # print(f'Non-RANSAC: {matches1[:2, :].shape}, {matches2[:2, :].shape}')\n",
    "\n",
    "    # MSE and TRE before transformation\n",
    "    mse_before = np.mean((matches1[:2, :] - matches2[:2, :])**2)\n",
    "    tre_before = np.mean(np.sqrt(np.sum((matches1[:2, :] - matches2[:2, :])**2, axis=0)))\n",
    "\n",
    "    # Part 2 - Affine Transform\n",
    "    # create affine transform matrix from points1 to points2 and apply it to points1\n",
    "    # also from points2 to points1 and apply it to points2\n",
    "    # points1 and points2 are 2D arrays of shape (2, N)\n",
    "    # where N is the number of points\n",
    "    # the first row is the x coordinate\n",
    "    # the second row is the y coordinate\n",
    "    # the image size is 512 x 512\n",
    "\n",
    "    # create affine transform matrix from points1 to points2\n",
    "    # and apply it to points1\n",
    "    affine_transform1 = cv2.estimateAffinePartial2D(matches1[:2, :].T, matches2[:2, :].T)\n",
    "    # print(f'Affine transform matrix from points1 to points2:\\n{affine_transform1[0]}')\n",
    "    matches1_transformed = cv2.transform(matches1.T[None, :, :], affine_transform1[0])\n",
    "    matches1_transformed = matches1_transformed[0].T\n",
    "    mse12 = np.mean((matches1_transformed - matches2[:2, :])**2)\n",
    "    tre12 = np.mean(np.sqrt(np.sum((matches1_transformed - matches2[:2, :])**2, axis=0)))\n",
    "\n",
    "    # create affine transform matrix from points2 to points1\n",
    "    # and apply it to points2\n",
    "    affine_transform2 = cv2.estimateAffinePartial2D(matches2[:2, :].T, matches1[:2, :].T)\n",
    "    matches2_transformed = cv2.transform(matches2.T[None, :, :], affine_transform2[0])\n",
    "    matches2_transformed = matches2_transformed[0].T\n",
    "    mse21 = np.mean((matches2_transformed - matches1[:2, :])**2)\n",
    "    tre21 = np.mean(np.sqrt(np.sum((matches2_transformed - matches1[:2, :])**2, axis=0)))\n",
    "\n",
    "    # transform image 1 and 2 using the affine transform matrix\n",
    "    image1_transformed = cv2.warpAffine(image1, affine_transform1[0], (512, 512))\n",
    "    image2_transformed = cv2.warpAffine(image2, affine_transform2[0], (512, 512))\n",
    "\n",
    "    # calculate the MSE between image1_transformed and image2\n",
    "    mse12_image = np.mean((image1_transformed - image2)**2)\n",
    "    # calculate the MSE between image2_transformed and image1\n",
    "    mse21_image = np.mean((image2_transformed - image1)**2)\n",
    "\n",
    "    # calculate SSIM between image1_transformed and image2\n",
    "    ssim12_image = ssim(image1_transformed, image2)\n",
    "    # calculate SSIM between image2_transformed and image1\n",
    "    ssim21_image = ssim(image2_transformed, image1)\n",
    "\n",
    "    # Part 3 - Plotting\n",
    "    if plot:\n",
    "        # Create a subplot with 2 rows and 2 columns\n",
    "        fig, axes = plt.subplot_mosaic(\"AAEF;BDCG\", figsize=(20, 10))\n",
    "\n",
    "        overlaid1 = overlay_points(image1.copy(), matches1, radius=2)\n",
    "        overlaid2 = overlay_points(image2.copy(), matches2, radius=2)\n",
    "        \n",
    "        # Row 1: Two images side-by-side with overlaid points and lines\n",
    "        # show also MSE and SSIM between the two images\n",
    "        axes[\"A\"].imshow(draw_lines(overlaid1, overlaid2, matches1, matches2, matches[2, :]))\n",
    "        axes[\"A\"].set_title(f\"Pair {image1_name} with matched points. MSE (x100): {100*np.mean((image1 - image2)**2):.4f} SSIM (x10): {10*ssim(image1, image2):.4f}\")\n",
    "        axes[\"A\"].axis('off')\n",
    "        \n",
    "        '''# Row 2: Heatmap for image 1 and image 2\n",
    "        axes[\"B\"].imshow(image1, cmap='gray')\n",
    "        axes[\"B\"].imshow(heatmap1, cmap='hot', alpha=0.8)\n",
    "        axes[\"B\"].set_title(\"Heatmap for Image 1\")\n",
    "        axes[\"B\"].axis('off')\n",
    "\n",
    "        axes[\"C\"].imshow(image2, cmap='gray')\n",
    "        axes[\"C\"].imshow(heatmap2, cmap='hot', alpha=0.8)\n",
    "        axes[\"C\"].set_title(\"Heatmap for Image 2\")\n",
    "        axes[\"C\"].axis('off')'''\n",
    "\n",
    "        axes[\"B\"].imshow(image1_transformed, cmap='gray')\n",
    "        axes[\"B\"].set_title(f\"Image A transformed to B\")\n",
    "        axes[\"B\"].axis('off')\n",
    "\n",
    "        axes[\"C\"].imshow(image2_transformed, cmap='gray')\n",
    "        axes[\"C\"].set_title(f\"Image B transformed to A\")\n",
    "        axes[\"C\"].axis('off')\n",
    "\n",
    "        # New subplot for histograms of 'match_score' array\n",
    "        '''axes[\"D\"].hist(match_score, bins=20, color='blue', alpha=0.7)\n",
    "        axes[\"D\"].set_title(f\"Match Score Histogram, {len(match_score)} matches\")\n",
    "        axes[\"D\"].set_xlabel(\"Value\")\n",
    "        axes[\"D\"].set_ylabel(\"Frequency\")'''\n",
    "\n",
    "        # New subplot for the transformed points\n",
    "        # Blue: from original locations from image 2/1 to the affine-transformed locations\n",
    "        # Red: from affine-transformed locations of points from image 2/1 to \n",
    "        # the locations they supposed to be in image 1/2\n",
    "        img1 = draw_lines_one_image(overlaid1, matches2_transformed, matches2, line_color=(0, 0, 155))\n",
    "        img1 = draw_lines_one_image(img1, matches1, matches2_transformed, line_color=(255, 0, 0))\n",
    "        axes[\"E\"].imshow(img1)\n",
    "        axes[\"E\"].set_title(f\"Image A with points B transformed to A. MSE: {mse21:.4f}\")\n",
    "        axes[\"E\"].axis('off')\n",
    "\n",
    "        img2 = draw_lines_one_image(overlaid2, matches1_transformed, matches1, line_color=(0, 0, 155))\n",
    "        img2 = draw_lines_one_image(img2, matches2, matches1_transformed, line_color=(255, 0, 0))\n",
    "        axes[\"F\"].imshow(img2)\n",
    "        axes[\"F\"].set_title(f\"Image B with points A transformed to B. MSE: {mse12:.4f}\")\n",
    "        axes[\"F\"].axis('off')\n",
    "\n",
    "        # Display the checkerboard image 1 transformed to 2\n",
    "        checkerboard = create_checkerboard(image1_transformed, image2)\n",
    "        axes[\"D\"].imshow(checkerboard, cmap='gray')\n",
    "        axes[\"D\"].set_title(f\"Checkerboard A to B: {mse12_image:.4f}\")\n",
    "        axes[\"D\"].axis('off')\n",
    "\n",
    "        # Display the checkerboard image 2 transformed to 1\n",
    "        checkerboard = create_checkerboard(image2_transformed, image1)\n",
    "        axes[\"G\"].imshow(checkerboard, cmap='gray')\n",
    "        axes[\"G\"].set_title(f\"Checkerboard B to A: {mse21_image:.4f}\")\n",
    "        axes[\"G\"].axis('off')\n",
    "        \n",
    "        plt.tight_layout()  # Adjust the layout to leave space for the histogram\n",
    "        # create a folder named as yyyy-mm-dd_hh in output_images folder      \n",
    "        plt.savefig(os.path.join(dir_name, f\"{image1_name}_{name}.png\"))\n",
    "        \n",
    "        # save images to output folder\n",
    "        '''cv2.imwrite(f\"../Dataset/output_images/transformed_images/{image1_name}_{image2_name}_{name}_1.png\", image1_transformed*255)\n",
    "        cv2.imwrite(f\"../Dataset/output_images/transformed_images/{image1_name}_{image2_name}_{name}_2.png\", image2_transformed*255)'''\n",
    "        plt.close()\n",
    "\n",
    "    return matches1_transformed.shape[-1], mse_before, mse12, mse21, tre_before, tre12, tre21, mse12_image, mse21_image, ssim12_image, ssim21_image\n",
    "\n",
    "def RANSAC_affine_plot(name, dir_name, image1_name, image1, image2, points1, points2, desc1, desc2, affine_GT, plot=True):\n",
    "    print('RANSAC')\n",
    "    # input both matches from RANSAC and non-RANSAC to affine_transform\n",
    "    # because we have to create affine transform matrix from matches_RANSAC\n",
    "    # and apply it to matches1 and matches2\n",
    "    # display them on the image with different colors\n",
    "\n",
    "    # Part 1 - RANSAC\n",
    "    # match the points between the two images\n",
    "    tracker = PointTracker(5, nn_thresh=0.7)\n",
    "    matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=0.7)\n",
    "\n",
    "    # take the elements from points1 and points2 using the matches as indices\n",
    "    matches1 = points1[:, matches[0, :].astype(int)]\n",
    "    matches2 = points2[:, matches[1, :].astype(int)]\n",
    "    # print(f'Before RANSAC: {matches1[:2, :].shape}, {matches2[:2, :].shape}')\n",
    "            \n",
    "    matches_RANSAC = tracker.nn_match_two_way_with_ransac(matches1[:2, :].T, matches2[:2, :].T, matches, max_reproj_error=2)\n",
    "    # print amount of TRUE in matches_RANSAC\n",
    "    # print(f'After RANSAC: {np.sum(matches_RANSAC)}')\n",
    "            \n",
    "    # take elements from matches1 and matches2 where matches_RANSAC is TRUE\n",
    "    matches_RANSAC = matches_RANSAC.reshape(-1)\n",
    "    matches1_RANSAC = matches1[:2, matches_RANSAC]\n",
    "    matches2_RANSAC = matches2[:2, matches_RANSAC]\n",
    "    # print(f'{matches1_RANSAC.shape}, {matches2_RANSAC.shape}')\n",
    "\n",
    "    \"\"\"# take elements from matches1 and matches2 where matches_RANSAC is FALSE\n",
    "    matches1_outliers = matches1[:2, ~matches_RANSAC]\n",
    "    matches2_outliers = matches2[:2, ~matches_RANSAC]\n",
    "    # print(f'outliers: {matches1_outliers.shape}, {matches2_outliers.shape}')\"\"\"\n",
    "\n",
    "    # Part 2 - Affine Transform\n",
    "    # create affine transform matrix from points1 to points2 and apply it to points1\n",
    "    # also from points2 to points1 and apply it to points2\n",
    "    # points1 and points2 are 2D arrays of shape (2, N)\n",
    "    # where N is the number of points\n",
    "    # the first row is the x coordinate\n",
    "    # the second row is the y coordinate\n",
    "    # the image size is 512 x 512\n",
    "\n",
    "    #################################### FIX THIS ####################################\n",
    "    # create affine transform matrix from only matches_RANSAC\n",
    "    # and apply it to all points (RANSAC, non-RANSAC separately to plot in different colors)\n",
    "    affine_transform1 = cv2.estimateAffinePartial2D(matches1_RANSAC.T, matches2_RANSAC.T)\n",
    "    matches1_all_transformed = cv2.transform(matches1.T[None, :, :], affine_transform1[0])\n",
    "    matches1_all_transformed = matches1_all_transformed[0].T\n",
    "\n",
    "    matches1_inliers_transformed = matches1_all_transformed[:2, matches_RANSAC]\n",
    "\n",
    "    # create affine transform matrix from points2 to points1\n",
    "    # and apply it to all points, plot all first, then plot RANSAC over it\n",
    "    affine_transform2 = cv2.estimateAffinePartial2D(matches2_RANSAC.T, matches1_RANSAC.T)\n",
    "    matches2_all_transformed = cv2.transform(matches2.T[None, :, :], affine_transform2[0], (512, 512), flags=cv2.INTER_LINEAR, borderMode=0)\n",
    "    matches2_all_transformed = matches2_all_transformed[0].T\n",
    "\n",
    "    matches2_inliers_transformed = matches2_all_transformed[:2, matches_RANSAC]\n",
    "\n",
    "    #################################### Metrics for points ####################################\n",
    "        \n",
    "    # calculate the MSE between points1_transformed and points2\n",
    "    mse12 = np.mean((matches1_all_transformed - matches2[:2, :])**2)\n",
    "    tre12 = np.mean(np.sqrt(np.sum((matches1_all_transformed - matches2[:2, :])**2, axis=0)))\n",
    "\n",
    "    # calculate the MSE between points2_transformed and points1\n",
    "    mse21 = np.mean((matches2_all_transformed - matches1[:2, :])**2)\n",
    "    tre21 = np.mean(np.sqrt(np.sum((matches2_all_transformed - matches1[:2, :])**2, axis=0)))\n",
    "\n",
    "    #################################### Metrics for images ####################################\n",
    "    # transform image 1 and 2 using the affine transform matrix\n",
    "    image1_transformed = cv2.warpAffine(image1, affine_transform1[0], (512, 512))\n",
    "    image2_transformed = cv2.warpAffine(image2, affine_transform2[0], (512, 512))\n",
    "\n",
    "    # calculate the MSE between image1_transformed and image2\n",
    "    mse12_image = np.mean((image1_transformed - image2)**2)\n",
    "    # calculate the MSE between image2_transformed and image1\n",
    "    mse21_image = np.mean((image2_transformed - image1)**2)\n",
    "\n",
    "    # calculate SSIM between image1_transformed and image2\n",
    "    ssim12_image = ssim(image1_transformed, image2)\n",
    "    # calculate SSIM between image2_transformed and image1\n",
    "    ssim21_image = ssim(image2_transformed, image1)\n",
    "\n",
    "    # take the match score of the elements in matches_RANSAC to plot with lines\n",
    "    match_score = matches[2, matches_RANSAC]\n",
    "\n",
    "    # Part 3 - Plotting\n",
    "    if plot:\n",
    "        # Create a subplot with 2 rows and 2 columns\n",
    "        fig, axes = plt.subplot_mosaic(\"AAEF;BDCG\", figsize=(20, 10))\n",
    "\n",
    "        overlaid1 = overlay_points(image1.copy(), matches1, radius=2)\n",
    "        overlaid2 = overlay_points(image2.copy(), matches2, radius=2)\n",
    "        \n",
    "        # Row 1: Two images side-by-side with overlaid points and lines\n",
    "        # show also MSE and SSIM between the two images\n",
    "        axes[\"A\"].imshow(draw_lines(overlaid1, overlaid2, matches1_RANSAC, matches2_RANSAC, match_score))\n",
    "        axes[\"A\"].set_title(f\"(RANSAC) Pair {image1_name} with matched points. MSE (x100): {100*np.mean((image1 - image2)**2):.4f} SSIM (x10): {10*ssim(image1, image2):.4f}\")\n",
    "        axes[\"A\"].axis('off')\n",
    "\n",
    "        axes[\"B\"].imshow(image1_transformed, cmap='gray')\n",
    "        axes[\"B\"].set_title(f\"Image A transformed to B\")\n",
    "        axes[\"B\"].axis('off')\n",
    "\n",
    "        axes[\"C\"].imshow(image2_transformed, cmap='gray')\n",
    "        axes[\"C\"].set_title(f\"Image B transformed to A\")\n",
    "        axes[\"C\"].axis('off')\n",
    "\n",
    "        # New subplot for the transformed points\n",
    "        # Blue: from original locations from image 2/1 to the affine-transformed locations\n",
    "        # Red: from affine-transformed locations of points from image 2/1 to \n",
    "        # the locations they supposed to be in image 1/2 (Non-RANSAC)\n",
    "        # Green: from affine-transformed locations of points from image 2/1 to \n",
    "        # the locations they supposed to be in image 1/2 (RANSAC)\n",
    "        img1 = draw_lines_one_image(overlaid1, matches2_all_transformed, matches2, line_color=(0, 0, 155))\n",
    "        img1 = draw_lines_one_image(img1, matches1, matches2_all_transformed, line_color=(255, 0, 0))\n",
    "        img1 = draw_lines_one_image(img1, matches2_inliers_transformed, matches2_RANSAC, line_color=(0, 0, 155))\n",
    "        img1 = draw_lines_one_image(img1, matches1_RANSAC, matches2_inliers_transformed, line_color=(0, 255, 0))\n",
    "        \n",
    "        axes[\"E\"].imshow(img1)\n",
    "        axes[\"E\"].set_title(f\"Image A with points B transformed to A. MSE: {mse21:.4f}\")\n",
    "        axes[\"E\"].axis('off')\n",
    "\n",
    "        # New subplot for the transformed points\n",
    "        # image 2 with lines connecting RANSAC inliers and outliers in different colors\n",
    "        img2 = draw_lines_one_image(overlaid2, matches1_all_transformed, matches1, line_color=(0, 0, 155))\n",
    "        img2 = draw_lines_one_image(img2, matches2, matches1_all_transformed, line_color=(255, 0, 0))\n",
    "        img2 = draw_lines_one_image(img2, matches1_inliers_transformed, matches1_RANSAC, line_color=(0, 0, 155))\n",
    "        img2 = draw_lines_one_image(img2, matches2_RANSAC, matches1_inliers_transformed, line_color=(0, 255, 0))\n",
    "\n",
    "        axes[\"F\"].imshow(img2)\n",
    "        axes[\"F\"].set_title(f\"Image B with points A transformed to B. MSE: {mse12:.4f}\")\n",
    "        axes[\"F\"].axis('off')\n",
    "\n",
    "        # Display the checkerboard image 1 transformed to 2\n",
    "        checkerboard = create_checkerboard(image1_transformed, image2)\n",
    "        axes[\"D\"].imshow(checkerboard, cmap='gray')\n",
    "        axes[\"D\"].set_title(f\"Checkerboard A to B: {mse12_image:.4f}\")\n",
    "        axes[\"D\"].axis('off')\n",
    "\n",
    "        # Display the checkerboard image 2 transformed to 1\n",
    "        checkerboard = create_checkerboard(image2_transformed, image1)\n",
    "        axes[\"G\"].imshow(checkerboard, cmap='gray')\n",
    "        axes[\"G\"].set_title(f\"Checkerboard B to A: {mse21_image:.4f}\")\n",
    "        axes[\"G\"].axis('off')\n",
    "        \n",
    "        plt.tight_layout()  # Adjust the layout to leave space for the histogram\n",
    "        plt.savefig(os.path.join(dir_name, f\"{image1_name}_{name}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    return matches1_inliers_transformed.shape[-1], mse12, mse21, tre12, tre21, mse12_image, mse21_image, ssim12_image, ssim21_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2 methods on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     imageID  translate1  translate2    rotate     shear     scale  \\\n",
      "0  img_0.png          13         -11 -1.826501 -1.723352  1.053680   \n",
      "1  img_1.png         -17          -8  3.769238 -1.413395  1.099760   \n",
      "2  img_2.png         -10          10  2.800051 -3.910975  1.043827   \n",
      "3  img_3.png         -17         -18 -7.410359  2.849766  1.059430   \n",
      "4  img_4.png          20          10  6.031545 -3.787840  1.069417   \n",
      "\n",
      "                                             imgName  \n",
      "0  Dataset/Dataset-processed/15-12-2559/2011248/L...  \n",
      "1  Dataset/Dataset-processed/15-12-2559/2011248/L...  \n",
      "2  Dataset/Dataset-processed/06-10-2560/c2/011025...  \n",
      "3  Dataset/Dataset-processed/30-12-2559/2477598/b...  \n",
      "4  Dataset/Dataset-processed/30-12-2559/2477598/b...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "img_dir_train = \"Dataset/synthetic_eye_dataset_train\"\n",
    "img_dir_test = \"Dataset/synthetic_eye_dataset_test\"\n",
    "\n",
    "# read csv file to get the image names\n",
    "df_test = pd.read_csv('synthetic_dataset_eye_test.csv')\n",
    "df_test.columns = ['imageID', 'translate1', 'translate2', 'rotate', 'shear', 'scale', \\\n",
    "              'imgName']\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "def load_image(img_path, img_size=(512, 512)):\n",
    "    grayim = cv2.imread(img_path, 0)\n",
    "    interp = cv2.INTER_AREA\n",
    "    grayim = cv2.resize(grayim, (img_size[1], img_size[0]), interpolation=interp)\n",
    "    image = (grayim.astype('float32') / 255.)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "Row 0: Dataset/synthetic_eye_dataset_test/img_0.png, Dataset/synthetic_eye_dataset_test/img_0_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/pkhamchuai/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/nn/functional.py:3981: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# main function for testing 2 methods and calculate the MSE, TRE, and SSIM\n",
    "def main_test(weights_path='superpoint_v1.pth', cuda=True):\n",
    "    # print if cuda is available\n",
    "    if cuda:\n",
    "        print('CUDA is available')\n",
    "    else:\n",
    "        print('CUDA is not available')\n",
    "\n",
    "    # Initialize SuperPointFrontend\n",
    "    superpoint = SuperPointFrontend(weights_path, nms_dist=4,\n",
    "                          conf_thresh=0.015,\n",
    "                          nn_thresh=0.7, cuda=cuda)\n",
    "    \n",
    "    # list to store image metrics (MSE, SSIM - before and after)\n",
    "    mse_image_before_list = []\n",
    "    mse_after12_list = []\n",
    "    mse_after21_list = []\n",
    "    mse_after12_RANSAC_list = []\n",
    "    mse_after21_RANSAC_list = []\n",
    "    ssim_before_list = []\n",
    "    ssim_after12_list = []\n",
    "    ssim_after21_list = []\n",
    "    ssim_after12_RANSAC_list = []\n",
    "    ssim_after21_RANSAC_list = []\n",
    "    \n",
    "    # create a list to store the MSE, TRE values\n",
    "    mse_before_list = []\n",
    "    mse12_list = []\n",
    "    mse21_list = []\n",
    "    mse12_RANSAC_list = []\n",
    "    mse21_RANSAC_list = []\n",
    "    tre_before_list = []\n",
    "    tre12_list = []\n",
    "    tre21_list = []\n",
    "    tre12_RANSAC_list = []\n",
    "    tre21_RANSAC_list = []\n",
    "\n",
    "    # create a dataframe to store the metrics\n",
    "    df_metrics = pd.DataFrame(columns=['ID', 'Image 1', 'Image 2', 'MSE before', \\\n",
    "            'MSE 1 to 2', 'MSE 2 to 1', 'TRE before', 'TRE 1 to 2', 'TRE 2 to 1', 'MSE before image', 'MSE 1 to 2 image', \\\n",
    "            'MSE 2 to 1 image', 'SSIM before', 'SSIM 1 to 2 image', 'SSIM 2 to 1 image', 'number of points', \\\n",
    "            'MSE 1 to 2 RANSAC', 'MSE 2 to 1 RANSAC', 'TRE 1 to 2 RANSAC', 'TRE 2 to 1 RANSAC', 'MSE 1 to 2 image RANSAC', \\\n",
    "            'MSE 2 to 1 image RANSAC', 'SSIM 1 to 2 image RANSAC', 'SSIM 2 to 1 image RANSAC', 'number of points RANSAC'])\n",
    "    \n",
    "    # Save the current sys.stdout\n",
    "    original_stdout = sys.stdout\n",
    "\n",
    "    # Create a folder named as yyyy-mm-dd_hh in output_images folder\n",
    "    dirname = f\"../results/{datetime.now().strftime('%Y-%m-%d_%H')+'_eye_synthetic_test'}\"\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "    # create a new file where you want to save the print statements\n",
    "    output_file = os.path.join(dirname, 'output.txt')\n",
    "    # if the file is already there, delete it\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    i = 0\n",
    "    # load images from df_test\n",
    "    for index, row in df_test.iterrows():\n",
    "        img1_path = os.path.join(img_dir_test, row['imageID'])\n",
    "        img2_path = os.path.join(img_dir_test, f\"{row['imageID'][:-4]}_transformed.png\")\n",
    "        print(f'Row {i}: {img1_path}, {img2_path}')\n",
    "        image1 = load_image(img1_path)\n",
    "        image2 = load_image(img2_path)\n",
    "\n",
    "        # Process the first image\n",
    "        points1, desc1, heatmap1 = superpoint.run(image1)\n",
    "\n",
    "        # Process the second image\n",
    "        points2, desc2, heatmap2 = superpoint.run(image2)\n",
    "\n",
    "        # calculate image similarity before transformation\n",
    "        mse_image_before_list.append(np.mean((image1 - image2)**2))\n",
    "        ssim_before_list.append(ssim(image1, image2))\n",
    "\n",
    "        try: # need to do this because the warning cannot be suppressed, fix clipping makes the image all black\n",
    "            # Open the file in append mode to save the print statements\n",
    "            with open(output_file, 'a') as file:\n",
    "                # Redirect sys.stdout to the file\n",
    "                sys.stdout = file\n",
    "                print(f'Row {i}: {img1_path}, {img2_path}')\n",
    "\n",
    "                # Redirect warnings to the file\n",
    "                warnings.filterwarnings('always')  # Capture all warnings\n",
    "                warnings_file = open(output_file, 'a')\n",
    "                warnings.showwarning = lambda message, category, filename, lineno, file=warnings_file, line=None: \\\n",
    "                    file.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
    "                \n",
    "                with capture_output():\n",
    "                    number_points, mse_before, mse12, mse21, tre_before, tre12, tre21, mse12_image, mse21_image, ssim12_image, ssim21_image = affine_and_plot(\n",
    "                        'eye_synth_test_N', dirname, i, image1, image2, points1, points2, desc1, desc2, heatmap1, heatmap2, plot=True)\n",
    "                    # RANSAC, Affine Transform, Plotting in one function\n",
    "                    number_points_RANSAC, mse12_RANSAC, mse21_RANSAC, tre12_RANSAC, tre21_RANSAC, mse12_image_RANSAC, mse21_image_RANSAC, ssim12_image_RANSAC, ssim21_image_RANSAC = RANSAC_affine_plot(\n",
    "                        'eye_synth_test_RANSAC', dirname, i, image1, image2, points1, points2, desc1, desc2, heatmap1, heatmap2, plot=True)\n",
    "                \n",
    "                # After the code execution, restore sys.stdout to its original value\n",
    "                sys.stdout = original_stdout\n",
    "\n",
    "                # Close the warnings file\n",
    "                warnings_file.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle exceptions, if any\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        # calculate statistics\n",
    "        mse_before_list.append(mse_before)\n",
    "        tre_before_list.append(tre_before)\n",
    "        mse12_list.append(mse12)\n",
    "        mse21_list.append(mse21)\n",
    "        tre12_list.append(tre12)\n",
    "        tre21_list.append(tre21)\n",
    "        mse_after12_list.append(mse12_image)\n",
    "        mse_after21_list.append(mse21_image)\n",
    "        ssim_after12_list.append(ssim12_image)\n",
    "        ssim_after21_list.append(ssim21_image)\n",
    "        \n",
    "        mse12_RANSAC_list.append(mse12_RANSAC)\n",
    "        mse21_RANSAC_list.append(mse21_RANSAC)\n",
    "        tre12_RANSAC_list.append(tre12_RANSAC)\n",
    "        tre21_RANSAC_list.append(tre21_RANSAC)\n",
    "        mse_after12_RANSAC_list.append(mse12_image_RANSAC)\n",
    "        mse_after21_RANSAC_list.append(mse21_image_RANSAC)\n",
    "        ssim_after12_RANSAC_list.append(ssim12_image_RANSAC)\n",
    "        ssim_after21_RANSAC_list.append(ssim21_image_RANSAC)\n",
    "\n",
    "        # append the metrics to the dataframe\n",
    "        df_metrics = pd.concat([df_metrics, pd.DataFrame(data={'ID': i, 'Image 1': row['Source image'], 'Image 2': row['Target image'], \\\n",
    "            'MSE before': mse_before, 'MSE 1 to 2': mse12, 'MSE 2 to 1': mse21, 'TRE before': tre_before, \\\n",
    "            'TRE 1 to 2': tre12, 'TRE 2 to 1': tre21, 'MSE before image': np.mean((image1 - image2)**2), 'MSE 1 to 2 image': mse12_image, \\\n",
    "            'MSE 2 to 1 image': mse21_image, 'SSIM before': ssim(image1, image2), 'SSIM 1 to 2 image': ssim12_image, 'SSIM 2 to 1 image': ssim21_image, \\\n",
    "            'number of points': number_points, 'MSE 1 to 2 RANSAC': mse12_RANSAC, 'MSE 2 to 1 RANSAC': mse21_RANSAC, \\\n",
    "            'TRE 1 to 2 RANSAC': tre12_RANSAC, 'TRE 2 to 1 RANSAC': tre21_RANSAC, 'MSE 1 to 2 image RANSAC': mse12_image_RANSAC, \\\n",
    "            'MSE 2 to 1 image RANSAC': mse21_image_RANSAC, 'SSIM 1 to 2 image RANSAC': ssim12_image_RANSAC, \\\n",
    "            'SSIM 2 to 1 image RANSAC': ssim21_image_RANSAC, 'number of points RANSAC': number_points_RANSAC}, index=[i])])\n",
    "\n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break\n",
    "\n",
    "    # print the average MSE\n",
    "    print('\\nNon-RANSAC:')\n",
    "    print('Landmark points:')\n",
    "    print(f'Average MSE before transformation: {np.mean(mse_before_list):.4f}')\n",
    "    print(f'Average MSE for image 1 transformed to image 2: {np.mean(mse12_list):.4f}')\n",
    "    print(f'Average MSE for image 2 transformed to image 1: {np.mean(mse21_list):.4f}')\n",
    "    # print the average MSE both ways\n",
    "    print(f'Average MSE both ways: {np.mean(mse12_list + mse21_list):.4f}')\n",
    "    # print TRE before transformation\n",
    "    print(f'Average TRE before transformation: {np.mean(tre_before_list):.4f}')\n",
    "    # print TRE after transformation\n",
    "    print(f'Average TRE for image 1 transformed to image 2: {np.mean(tre12_list):.4f}')\n",
    "    print(f'Average TRE for image 2 transformed to image 1: {np.mean(tre21_list):.4f}')\n",
    "    # print the average TRE both ways\n",
    "    print(f'Average TRE both ways: {np.mean(tre12_list + tre21_list):.4f}')\n",
    "\n",
    "    print('\\nImage similarity:')\n",
    "    print(f'Average MSE before transformation: {np.mean(mse_image_before_list):.4f}')\n",
    "    print(f'Average MSE for image 1 transformed to image 2: {np.mean(mse_after12_list):.4f}')\n",
    "    print(f'Average MSE for image 2 transformed to image 1: {np.mean(mse_after21_list):.4f}')\n",
    "    # print the average MSE both ways\n",
    "    print(f'Average MSE both ways: {np.mean(mse_after12_list + mse_after21_list):.4f}')\n",
    "    # print image similarity before transformation\n",
    "    print(f'Average SSIM before transformation: {np.mean(ssim_before_list):.4f}')\n",
    "    # print image similarity after transformation\n",
    "    print(f'Average SSIM for image 1 transformed to image 2: {np.mean(ssim_after12_list):.4f}')\n",
    "    print(f'Average SSIM for image 2 transformed to image 1: {np.mean(ssim_after21_list):.4f}')\n",
    "    # print the average SSIM both ways\n",
    "    print(f'Average SSIM both ways: {np.mean(ssim_after12_list + ssim_after21_list):.4f}')\n",
    "\n",
    "    print('\\nRANSAC:')\n",
    "    print('Landmark points:')\n",
    "    print(f'Average MSE before transformation: {np.mean(mse_before_list):.4f}')\n",
    "    print(f'Average MSE for image 1 transformed to image 2: {np.mean(mse12_RANSAC_list):.4f}')\n",
    "    print(f'Average MSE for image 2 transformed to image 1: {np.mean(mse21_RANSAC_list):.4f}')\n",
    "    # print the average MSE both ways\n",
    "    print(f'Average MSE both ways: {np.mean(mse12_RANSAC_list + mse21_RANSAC_list):.4f}')\n",
    "    # print TRE before transformation\n",
    "    print(f'Average TRE before transformation: {np.mean(tre_before_list):.4f}')\n",
    "    # print TRE after transformation\n",
    "    print(f'Average TRE for image 1 transformed to image 2: {np.mean(tre12_RANSAC_list):.4f}')\n",
    "    print(f'Average TRE for image 2 transformed to image 1: {np.mean(tre21_RANSAC_list):.4f}')\n",
    "    # print the average TRE both ways\n",
    "    print(f'Average TRE both ways: {np.mean(tre12_RANSAC_list + tre21_RANSAC_list):.4f}')\n",
    "\n",
    "    print('\\nImage similarity:')\n",
    "    print(f'Average MSE before transformation: {np.mean(mse_image_before_list):.4f}')\n",
    "    print(f'Average MSE for image 1 transformed to image 2: {np.mean(mse_after12_RANSAC_list):.4f}')\n",
    "    print(f'Average MSE for image 2 transformed to image 1: {np.mean(mse_after21_RANSAC_list):.4f}')\n",
    "    # print the average MSE both ways\n",
    "    print(f'Average MSE both ways: {np.mean(mse_after12_RANSAC_list + mse_after21_RANSAC_list):.4f}')\n",
    "    # print image similarity before transformation\n",
    "    print(f'Average SSIM before transformation: {np.mean(ssim_before_list):.4f}')\n",
    "    # print image similarity after transformation\n",
    "    print(f'Average SSIM for image 1 transformed to image 2: {np.mean(ssim_after12_RANSAC_list):.4f}')\n",
    "    print(f'Average SSIM for image 2 transformed to image 1: {np.mean(ssim_after21_RANSAC_list):.4f}')\n",
    "    # print the average SSIM both ways\n",
    "    print(f'\\nAverage SSIM both ways: {np.mean(ssim_after12_RANSAC_list + ssim_after21_RANSAC_list):.4f}')\n",
    "    \n",
    "    # create a new dataframe to store the recently printed metrics\n",
    "    '''df_metrics = pd.DataFrame(columns=['Image 1', 'Image 2', 'MSE before', \\\n",
    "            'MSE 1 to 2', 'MSE 2 to 1', 'TRE before', 'TRE 1 to 2', 'TRE 2 to 1', 'MSE 1 to 2 image', \\\n",
    "            'MSE 2 to 1 image', 'SSIM 1 to 2 image', 'SSIM 2 to 1 image', 'number of points', \\\n",
    "            'MSE 1 to 2 RANSAC', 'MSE 2 to 1 RANSAC', 'TRE 1 to 2 RANSAC', 'TRE 2 to 1 RANSAC', 'MSE 1 to 2 image RANSAC', \\\n",
    "            'MSE 2 to 1 image RANSAC', 'SSIM 1 to 2 image RANSAC', 'SSIM 2 to 1 image RANSAC', 'number of points RANSAC'])'''\n",
    "\n",
    "    df_metrics.to_csv('dataset_eye_synth_metrics_test.csv', index=False)\n",
    "    # delete the file where the print statements are saved\n",
    "    # os.remove(output_file)\n",
    "    print(\"Done!\")\n",
    "    return df_metrics\n",
    "\n",
    "df_results = main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: ../Dataset/synthetic_eye_dataset_test/img_1.png, ../Dataset/synthetic_eye_dataset_test/img_1_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: ../Dataset/synthetic_eye_dataset_test/img_2.png, ../Dataset/synthetic_eye_dataset_test/img_2_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2: ../Dataset/synthetic_eye_dataset_test/img_3.png, ../Dataset/synthetic_eye_dataset_test/img_3_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 3: ../Dataset/synthetic_eye_dataset_test/img_4.png, ../Dataset/synthetic_eye_dataset_test/img_4_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 4: ../Dataset/synthetic_eye_dataset_test/img_5.png, ../Dataset/synthetic_eye_dataset_test/img_5_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 5: ../Dataset/synthetic_eye_dataset_test/img_6.png, ../Dataset/synthetic_eye_dataset_test/img_6_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 6: ../Dataset/synthetic_eye_dataset_test/img_7.png, ../Dataset/synthetic_eye_dataset_test/img_7_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 7: ../Dataset/synthetic_eye_dataset_test/img_8.png, ../Dataset/synthetic_eye_dataset_test/img_8_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 8: ../Dataset/synthetic_eye_dataset_test/img_9.png, ../Dataset/synthetic_eye_dataset_test/img_9_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 9: ../Dataset/synthetic_eye_dataset_test/img_10.png, ../Dataset/synthetic_eye_dataset_test/img_10_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 10: ../Dataset/synthetic_eye_dataset_test/img_11.png, ../Dataset/synthetic_eye_dataset_test/img_11_transformed.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-RANSAC:\n",
      "Landmark points:\n",
      "\n",
      "Average MSE before transformation: 427.8998\n",
      "\n",
      "Average MSE for image 1 transformed to image 2: 1.4401\n",
      "Average MSE for image 2 transformed to image 1: 1.2879\n",
      "\n",
      "Average MSE both ways: 1.3640\n",
      "\n",
      "Average TRE before transformation: 26.7480\n",
      "\n",
      "Average TRE for image 1 transformed to image 2: 0.8599\n",
      "Average TRE for image 2 transformed to image 1: 0.8144\n",
      "\n",
      "Average TRE both ways: 0.8372\n",
      "\n",
      "Image similarity:\n",
      "\n",
      "Average MSE before transformation: 0.0135\n",
      "\n",
      "Average MSE for image 1 transformed to image 2: 0.0000\n",
      "Average MSE for image 2 transformed to image 1: 0.0061\n",
      "\n",
      "Average MSE both ways: 0.0030\n",
      "\n",
      "Average SSIM before transformation: 0.7349\n",
      "\n",
      "Average SSIM for image 1 transformed to image 2: 0.9995\n",
      "Average SSIM for image 2 transformed to image 1: 0.9007\n",
      "\n",
      "Average SSIM both ways: 0.9501\n",
      "\n",
      "RANSAC:\n",
      "Landmark points:\n",
      "\n",
      "Average MSE before transformation: 427.8998\n",
      "\n",
      "Average MSE for image 1 transformed to image 2: 710.9862\n",
      "Average MSE for image 2 transformed to image 1: 651.8953\n",
      "\n",
      "Average MSE both ways: 681.4407\n",
      "\n",
      "Average TRE before transformation: 26.7480\n",
      "\n",
      "Average TRE for image 1 transformed to image 2: 33.7776\n",
      "Average TRE for image 2 transformed to image 1: 32.4951\n",
      "\n",
      "Average TRE both ways: 33.1363\n",
      "\n",
      "Image similarity:\n",
      "\n",
      "Average MSE before transformation: 0.0135\n",
      "\n",
      "Average MSE for image 1 transformed to image 2: 0.0000\n",
      "Average MSE for image 2 transformed to image 1: 0.0061\n",
      "\n",
      "Average MSE both ways: 0.0030\n",
      "\n",
      "Average SSIM before transformation: 0.7349\n",
      "\n",
      "Average SSIM for image 1 transformed to image 2: 0.9994\n",
      "Average SSIM for image 2 transformed to image 1: 0.9008\n",
      "\n",
      "Average SSIM both ways: 0.9501\n",
      "Done!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
