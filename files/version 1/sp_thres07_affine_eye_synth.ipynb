{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import FundamentalMatrixTransform, AffineTransform\n",
    "# Suppress the specific warning\n",
    "import warnings\n",
    "import csv\n",
    "import sys\n",
    "from IPython.utils.capture import capture_output\n",
    "from datetime import datetime\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchir.metrics import NCC\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3: # pragma: no cover\n",
    "  print('Warning: OpenCV 3 is not installed')\n",
    "\n",
    "# Jet colormap for visualization.\n",
    "myjet = np.array([[0.        , 0.        , 0.5       ],\n",
    "                  [0.        , 0.        , 0.99910873],\n",
    "                  [0.        , 0.37843137, 1.        ],\n",
    "                  [0.        , 0.83333333, 1.        ],\n",
    "                  [0.30044276, 1.        , 0.66729918],\n",
    "                  [0.66729918, 1.        , 0.30044276],\n",
    "                  [1.        , 0.90123457, 0.        ],\n",
    "                  [1.        , 0.48002905, 0.        ],\n",
    "                  [0.99910873, 0.07334786, 0.        ],\n",
    "                  [0.5       , 0.        , 0.        ]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuperPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperPointNet(torch.nn.Module):\n",
    "  \"\"\" Pytorch definition of SuperPoint Network. \"\"\"\n",
    "  def __init__(self):\n",
    "    super(SuperPointNet, self).__init__()\n",
    "    self.relu = torch.nn.ReLU(inplace=True)\n",
    "    self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    c1, c2, c3, c4, c5, d1 = 64, 64, 128, 128, 256, 256\n",
    "    # Shared Encoder.\n",
    "    self.conv1a = torch.nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv1b = torch.nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2a = torch.nn.Conv2d(c1, c2, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2b = torch.nn.Conv2d(c2, c2, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3a = torch.nn.Conv2d(c2, c3, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3b = torch.nn.Conv2d(c3, c3, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv4a = torch.nn.Conv2d(c3, c4, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv4b = torch.nn.Conv2d(c4, c4, kernel_size=3, stride=1, padding=1)\n",
    "    # Detector Head.\n",
    "    self.convPa = torch.nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "    self.convPb = torch.nn.Conv2d(c5, 65, kernel_size=1, stride=1, padding=0)\n",
    "    # Descriptor Head.\n",
    "    self.convDa = torch.nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "    self.convDb = torch.nn.Conv2d(c5, d1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" Forward pass that jointly computes unprocessed point and descriptor\n",
    "    tensors.\n",
    "    Input\n",
    "      x: Image pytorch tensor shaped N x 1 x H x W.\n",
    "    Output\n",
    "      semi: Output point pytorch tensor shaped N x 65 x H/8 x W/8.\n",
    "      desc: Output descriptor pytorch tensor shaped N x 256 x H/8 x W/8.\n",
    "    \"\"\"\n",
    "    # Shared Encoder.\n",
    "    x = self.relu(self.conv1a(x))\n",
    "    x = self.relu(self.conv1b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv2a(x))\n",
    "    x = self.relu(self.conv2b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv3a(x))\n",
    "    x = self.relu(self.conv3b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv4a(x))\n",
    "    x = self.relu(self.conv4b(x))\n",
    "    # Detector Head.\n",
    "    cPa = self.relu(self.convPa(x))\n",
    "    semi = self.convPb(cPa)\n",
    "    # Descriptor Head.\n",
    "    cDa = self.relu(self.convDa(x))\n",
    "    desc = self.convDb(cDa)\n",
    "    dn = torch.norm(desc, p=2, dim=1) # Compute the norm.\n",
    "    desc = desc.div(torch.unsqueeze(dn, 1)) # Divide by norm to normalize.\n",
    "    return semi, desc\n",
    "\n",
    "\n",
    "class SuperPointFrontend(object):\n",
    "  \"\"\" Wrapper around pytorch net to help with pre and post image processing. \"\"\"\n",
    "  def __init__(self, weights_path, nms_dist, conf_thresh, nn_thresh,\n",
    "               cuda=False):\n",
    "    self.name = 'SuperPoint'\n",
    "    self.cuda = cuda\n",
    "    self.nms_dist = nms_dist\n",
    "    self.conf_thresh = conf_thresh\n",
    "    self.nn_thresh = nn_thresh # L2 descriptor distance for good match.\n",
    "    self.cell = 8 # Size of each output cell. Keep this fixed.\n",
    "    self.border_remove = 4 # Remove points this close to the border.\n",
    "\n",
    "    # Load the network in inference mode.\n",
    "    self.net = SuperPointNet()\n",
    "    if cuda:\n",
    "      # Train on GPU, deploy on GPU.\n",
    "      self.net.load_state_dict(torch.load(weights_path))\n",
    "      self.net = self.net.cuda()\n",
    "    else:\n",
    "      # Train on GPU, deploy on CPU.\n",
    "      self.net.load_state_dict(torch.load(weights_path,\n",
    "                               map_location=lambda storage, loc: storage))\n",
    "    self.net.eval()\n",
    "\n",
    "  def nms_fast(self, in_corners, H, W, dist_thresh):\n",
    "    \"\"\"\n",
    "    Run a faster approximate Non-Max-Suppression on numpy corners shaped:\n",
    "      3xN [x_i,y_i,conf_i]^T\n",
    "  \n",
    "    Algo summary: Create a grid sized HxW. Assign each corner location a 1, rest\n",
    "    are zeros. Iterate through all the 1's and convert them either to -1 or 0.\n",
    "    Suppress points by setting nearby values to 0.\n",
    "  \n",
    "    Grid Value Legend:\n",
    "    -1 : Kept.\n",
    "     0 : Empty or suppressed.\n",
    "     1 : To be processed (converted to either kept or supressed).\n",
    "  \n",
    "    NOTE: The NMS first rounds points to integers, so NMS distance might not\n",
    "    be exactly dist_thresh. It also assumes points are within image boundaries.\n",
    "  \n",
    "    Inputs\n",
    "      in_corners - 3xN numpy array with corners [x_i, y_i, confidence_i]^T.\n",
    "      H - Image height.\n",
    "      W - Image width.\n",
    "      dist_thresh - Distance to suppress, measured as an infinty norm distance.\n",
    "    Returns\n",
    "      nmsed_corners - 3xN numpy matrix with surviving corners.\n",
    "      nmsed_inds - N length numpy vector with surviving corner indices.\n",
    "    \"\"\"\n",
    "    grid = np.zeros((H, W)).astype(int) # Track NMS data.\n",
    "    inds = np.zeros((H, W)).astype(int) # Store indices of points.\n",
    "    # Sort by confidence and round to nearest int.\n",
    "    inds1 = np.argsort(-in_corners[2,:])\n",
    "    corners = in_corners[:,inds1]\n",
    "    rcorners = corners[:2,:].round().astype(int) # Rounded corners.\n",
    "    # Check for edge case of 0 or 1 corners.\n",
    "    if rcorners.shape[1] == 0:\n",
    "      return np.zeros((3,0)).astype(int), np.zeros(0).astype(int)\n",
    "    if rcorners.shape[1] == 1:\n",
    "      out = np.vstack((rcorners, in_corners[2])).reshape(3,1)\n",
    "      return out, np.zeros((1)).astype(int)\n",
    "    # Initialize the grid.\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "      grid[rcorners[1,i], rcorners[0,i]] = 1\n",
    "      inds[rcorners[1,i], rcorners[0,i]] = i\n",
    "    # Pad the border of the grid, so that we can NMS points near the border.\n",
    "    pad = dist_thresh\n",
    "    grid = np.pad(grid, ((pad,pad), (pad,pad)), mode='constant')\n",
    "    # Iterate through points, highest to lowest conf, suppress neighborhood.\n",
    "    count = 0\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "      # Account for top and left padding.\n",
    "      pt = (rc[0]+pad, rc[1]+pad)\n",
    "      if grid[pt[1], pt[0]] == 1: # If not yet suppressed.\n",
    "        grid[pt[1]-pad:pt[1]+pad+1, pt[0]-pad:pt[0]+pad+1] = 0\n",
    "        grid[pt[1], pt[0]] = -1\n",
    "        count += 1\n",
    "    # Get all surviving -1's and return sorted array of remaining corners.\n",
    "    keepy, keepx = np.where(grid==-1)\n",
    "    keepy, keepx = keepy - pad, keepx - pad\n",
    "    inds_keep = inds[keepy, keepx]\n",
    "    out = corners[:, inds_keep]\n",
    "    values = out[-1, :]\n",
    "    inds2 = np.argsort(-values)\n",
    "    out = out[:, inds2]\n",
    "    out_inds = inds1[inds_keep[inds2]]\n",
    "    return out, out_inds\n",
    "\n",
    "  def run(self, img):\n",
    "    \"\"\" Process a numpy image to extract points and descriptors.\n",
    "    Input\n",
    "      img - HxW numpy float32 input image in range [0,1].\n",
    "    Output\n",
    "      corners - 3xN numpy array with corners [x_i, y_i, confidence_i]^T.\n",
    "      desc - 256xN numpy array of corresponding unit normalized descriptors.\n",
    "      heatmap - HxW numpy heatmap in range [0,1] of point confidences.\n",
    "      \"\"\"\n",
    "    assert img.ndim == 2, 'Image must be grayscale.'\n",
    "    assert img.dtype == np.float32, 'Image must be float32.'\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    inp = img.copy()\n",
    "    inp = (inp.reshape(1, H, W))\n",
    "    inp = torch.from_numpy(inp)\n",
    "    inp = torch.autograd.Variable(inp).view(1, 1, H, W)\n",
    "    if self.cuda:\n",
    "      inp = inp.cuda()\n",
    "    # Forward pass of network.\n",
    "    outs = self.net.forward(inp)\n",
    "    semi, coarse_desc = outs[0], outs[1]\n",
    "    # Convert pytorch -> numpy.\n",
    "    semi = semi.data.cpu().numpy().squeeze()\n",
    "    # --- Process points.\n",
    "    dense = np.exp(semi) # Softmax.\n",
    "    dense = dense / (np.sum(dense, axis=0)+.00001) # Should sum to 1.\n",
    "    # Remove dustbin.\n",
    "    nodust = dense[:-1, :, :]\n",
    "    # Reshape to get full resolution heatmap.\n",
    "    Hc = int(H / self.cell)\n",
    "    Wc = int(W / self.cell)\n",
    "    nodust = nodust.transpose(1, 2, 0)\n",
    "    heatmap = np.reshape(nodust, [Hc, Wc, self.cell, self.cell])\n",
    "    heatmap = np.transpose(heatmap, [0, 2, 1, 3])\n",
    "    heatmap = np.reshape(heatmap, [Hc*self.cell, Wc*self.cell])\n",
    "    xs, ys = np.where(heatmap >= self.conf_thresh) # Confidence threshold.\n",
    "    if len(xs) == 0:\n",
    "      return np.zeros((3, 0)), None, None\n",
    "    pts = np.zeros((3, len(xs))) # Populate point data sized 3xN.\n",
    "    pts[0, :] = ys\n",
    "    pts[1, :] = xs\n",
    "    pts[2, :] = heatmap[xs, ys]\n",
    "    pts, _ = self.nms_fast(pts, H, W, dist_thresh=self.nms_dist) # Apply NMS.\n",
    "    inds = np.argsort(pts[2,:])\n",
    "    pts = pts[:,inds[::-1]] # Sort by confidence.\n",
    "    # Remove points along border.\n",
    "    bord = self.border_remove\n",
    "    toremoveW = np.logical_or(pts[0, :] < bord, pts[0, :] >= (W-bord))\n",
    "    toremoveH = np.logical_or(pts[1, :] < bord, pts[1, :] >= (H-bord))\n",
    "    toremove = np.logical_or(toremoveW, toremoveH)\n",
    "    pts = pts[:, ~toremove]\n",
    "    # --- Process descriptor.\n",
    "    D = coarse_desc.shape[1]\n",
    "    if pts.shape[1] == 0:\n",
    "      desc = np.zeros((D, 0))\n",
    "    else:\n",
    "      # Interpolate into descriptor map using 2D point locations.\n",
    "      samp_pts = torch.from_numpy(pts[:2, :].copy())\n",
    "      samp_pts[0, :] = (samp_pts[0, :] / (float(W)/2.)) - 1.\n",
    "      samp_pts[1, :] = (samp_pts[1, :] / (float(H)/2.)) - 1.\n",
    "      samp_pts = samp_pts.transpose(0, 1).contiguous()\n",
    "      samp_pts = samp_pts.view(1, 1, -1, 2)\n",
    "      samp_pts = samp_pts.float()\n",
    "      if self.cuda:\n",
    "        samp_pts = samp_pts.cuda()\n",
    "      desc = torch.nn.functional.grid_sample(coarse_desc, samp_pts, align_corners=True)\n",
    "      desc = desc.data.cpu().numpy().reshape(D, -1)\n",
    "      desc /= np.linalg.norm(desc, axis=0)[np.newaxis, :]\n",
    "    return pts, desc, heatmap\n",
    "\n",
    "\n",
    "class PointTracker(object):\n",
    "  \"\"\" Class to manage a fixed memory of points and descriptors that enables\n",
    "  sparse optical flow point tracking.\n",
    "\n",
    "  Internally, the tracker stores a 'tracks' matrix sized M x (2+L), of M\n",
    "  tracks with maximum length L, where each row corresponds to:\n",
    "  row_m = [track_id_m, avg_desc_score_m, point_id_0_m, ..., point_id_L-1_m].\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, max_length, nn_thresh):\n",
    "    if max_length < 2:\n",
    "      raise ValueError('max_length must be greater than or equal to 2.')\n",
    "    self.maxl = max_length\n",
    "    self.nn_thresh = nn_thresh\n",
    "    self.all_pts = []\n",
    "    for n in range(self.maxl):\n",
    "      self.all_pts.append(np.zeros((2, 0)))\n",
    "    self.last_desc = None\n",
    "    self.tracks = np.zeros((0, self.maxl+2))\n",
    "    self.track_count = 0\n",
    "    self.max_score = 9999\n",
    "\n",
    "  def nn_match_two_way(self, desc1, desc2, nn_thresh):\n",
    "    \"\"\"\n",
    "    Performs two-way nearest neighbor matching of two sets of descriptors, such\n",
    "    that the NN match from descriptor A->B must equal the NN match from B->A.\n",
    "\n",
    "    Inputs:\n",
    "      desc1 - NxM numpy matrix of N corresponding M-dimensional descriptors.\n",
    "      desc2 - NxM numpy matrix of N corresponding M-dimensional descriptors.\n",
    "      nn_thresh - Optional descriptor distance below which is a good match.\n",
    "\n",
    "    Returns:\n",
    "      matches - 3xL numpy array, of L matches, where L <= N and each column i is\n",
    "                a match of two descriptors, d_i in image 1 and d_j' in image 2:\n",
    "                [d_i index, d_j' index, match_score]^T\n",
    "    \"\"\"\n",
    "    assert desc1.shape[0] == desc2.shape[0]\n",
    "    if desc1.shape[1] == 0 or desc2.shape[1] == 0:\n",
    "      return np.zeros((3, 0))\n",
    "    if nn_thresh < 0.0:\n",
    "      raise ValueError('\\'nn_thresh\\' should be non-negative')\n",
    "    # Compute L2 distance. Easy since vectors are unit normalized.\n",
    "    dmat = np.dot(desc1.T, desc2)\n",
    "    dmat = np.sqrt(2-2*np.clip(dmat, -1, 1))\n",
    "    # Get NN indices and scores.\n",
    "    idx = np.argmin(dmat, axis=1)\n",
    "    scores = dmat[np.arange(dmat.shape[0]), idx]\n",
    "    # Threshold the NN matches.  <<< ======================= this threshold is not good\n",
    "    keep = scores < nn_thresh\n",
    "    # Check if nearest neighbor goes both directions and keep those.\n",
    "    idx2 = np.argmin(dmat, axis=0)\n",
    "    keep_bi = np.arange(len(idx)) == idx2[idx]\n",
    "    keep = np.logical_and(keep, keep_bi)\n",
    "    idx = idx[keep]\n",
    "    scores = scores[keep]\n",
    "    # Get the surviving point indices.\n",
    "    m_idx1 = np.arange(desc1.shape[1])[keep]\n",
    "    m_idx2 = idx\n",
    "    # Populate the final 3xN match data structure.\n",
    "    matches = np.zeros((3, int(keep.sum())))\n",
    "    matches[0, :] = m_idx1\n",
    "    matches[1, :] = m_idx2\n",
    "    matches[2, :] = scores\n",
    "    return matches\n",
    "  \n",
    "  def ransac(self, points1, points2, matches, max_reproj_error=5.0):  # <<=========================================== TODO Here is the RANSAC\n",
    "    '''find matching points between two images using RANSAC'''\n",
    "    \n",
    "    # estimate affine transform model using all coordinates\n",
    "    model = AffineTransform()\n",
    "    model.estimate(points1, points2)\n",
    "\n",
    "    # try min_samples = 3, if fail, try min_samples = 2\n",
    "    try:\n",
    "      # Find the best fundamental matrix using RANSAC\n",
    "      best_model, best_inliers = ransac((points1, points2),\n",
    "                                        AffineTransform, min_samples=3,\n",
    "                                        residual_threshold=max_reproj_error, max_trials=100)\n",
    "    except:\n",
    "      try:\n",
    "        best_model, best_inliers = ransac((points1, points2),\n",
    "                                          AffineTransform, min_samples=2,\n",
    "                                          residual_threshold=max_reproj_error, max_trials=100)\n",
    "        \n",
    "      except:\n",
    "        # if ransac failed, return an array of TRUE with the original matches shape\n",
    "        print('ransac failed')\n",
    "        print('matches shape: ', matches.shape)\n",
    "        return np.ones((matches.shape[1])).astype(bool)\n",
    "    \n",
    "    # the inliners are the matching points\n",
    "    matches = np.array(best_inliers).T\n",
    "\n",
    "    # print(f'matches: {matches.shape}')\n",
    "    return matches\n",
    "\n",
    "  def get_offsets(self):\n",
    "    \"\"\" Iterate through list of points and accumulate an offset value. Used to\n",
    "    index the global point IDs into the list of points.\n",
    "\n",
    "    Returns\n",
    "      offsets - N length array with integer offset locations.\n",
    "    \"\"\"\n",
    "    # Compute id offsets.\n",
    "    offsets = []\n",
    "    offsets.append(0)\n",
    "    for i in range(len(self.all_pts)-1): # Skip last camera size, not needed.\n",
    "      offsets.append(self.all_pts[i].shape[1])\n",
    "    offsets = np.array(offsets)\n",
    "    offsets = np.cumsum(offsets)\n",
    "    return offsets\n",
    "\n",
    "  def update(self, pts, desc):\n",
    "    \"\"\" Add a new set of point and descriptor observations to the tracker.\n",
    "\n",
    "    Inputs\n",
    "      pts - 3xN numpy array of 2D point observations.\n",
    "      desc - DxN numpy array of corresponding D dimensional descriptors.\n",
    "    \"\"\"\n",
    "    if pts is None or desc is None:\n",
    "      print('PointTracker: Warning, no points were added to tracker.')\n",
    "      return\n",
    "    assert pts.shape[1] == desc.shape[1]\n",
    "    # Initialize last_desc.\n",
    "    if self.last_desc is None:\n",
    "      self.last_desc = np.zeros((desc.shape[0], 0))\n",
    "    # Remove oldest points, store its size to update ids later.\n",
    "    remove_size = self.all_pts[0].shape[1]\n",
    "    self.all_pts.pop(0)\n",
    "    self.all_pts.append(pts)\n",
    "    # Remove oldest point in track.\n",
    "    self.tracks = np.delete(self.tracks, 2, axis=1)\n",
    "    # Update track offsets.\n",
    "    for i in range(2, self.tracks.shape[1]):\n",
    "      self.tracks[:, i] -= remove_size\n",
    "    self.tracks[:, 2:][self.tracks[:, 2:] < -1] = -1\n",
    "    offsets = self.get_offsets()\n",
    "    # Add a new -1 column.\n",
    "    self.tracks = np.hstack((self.tracks, -1*np.ones((self.tracks.shape[0], 1))))\n",
    "    # Try to append to existing tracks.\n",
    "    matched = np.zeros((pts.shape[1])).astype(bool)\n",
    "    matches = self.nn_match_two_way(self.last_desc, desc, self.nn_thresh)\n",
    "    for match in matches.T:\n",
    "      # Add a new point to it's matched track.\n",
    "      id1 = int(match[0]) + offsets[-2]\n",
    "      id2 = int(match[1]) + offsets[-1]\n",
    "      found = np.argwhere(self.tracks[:, -2] == id1)\n",
    "      if found.shape[0] > 0:\n",
    "        matched[int(match[1])] = True\n",
    "        row = int(found)\n",
    "        self.tracks[row, -1] = id2\n",
    "        if self.tracks[row, 1] == self.max_score:\n",
    "          # Initialize track score.\n",
    "          self.tracks[row, 1] = match[2]\n",
    "        else:\n",
    "          # Update track score with running average.\n",
    "          # NOTE(dd): this running average can contain scores from old matches\n",
    "          #           not contained in last max_length track points.\n",
    "          track_len = (self.tracks[row, 2:] != -1).sum() - 1.\n",
    "          frac = 1. / float(track_len)\n",
    "          self.tracks[row, 1] = (1.-frac)*self.tracks[row, 1] + frac*match[2]\n",
    "    # Add unmatched tracks.\n",
    "    new_ids = np.arange(pts.shape[1]) + offsets[-1]\n",
    "    new_ids = new_ids[~matched]\n",
    "    new_tracks = -1*np.ones((new_ids.shape[0], self.maxl + 2))\n",
    "    new_tracks[:, -1] = new_ids\n",
    "    new_num = new_ids.shape[0]\n",
    "    new_trackids = self.track_count + np.arange(new_num)\n",
    "    new_tracks[:, 0] = new_trackids\n",
    "    new_tracks[:, 1] = self.max_score*np.ones(new_ids.shape[0])\n",
    "    self.tracks = np.vstack((self.tracks, new_tracks))\n",
    "    self.track_count += new_num # Update the track count.\n",
    "    # Remove empty tracks.\n",
    "    keep_rows = np.any(self.tracks[:, 2:] >= 0, axis=1)\n",
    "    self.tracks = self.tracks[keep_rows, :]\n",
    "    # Store the last descriptors.\n",
    "    self.last_desc = desc.copy()\n",
    "    return\n",
    "\n",
    "  def get_tracks(self, min_length):\n",
    "    \"\"\" Retrieve point tracks of a given minimum length.\n",
    "    Input\n",
    "      min_length - integer >= 1 with minimum track length\n",
    "    Output\n",
    "      returned_tracks - M x (2+L) sized matrix storing track indices, where\n",
    "        M is the number of tracks and L is the maximum track length.\n",
    "    \"\"\"\n",
    "    if min_length < 1:\n",
    "      raise ValueError('\\'min_length\\' too small.')\n",
    "    valid = np.ones((self.tracks.shape[0])).astype(bool)\n",
    "    good_len = np.sum(self.tracks[:, 2:] != -1, axis=1) >= min_length\n",
    "    # Remove tracks which do not have an observation in most recent frame.\n",
    "    not_headless = (self.tracks[:, -1] != -1)\n",
    "    keepers = np.logical_and.reduce((valid, good_len, not_headless))\n",
    "    returned_tracks = self.tracks[keepers, :].copy()\n",
    "    return returned_tracks\n",
    "\n",
    "  def draw_tracks(self, out, tracks):\n",
    "    \"\"\" Visualize tracks all overlayed on a single image.\n",
    "    Inputs\n",
    "      out - numpy uint8 image sized HxWx3 upon which tracks are overlayed.\n",
    "      tracks - M x (2+L) sized matrix storing track info.\n",
    "    \"\"\"\n",
    "    # Store the number of points per camera.\n",
    "    pts_mem = self.all_pts\n",
    "    N = len(pts_mem) # Number of cameras/images.\n",
    "    # Get offset ids needed to reference into pts_mem.\n",
    "    offsets = self.get_offsets()\n",
    "    # Width of track and point circles to be drawn.\n",
    "    stroke = 1\n",
    "    # Iterate through each track and draw it.\n",
    "    for track in tracks:\n",
    "      clr = myjet[int(np.clip(np.floor(track[1]*10), 0, 9)), :]*255\n",
    "      for i in range(N-1):\n",
    "        if track[i+2] == -1 or track[i+3] == -1:\n",
    "          continue\n",
    "        offset1 = offsets[i]\n",
    "        offset2 = offsets[i+1]\n",
    "        idx1 = int(track[i+2]-offset1)\n",
    "        idx2 = int(track[i+3]-offset2)\n",
    "        pt1 = pts_mem[i][:2, idx1]\n",
    "        pt2 = pts_mem[i+1][:2, idx2]\n",
    "        p1 = (int(round(pt1[0])), int(round(pt1[1])))\n",
    "        p2 = (int(round(pt2[0])), int(round(pt2[1])))\n",
    "        cv2.line(out, p1, p2, clr, thickness=stroke, lineType=16)\n",
    "        # Draw end points of each track.\n",
    "        if i == N-2:\n",
    "          clr2 = (255, 0, 0)\n",
    "          cv2.circle(out, p2, stroke, clr2, -1, lineType=16)\n",
    "\n",
    "  def read_image(self, impath, img_size):\n",
    "    \"\"\" Read image as grayscale and resize to img_size.\n",
    "    Inputs\n",
    "      impath: Path to input image.\n",
    "      img_size: (W, H) tuple specifying resize size.\n",
    "    Returns\n",
    "      grayim: float32 numpy array sized H x W with values in range [0, 1].\n",
    "    \"\"\"\n",
    "    grayim = cv2.imread(impath, 0)\n",
    "    if grayim is None:\n",
    "      raise Exception('Error reading image %s' % impath)\n",
    "    # Image is resized via opencv.\n",
    "    interp = cv2.INTER_AREA\n",
    "    grayim = cv2.resize(grayim, (img_size[1], img_size[0]), interpolation=interp)\n",
    "    grayim = (grayim.astype('float32') / 255.)\n",
    "    return grayim\n",
    "\n",
    "  def next_frame(self):\n",
    "    \"\"\" Return the next frame, and increment internal counter.\n",
    "    Returns\n",
    "       image: Next H x W image.\n",
    "       status: True or False depending whether image was loaded.\n",
    "    \"\"\"\n",
    "    if self.i == self.maxlen:\n",
    "      return (None, False)\n",
    "    if self.camera:\n",
    "      ret, input_image = self.cap.read()\n",
    "      if ret is False:\n",
    "        print('VideoStreamer: Cannot get image from camera (maybe bad --camid?)')\n",
    "        return (None, False)\n",
    "      if self.video_file:\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.listing[self.i])\n",
    "      input_image = cv2.resize(input_image, (self.sizer[1], self.sizer[0]),\n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "      input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)\n",
    "      input_image = input_image.astype('float')/255.0\n",
    "    else:\n",
    "      image_file = self.listing[self.i]\n",
    "      input_image = self.read_image(image_file, self.sizer)\n",
    "    # Increment internal counter.\n",
    "    self.i = self.i + 1\n",
    "    input_image = input_image.astype('float32')\n",
    "    return (input_image, True)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImgReg Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128 # image size for CNN\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def forward(self, source1, grad_grid, affine_grid=None):\n",
    "        # image_size = source1.size(-1)\n",
    "        y_, x_ = torch.meshgrid(\n",
    "            [torch.arange(0, image_size).float(), torch.arange(0, image_size).float()])\n",
    "        y_, x_ = 2.0 * y_ / (image_size - 1) - 1.0, 2.0 * x_ / (image_size - 1) - 1.0\n",
    "\n",
    "        x = grad_grid[0, 0].type(torch.FloatTensor) + x_\n",
    "        x = x[np.newaxis, ..., np.newaxis]\n",
    "        y = grad_grid[0, 1].type(torch.FloatTensor) + y_\n",
    "        y = y[np.newaxis, ..., np.newaxis]\n",
    "        grad_grid = torch.cat((x, y), dim=-1)\n",
    "\n",
    "        '''xy = torch.stack([x_, y_], 2)\n",
    "        xy[:,:,0], \\\n",
    "        xy[:,:,1]'''\n",
    "        x = F.grid_sample(source1.cpu(), grad_grid, align_corners=True) # add grid_x and grid_y ? TODO\n",
    "        if affine_grid is not None:\n",
    "            x = F.grid_sample(x, affine_grid, align_corners=True)\n",
    "        return x\n",
    "\n",
    "class AffineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AffineNet, self).__init__()\n",
    "        self.conv1f = 64\n",
    "        self.conv2f = 128\n",
    "        self.conv3f = 256\n",
    "        self.conv4f = 512\n",
    "        self.conv5f = 512\n",
    "        self.conv1 = nn.Conv2d(1, self.conv1f, 3, padding=1, padding_mode='zeros')\n",
    "        self.conv2 = nn.Conv2d(self.conv1f, self.conv2f, 3, padding=1, padding_mode='zeros')\n",
    "        self.conv3 = nn.Conv2d(self.conv2f, self.conv3f, 3, padding=1, padding_mode='zeros')\n",
    "        self.conv4 = nn.Conv2d(self.conv3f, self.conv4f, 3, padding=1, padding_mode='zeros')\n",
    "        self.conv5 = nn.Conv2d(self.conv4f, self.conv5f, 3, padding=1, padding_mode='zeros')\n",
    "\n",
    "        self.conv1s = nn.Conv2d(self.conv1f, self.conv1f, 2, stride=2, padding_mode='zeros')\n",
    "        self.conv2s = nn.Conv2d(self.conv2f, self.conv2f, 2, stride=2, padding_mode='zeros')\n",
    "        self.conv3s = nn.Conv2d(self.conv3f, self.conv3f, 2, stride=2, padding_mode='zeros')\n",
    "        self.conv4s = nn.Conv2d(self.conv4f, self.conv4f, 2, stride=2, padding_mode='zeros')\n",
    "        self.fc1 = nn.Linear(self.conv5f*2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 6)\n",
    "\n",
    "        self.aPooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.ReLU = nn.PReLU()\n",
    "        self.ReLU = nn.LeakyReLU()\n",
    "        self.Act1 = nn.GroupNorm(int(self.conv1f/2), self.conv1f, eps=1e-05, affine=True)\n",
    "        self.Act2 = nn.GroupNorm(int(self.conv2f/2), self.conv2f, eps=1e-05, affine=True)\n",
    "        self.Act3 = nn.GroupNorm(int(self.conv3f/2), self.conv3f, eps=1e-05, affine=True)\n",
    "        self.Act4 = nn.GroupNorm(int(self.conv4f/2), self.conv4f, eps=1e-05, affine=True)\n",
    "        self.Act5 = nn.GroupNorm(int(self.conv5f/2), self.conv5f, eps=1e-05, affine=True)\n",
    "        '''self.Act1 = nn.BatchNorm2d(self.conv1f)\n",
    "        self.Act2 = nn.BatchNorm2d(self.conv2f)\n",
    "        self.Act3 = nn.BatchNorm2d(self.conv3f)\n",
    "        self.Act4 = nn.BatchNorm2d(self.conv4f)\n",
    "        self.Act5 = nn.BatchNorm2d(self.conv5f)'''\n",
    "        '''\n",
    "        self.pooling = nn.AvgPool2d(2, ceil_mode=False)\n",
    "        self.elu = nn.ELU()\n",
    "        self.GN = nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)'''\n",
    "\n",
    "        # self.flow = torch.nn.Parameter(torch.zeros(1, 2, image_size, image_size).to(device), requires_grad=True)\n",
    "        # self.img = torch.nn.Parameter(torch.zeros(1, 1, image_size, image_size).to(device), requires_grad=True)\n",
    "\n",
    "    def forward(self, source_image, target_image):\n",
    "\n",
    "        # print(source_image.size(), target_image.size())\n",
    "        # x = self.conv1(source_image)\n",
    "        x = self.Act1(self.ReLU(self.conv1s(self.Act1(self.ReLU(self.conv1(source_image))))))\n",
    "        y = self.Act1(self.ReLU(self.conv1s(self.Act1(self.ReLU(self.conv1(target_image))))))\n",
    "        # print(x.shape, y.shape)\n",
    "        x = self.Act2(self.ReLU(self.conv2s(self.Act2(self.ReLU(self.conv2(x))))))\n",
    "        y = self.Act2(self.ReLU(self.conv2s(self.Act2(self.ReLU(self.conv2(y))))))\n",
    "        # print(x.shape, y.shape)\n",
    "        x = self.Act3(self.ReLU(self.conv3s(self.Act3(self.ReLU(self.conv3(x))))))\n",
    "        y = self.Act3(self.ReLU(self.conv3s(self.Act3(self.ReLU(self.conv3(y))))))\n",
    "        # print(x.shape, y.shape)\n",
    "        x = self.Act4(self.ReLU(self.conv4s(self.Act4(self.ReLU(self.conv4(x))))))\n",
    "        y = self.Act4(self.ReLU(self.conv4s(self.Act4(self.ReLU(self.conv4(y))))))\n",
    "        # print(x.shape, y.shape)\n",
    "        x = self.aPooling(self.Act5(self.ReLU(self.conv5(x))))\n",
    "        y = self.aPooling(self.Act5(self.ReLU(self.conv5(y))))\n",
    "        # print(x.shape, y.shape)\n",
    "        t = torch.cat((x, y), dim=1)\n",
    "        # print(t.shape)\n",
    "        t = self.fc1(t.flatten())\n",
    "        # print(t.shape)\n",
    "        t = self.fc4(self.fc3(self.fc2(t)))\n",
    "        t = t.view(-1, 2, 3)\n",
    "        # print(t.shape)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_reader(img_name):\n",
    "    X = cv2.imread(img_name)\n",
    "    X = cv2.resize(X, (image_size, image_size))\n",
    "    X = cv2.cvtColor(X, cv2.COLOR_BGR2GRAY)\n",
    "    # X = cv2.bitwise_not(X)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    X = clahe.apply(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "class DatasetIR(data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for converting the data into batches.\n",
    "    The data.Dataset class is a pyTorch class which help\n",
    "    in speeding up  this process with effective parallelization\n",
    "    source: https://github.com/Hsankesara/VoxelMorph-PyTorch/blob/master/main.py\n",
    "    \"\"\"\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "\n",
    "    def __init__(self, list_IDs, transform=None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.list_IDs = list_IDs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of image pairs\"\"\"\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one sample of image pairs\"\"\"\n",
    "        # moving_image = resize(rgb2gray(io.imread(self.list_IDs[index]['moving'])), (image_size, image_size))\n",
    "        moving_image = image_reader(self.list_IDs[index]['moving'])\n",
    "        moving_image = torch.Tensor([moving_image])\n",
    "        if self.transform is not None:\n",
    "            moving_image = self.transform(moving_image)\n",
    "\n",
    "        # fixed_image = resize(rgb2gray(io.imread(self.list_IDs[index]['fixed'])), (image_size, image_size))\n",
    "        fixed_image = image_reader(self.list_IDs[index]['fixed'])\n",
    "        fixed_image = torch.Tensor([fixed_image])\n",
    "        if self.transform is not None:\n",
    "            fixed_image = self.transform(fixed_image)\n",
    "\n",
    "        return moving_image, fixed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = 'all'\n",
    "# cost_grad = losses.Bend_Penalty_()\n",
    "if loss_fn == 'NCC':\n",
    "    cost_function = NCC()\n",
    "    # cost_function = cf.ncc_loss_global\n",
    "elif loss_fn == 'MSE':\n",
    "    cost_function = nn.MSELoss()\n",
    "elif loss_fn == 'all':\n",
    "    cost_NCC = NCC()\n",
    "    cost_MSE = nn.MSELoss()\n",
    "    # cost_grad = losses.Grad\n",
    "    # bending_energy_2d\n",
    "cost_function_params = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_name = 'Affine_MSE' # this one uses DatasetIR class, which is much slower (?)\n",
    "save_dir = 'saved_model'\n",
    "lrate = 0.001\n",
    "start_epoch = 0\n",
    "epoch_num = 300\n",
    "load_model = ''\n",
    "\n",
    "print(f'Training: {model_name} for {epoch_num} epochs')\n",
    "print(f'Load model: {load_model}')\n",
    "print(f'Learning rate: {lrate}')\n",
    "\n",
    "train_img_names = datagenerator.img_name_list_dict(True)\n",
    "assert len(train_img_names) > 0, \"Could not find any training data\"\n",
    "print(f'No. train_vol_names: {len(train_img_names)}')\n",
    "val_img_names = datagenerator.img_name_list_dict(False)\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(image_size),\n",
    "                                       # transforms.ToTensor(),\n",
    "                                       transforms.Normalize(0.5, 0.5)])\n",
    "train_data = DatasetIR(train_img_names, transform=train_transforms)\n",
    "trainloader = data.DataLoader(train_data, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(image_size),\n",
    "                                      transforms.CenterCrop(image_size),\n",
    "                                      # transforms.ToTensor(),\n",
    "                                      transforms.Normalize(0.5, 0.5)])\n",
    "val_data = DatasetIR(val_img_names, transform=train_transforms)\n",
    "valloader = data.DataLoader(val_data, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "train_historyA = []\n",
    "val_historyA = []\n",
    "initial_training_loss = 0.0\n",
    "initial_validation_loss = 0.0\n",
    "\n",
    "modelA = AffineNet().to(device)\n",
    "parametersA = modelA.parameters()\n",
    "optimizerA = optim.Adam(parametersA, lrate)\n",
    "schedulerA = optim.lr_scheduler.LambdaLR(optimizerA, lambda epoch_n: decay_rate ** epoch_n)\n",
    "if load_model != '':\n",
    "    modelA.load_state_dict(torch.load(load_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting, RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to overlay points on the image\n",
    "def overlay_points(image, points, color=(0, 255, 0), radius=5):\n",
    "    for point in points.T:\n",
    "        x, y = int(point[0]), int(point[1])\n",
    "        cv2.circle(image, (x, y), radius, color, -1)\n",
    "    return image\n",
    "\n",
    "# Predefined bright colors\n",
    "bright_colors = [\n",
    "    (255, 0, 0),    # Red\n",
    "    (0, 255, 0),    # Green\n",
    "    (0, 0, 255),    # Blue\n",
    "    (255, 255, 0),  # Yellow\n",
    "    (0, 255, 255),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (255, 128, 0),  # Orange\n",
    "    (0, 255, 128),  # Spring Green\n",
    "    (128, 0, 255),  # Purple\n",
    "    (255, 0, 128),  # Rose\n",
    "]\n",
    "\n",
    "# Function to draw lines connecting points from image 1 to image 2 with random bright colors\n",
    "def draw_lines(image1, image2, points1, points2, match, line_thickness=1, opacity=0.5):\n",
    "    # Convert grayscale images to 3-channel with repeated intensity value\n",
    "    if len(image1.shape) == 2:\n",
    "        image1 = cv2.cvtColor(image1, cv2.COLOR_GRAY2BGR)\n",
    "    if len(image2.shape) == 2:\n",
    "        image2 = cv2.cvtColor(image2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    combined_image = np.concatenate((image1, image2), axis=1)\n",
    "    for pt1, pt2, value in zip(points1.T, points2.T, match):\n",
    "        x1, y1 = int(pt1[0]), int(pt1[1])\n",
    "        x2, y2 = int(pt2[0] + image1.shape[1]), int(pt2[1])\n",
    "\n",
    "        # Randomly choose a color from the predefined bright colors\n",
    "        line_color = random.choice(bright_colors)\n",
    "\n",
    "        # Draw the line with the chosen color and specified thickness\n",
    "        cv2.line(combined_image, (x1, y1), (x2, y2), line_color, line_thickness)\n",
    "\n",
    "        # Add lines with the chosen color and opacity on top of the combined image\n",
    "        overlay = combined_image.copy()\n",
    "        cv2.line(overlay, (x1, y1), (x2, y2), line_color, line_thickness)\n",
    "        cv2.addWeighted(overlay, opacity, combined_image, 1 - opacity, 0, combined_image)\n",
    "\n",
    "        # Add value from 'match' array as text around the beginning of the line on the left image\n",
    "        value_text = f\"{value:.2f}\"\n",
    "        text_x = x1 - 50 if x1 > 50 else x1 + 10\n",
    "        text_y = y1 + 15\n",
    "        cv2.putText(combined_image, value_text, (text_x, text_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, line_color, 1)\n",
    "\n",
    "    return combined_image\n",
    "\n",
    "# function to draw lines connecting original points and the transformed points with random bright colors on one image\n",
    "def draw_lines_one_image(image, points1, points2, line_thickness=1, opacity=0.5, line_color=None):\n",
    "    # Convert grayscale images to 3-channel with repeated intensity value\n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    for pt1, pt2 in zip(points1.T, points2.T):\n",
    "        x1, y1 = int(pt1[0]), int(pt1[1])\n",
    "        x2, y2 = int(pt2[0]), int(pt2[1])\n",
    "\n",
    "        # If not specified, randomly choose a color from the predefined bright colors\n",
    "        if line_color is None:\n",
    "            line_color = random.choice(bright_colors)\n",
    "\n",
    "        # Draw the line with the chosen color and specified thickness\n",
    "        cv2.line(image, (x1, y1), (x2, y2), line_color, line_thickness)\n",
    "\n",
    "        # Add lines with the chosen color and opacity on top of the combined image\n",
    "        overlay = image.copy()\n",
    "        cv2.line(overlay, (x1, y1), (x2, y2), line_color, line_thickness)\n",
    "        cv2.addWeighted(overlay, opacity, image, 1 - opacity, 0, image)\n",
    "\n",
    "        # show the distance between the two points (MSE)\n",
    "        '''value = np.mean((pt1 - pt2)**2)\n",
    "        value_text = f\"{value:.2f}\"\n",
    "        text_x = x1 - 50 if x1 > 50 else x1 + 10\n",
    "        text_y = y1 + 15\n",
    "        cv2.putText(image, value_text, (text_x, text_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, line_color, 1)'''\n",
    "        \n",
    "    return image\n",
    "\n",
    "# Function to create a heatmap from keypoints\n",
    "def create_heatmap(image, keypoints, size=5, sigma=1):\n",
    "    heatmap = np.zeros_like(image)\n",
    "    for kp in keypoints.T:\n",
    "        x, y = int(kp[0]), int(kp[1])\n",
    "        heatmap[y - size:y + size + 1, x - size:x + size + 1] = 1\n",
    "    heatmap = cv2.GaussianBlur(heatmap, (0, 0), sigmaX=sigma, sigmaY=sigma)\n",
    "    return heatmap\n",
    "\n",
    "# Function to create a checkerboard image of image1 and image2 with 10x10 squares\n",
    "def create_checkerboard(image1, image2):\n",
    "    checkerboard = np.zeros((512, 512))\n",
    "    # Create the checkerboard pattern\n",
    "    width = 512\n",
    "\n",
    "    tile_size = width / 10\n",
    "    for i in range(checkerboard.shape[0]):\n",
    "        for j in range(checkerboard.shape[1]):\n",
    "            num = (math.floor(i / tile_size) + math.floor(j / tile_size)) % 2\n",
    "            if num == 0:\n",
    "                checkerboard[i, j] = image1[i, j]\n",
    "            else:\n",
    "                checkerboard[i, j] = image2[i, j]\n",
    "\n",
    "    return checkerboard\n",
    "\n",
    "def check_location(points):\n",
    "    # check if the location of the points in array is within the image\n",
    "    # return the number of points that are outside the image\n",
    "    # points is a 2D array of shape (2, N)\n",
    "    # where N is the number of points\n",
    "    # the first row is the x coordinate\n",
    "    # the second row is the y coordinate\n",
    "    # the image size is 512 x 512\n",
    "    # the points are assumed to be in the range of 0 to 511\n",
    "    # return the number of points that are outside the image\n",
    "    number_outside = np.sum((points < 0) | (points > 511))\n",
    "    \n",
    "    # print if there are points outside the image\n",
    "    if number_outside > 0:\n",
    "        print(f'Number of points outside the image: {number_outside}')\n",
    "    # return number_outside\n",
    "\n",
    "def load_images_from_folder(folder, img_size=(512, 512)):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        grayim = cv2.imread(img_path, 0)\n",
    "        interp = cv2.INTER_AREA\n",
    "        grayim = cv2.resize(grayim, (img_size[1], img_size[0]), interpolation=interp)\n",
    "        image = (grayim.astype('float32') / 255.)\n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "    return images\n",
    "\n",
    "def RANSAC_affine_plot(name, dir_name, image1_name, image1, image2, \n",
    "                       points1, points2, desc1, desc2, heatmap1, heatmap2, plot=True):\n",
    "    # input both matches from RANSAC and non-RANSAC to affine_transform\n",
    "    # because we have to create affine transform matrix from matches_RANSAC\n",
    "    # and apply it to matches1 and matches2\n",
    "    # display them on the image with different colors\n",
    "    print('Affine_plot')\n",
    "\n",
    "    # Part 1 - RANSAC\n",
    "    # match the points between the two images\n",
    "    tracker = PointTracker(5, nn_thresh=0.7)\n",
    "    matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=0.7)\n",
    "\n",
    "    # take the elements from points1 and points2 using the matches as indices\n",
    "    matches1 = points1[:2, matches[0, :].astype(int)]\n",
    "    matches2 = points2[:2, matches[1, :].astype(int)]\n",
    "\n",
    "    # MSE and TRE before transformation\n",
    "    mse_before = np.mean((matches1 - matches2)**2)\n",
    "    tre_before = np.mean(np.sqrt(np.sum((matches1 - matches2)**2, axis=0)))\n",
    "    \n",
    "    # create affine transform matrix from points1 to points2\n",
    "    # and apply it to points1\n",
    "    affine_transform1 = cv2.estimateAffinePartial2D(matches1.T, matches2.T)\n",
    "    print(f'Affine transform matrix from points1 to points2:\\n{affine_transform1[0]}')\n",
    "    matches1_transformed = cv2.transform(matches1.T[None, :, :], affine_transform1[0])\n",
    "    matches1_transformed = matches1_transformed[0].T\n",
    "\n",
    "    mse12 = np.mean((matches1_transformed - matches2)**2)\n",
    "    tre12 = np.mean(np.sqrt(np.sum((matches1_transformed - matches2)**2, axis=0)))\n",
    "\n",
    "    # create affine transform matrix from points2 to points1\n",
    "    # and apply it to points2\n",
    "    affine_transform2 = cv2.estimateAffinePartial2D(matches2[:2, :].T, matches1[:2, :].T)\n",
    "    print(f'Affine transform matrix from points2 to points1:\\n{affine_transform2[0]}')\n",
    "    matches2_transformed = cv2.transform(matches2.T[None, :, :], affine_transform2[0])\n",
    "    matches2_transformed = matches2_transformed[0].T\n",
    "\n",
    "    mse21 = np.mean((matches2_transformed - matches1)**2)\n",
    "    tre21 = np.mean(np.sqrt(np.sum((matches2_transformed - matches1)**2, axis=0)))\n",
    "\n",
    "    # transform image 1 and 2 using the affine transform matrix\n",
    "    image1_transformed = cv2.warpAffine(image1, affine_transform1[0], (512, 512))\n",
    "    image2_transformed = cv2.warpAffine(image2, affine_transform2[0], (512, 512))\n",
    "\n",
    "    mse_before_image = np.mean((image1 - image2)**2)\n",
    "    # calculate the MSE between image1_transformed and image2\n",
    "    mse12_image = np.mean((image1_transformed - image2)**2)\n",
    "    # calculate the MSE between image2_transformed and image1\n",
    "    mse21_image = np.mean((image2_transformed - image1)**2)\n",
    "\n",
    "    ssim_before = ssim(image1, image2, data_range=image2.max() - image2.min())\n",
    "    # calculate SSIM between image1_transformed and image2\n",
    "    ssim12_image = ssim(image1_transformed, image2)\n",
    "    # calculate SSIM between image2_transformed and image1\n",
    "    ssim21_image = ssim(image2_transformed, image1)\n",
    "\n",
    "    # Part 3 - Plotting\n",
    "    if plot:\n",
    "        # Create a subplot with 2 rows and 2 columns\n",
    "        fig, axes = plt.subplot_mosaic(\"AAFE;BDCG\", figsize=(20, 10))\n",
    "\n",
    "        overlaid1 = overlay_points(image1.copy(), matches1, radius=2)\n",
    "        overlaid2 = overlay_points(image2.copy(), matches2, radius=2)\n",
    "        overlaid1_transformed = overlay_points(image1_transformed.copy(), matches1_transformed, radius=2)\n",
    "        overlaid2_transformed = overlay_points(image2_transformed.copy(), matches2_transformed, radius=2)\n",
    "        \n",
    "        # Row 1: Two images side-by-side with overlaid points and lines\n",
    "        # show also MSE and SSIM between the two images\n",
    "        axes[\"A\"].imshow(draw_lines(overlaid1, overlaid2, matches1, matches2, matches[2, :]))\n",
    "        axes[\"A\"].set_title(f\"Pair {image1_name} with matched points. MSE (x100): {100*mse_before_image:.4f} SSIM (x10): {10*ssim_before:.4f}\")\n",
    "        axes[\"A\"].axis('off')\n",
    "\n",
    "        axes[\"B\"].imshow(overlaid1_transformed, cmap='gray')\n",
    "        axes[\"B\"].set_title(f\"Image A transformed to B\")\n",
    "        axes[\"B\"].axis('off')\n",
    "\n",
    "        axes[\"C\"].imshow(overlaid2_transformed, cmap='gray')\n",
    "        axes[\"C\"].set_title(f\"Image B transformed to A\")\n",
    "        axes[\"C\"].axis('off')\n",
    "\n",
    "        # New subplot for histograms of 'match_score' array\n",
    "        '''axes[\"D\"].hist(match_score, bins=20, color='blue', alpha=0.7)\n",
    "        axes[\"D\"].set_title(f\"Match Score Histogram, {len(match_score)} matches\")\n",
    "        axes[\"D\"].set_xlabel(\"Value\")\n",
    "        axes[\"D\"].set_ylabel(\"Frequency\")'''\n",
    "\n",
    "        # New subplot for the transformed points\n",
    "        # Blue: from original locations from image 2/1 to the affine-transformed locations\n",
    "        # Red: from affine-transformed locations of points from image 2/1 to \n",
    "        # the locations they supposed to be in image 1/2\n",
    "        img2 = draw_lines_one_image(overlaid2, matches1_transformed, matches1, line_color=(0, 0, 155))\n",
    "        img2 = draw_lines_one_image(img2, matches2, matches1_transformed, line_color=(255, 0, 0))\n",
    "        axes[\"F\"].imshow(img2)\n",
    "        axes[\"F\"].set_title(f\"Image B with points A transformed to B. MSE: {mse12:.4f}\")\n",
    "        axes[\"F\"].axis('off')\n",
    "        \n",
    "        img1 = draw_lines_one_image(overlaid1, matches2_transformed, matches2, line_color=(0, 0, 155))\n",
    "        img1 = draw_lines_one_image(img1, matches1, matches2_transformed, line_color=(255, 0, 0))\n",
    "        axes[\"E\"].imshow(img1)\n",
    "        axes[\"E\"].set_title(f\"Image A with points B transformed to A. MSE: {mse21:.4f}\")\n",
    "        axes[\"E\"].axis('off')        \n",
    "\n",
    "        # Display the checkerboard image 1 transformed to 2\n",
    "        checkerboard = create_checkerboard(overlaid1_transformed, overlaid2)\n",
    "        axes[\"D\"].imshow(checkerboard, cmap='gray')\n",
    "        axes[\"D\"].set_title(f\"Checkerboard A to B: {mse12_image:.4f}\")\n",
    "        axes[\"D\"].axis('off')\n",
    "\n",
    "        # Display the checkerboard image 2 transformed to 1\n",
    "        checkerboard = create_checkerboard(overlaid2_transformed, overlaid1)\n",
    "        axes[\"G\"].imshow(checkerboard, cmap='gray')\n",
    "        axes[\"G\"].set_title(f\"Checkerboard B to A: {mse21_image:.4f}\")\n",
    "        axes[\"G\"].axis('off')\n",
    "        \n",
    "        plt.tight_layout()  # Adjust the layout to leave space for the histogram\n",
    "        # create a folder named as yyyy-mm-dd_hh in output_images folder      \n",
    "        plt.savefig(os.path.join(dir_name, f\"{image1_name}_normal.png\"))\n",
    "        \n",
    "        # save images to output folder\n",
    "        '''cv2.imwrite(f\"../Dataset/output_images/transformed_images/{image1_name}_{image2_name}_{name}_1.png\", image1_transformed*255)\n",
    "        cv2.imwrite(f\"../Dataset/output_images/transformed_images/{image1_name}_{image2_name}_{name}_2.png\", image2_transformed*255)'''\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    print('RANSAC_affine_plot')\n",
    "    matches_RANSAC = tracker.ransac(matches1.T, matches2.T, matches, max_reproj_error=2)\n",
    "            \n",
    "    # take elements from matches1 and matches2 where matches_RANSAC is TRUE\n",
    "    matches_RANSAC = matches_RANSAC.reshape(-1)\n",
    "    matches1_RANSAC = matches1[:, matches_RANSAC]\n",
    "    matches2_RANSAC = matches2[:, matches_RANSAC]\n",
    "    print(f'{matches1_RANSAC.shape}, {matches2_RANSAC.shape}')\n",
    "\n",
    "    '''# take elements from matches1 and matches2 where matches_RANSAC is FALSE\n",
    "    matches1_outliers = matches1[:2, ~matches_RANSAC]\n",
    "    matches2_outliers = matches2[:2, ~matches_RANSAC]\n",
    "    # print(f'outliers: {matches1_outliers.shape}, {matches2_outliers.shape}')'''\n",
    "\n",
    "    # Part 2 - Affine Transform\n",
    "    # create affine transform matrix from points1 to points2 and apply it to points1\n",
    "    # also from points2 to points1 and apply it to points2\n",
    "    # points1 and points2 are 2D arrays of shape (2, N)\n",
    "    # where N is the number of points\n",
    "    # the first row is the x coordinate\n",
    "    # the second row is the y coordinate\n",
    "    # the image size is 512 x 512\n",
    "\n",
    "    #################################### FIX THIS ####################################\n",
    "    # create affine transform matrix using only matches_RANSAC\n",
    "    # and apply it to all points, plot all first, then plot RANSAC over it\n",
    "    affine_transform1 = cv2.estimateAffinePartial2D(matches1_RANSAC.T, matches2_RANSAC.T)\n",
    "    print(f'Affine transform matrix from points1 to points2:\\n{affine_transform1[0]}')\n",
    "    matches1_all_transformed = cv2.transform(matches1.T[None, :, :], affine_transform1[0], (512, 512))\n",
    "    matches1_all_transformed = matches1_all_transformed[0].T\n",
    "\n",
    "    matches1_inliers_transformed = matches1_all_transformed[:, matches_RANSAC]\n",
    "\n",
    "    # create affine transform matrix from points2 to points1\n",
    "    # and apply it to all points, plot all first, then plot RANSAC over it\n",
    "    affine_transform2 = cv2.estimateAffinePartial2D(matches2_RANSAC.T, matches1_RANSAC.T)\n",
    "    print(f'Affine transform matrix from points2 to points1:\\n{affine_transform2[0]}')\n",
    "    matches2_all_transformed = cv2.transform(matches2.T[None, :, :], affine_transform2[0], (512, 512))\n",
    "    matches2_all_transformed = matches2_all_transformed[0].T\n",
    "\n",
    "    matches2_inliers_transformed = matches2_all_transformed[:, matches_RANSAC]\n",
    "\n",
    "    #################################### Metrics for points ####################################\n",
    "        \n",
    "    # calculate the MSE between points1_transformed and points2\n",
    "    mse12_RANSAC = np.mean((matches1_all_transformed - matches2)**2)\n",
    "    tre12_RANSAC = np.mean(np.sqrt(np.sum((matches1_all_transformed - matches2)**2, axis=0)))\n",
    "\n",
    "    # calculate the MSE between points2_transformed and points1\n",
    "    mse21_RANSAC = np.mean((matches2_all_transformed - matches1)**2)\n",
    "    tre21_RANSAC = np.mean(np.sqrt(np.sum((matches2_all_transformed - matches1)**2, axis=0)))\n",
    "\n",
    "    #################################### Metrics for images ####################################\n",
    "    # transform image 1 and 2 using the affine transform matrix\n",
    "    image1_transformed = cv2.warpAffine(image1, affine_transform1[0], (512, 512))\n",
    "    image2_transformed = cv2.warpAffine(image2, affine_transform2[0], (512, 512))\n",
    "\n",
    "    # calculate the MSE between image1_transformed and image2\n",
    "    mse12_image_RANSAC = np.mean((image1_transformed - image2)**2)\n",
    "    # calculate the MSE between image2_transformed and image1\n",
    "    mse21_image_RANSAC = np.mean((image2_transformed - image1)**2)\n",
    "\n",
    "    # calculate SSIM between image1_transformed and image2\n",
    "    ssim12_image_RANSAC = ssim(image1_transformed, image2, data_range=image2.max() - image2.min())\n",
    "    # calculate SSIM between image2_transformed and image1\n",
    "    ssim21_image_RANSAC = ssim(image2_transformed, image1, data_range=image1.max() - image1.min())\n",
    "\n",
    "    # take the match score of the elements in matches_RANSAC to plot with lines\n",
    "    match_score = matches[2, matches_RANSAC]\n",
    "\n",
    "    # Part 3 - Plotting\n",
    "    if plot:\n",
    "        # Create a subplot with 2 rows and 2 columns\n",
    "        fig, axes = plt.subplot_mosaic(\"AAFE;BDCG\", figsize=(20, 10))\n",
    "\n",
    "        overlaid1 = overlay_points(image1.copy(), matches1, radius=2)\n",
    "        overlaid2 = overlay_points(image2.copy(), matches2, radius=2)\n",
    "        overlaid1_transformed = overlay_points(image1_transformed.copy(), matches1_all_transformed, radius=2)\n",
    "        overlaid2_transformed = overlay_points(image2_transformed.copy(), matches2_all_transformed, radius=2)\n",
    "        \n",
    "        # Row 1: Two images side-by-side with overlaid points and lines\n",
    "        # show also MSE and SSIM between the two images\n",
    "        axes[\"A\"].imshow(draw_lines(overlaid1, overlaid2, matches1_RANSAC, matches2_RANSAC, match_score))\n",
    "        axes[\"A\"].set_title(f\"(RANSAC) Pair {image1_name} with matched points. MSE (x100): {100*mse_before_image:.4f} SSIM (x10): {10*ssim_before:.4f}\")\n",
    "        axes[\"A\"].axis('off')\n",
    "\n",
    "        axes[\"B\"].imshow(overlaid1_transformed, cmap='gray')\n",
    "        axes[\"B\"].set_title(f\"Image A transformed to B\")\n",
    "        axes[\"B\"].axis('off')\n",
    "        \n",
    "        axes[\"C\"].imshow(overlaid2_transformed, cmap='gray')\n",
    "        # axes[\"C\"].imshow(image2_transformed, cmap='gray')\n",
    "        axes[\"C\"].set_title(f\"Image B transformed to A\")\n",
    "        axes[\"C\"].axis('off')\n",
    "\n",
    "        # New subplot for the transformed points\n",
    "        # Blue: from original locations from image 2/1 to the affine-transformed locations\n",
    "        # Red: from affine-transformed locations of points from image 2/1 to \n",
    "        # the locations they supposed to be in image 1/2 (Non-RANSAC)\n",
    "        # Green: from affine-transformed locations of points from image 2/1 to \n",
    "        # the locations they supposed to be in image 1/2 (Non-RANSAC)\n",
    "\n",
    "        # image 2 with lines connecting RANSAC inliers and outliers in different colors\n",
    "        img2 = draw_lines_one_image(overlaid2, matches1_all_transformed, matches1, line_color=(0, 0, 155))\n",
    "        img2 = draw_lines_one_image(img2, matches2, matches1_all_transformed, line_color=(255, 0, 0))\n",
    "        img2 = draw_lines_one_image(img2, matches1_inliers_transformed, matches1_RANSAC, line_color=(0, 255, 0))\n",
    "        # img2 = draw_lines_one_image(img2, matches2_RANSAC, matches1_inliers_transformed, line_color=(0, 255, 0))\n",
    "\n",
    "        axes[\"F\"].imshow(img2)\n",
    "        axes[\"F\"].set_title(f\"Image B with points A transformed to B. MSE: {mse12_RANSAC:.4f}\")\n",
    "        axes[\"F\"].axis('off')\n",
    "\n",
    "        img1 = draw_lines_one_image(overlaid1, matches2_all_transformed, matches2, line_color=(0, 0, 155)) # color: blue\n",
    "        img1 = draw_lines_one_image(img1, matches1, matches2_all_transformed, line_color=(255, 0, 0))\n",
    "        img1 = draw_lines_one_image(img1, matches2_inliers_transformed, matches2_RANSAC, line_color=(0, 255, 0)) # RANSAC inliers\n",
    "        # img1 = draw_lines_one_image(img1, matches1_RANSAC, matches2_inliers_transformed, line_color=(0, 255, 0)) # RANSAC inliers\n",
    "        \n",
    "        axes[\"E\"].imshow(img1)\n",
    "        axes[\"E\"].set_title(f\"Image A with points B transformed to A. MSE: {mse21_RANSAC:.4f}\")\n",
    "        axes[\"E\"].axis('off')\n",
    "\n",
    "        # Display the checkerboard image 1 transformed to 2\n",
    "        # checkerboard = create_checkerboard(image1_transformed, image2)\n",
    "        checkerboard = create_checkerboard(overlaid1_transformed, overlaid2)\n",
    "        axes[\"D\"].imshow(checkerboard, cmap='gray')\n",
    "        axes[\"D\"].set_title(f\"Checkerboard A to B: {mse12_image_RANSAC:.4f}\")\n",
    "        axes[\"D\"].axis('off')\n",
    "\n",
    "        # Display the checkerboard image 2 transformed to 1\n",
    "        # checkerboard = create_checkerboard(image2_transformed, image1)\n",
    "        checkerboard = create_checkerboard(overlaid2_transformed, overlaid1)\n",
    "        axes[\"G\"].imshow(checkerboard, cmap='gray')\n",
    "        axes[\"G\"].set_title(f\"Checkerboard B to A: {mse21_image_RANSAC:.4f}\")\n",
    "        axes[\"G\"].axis('off')\n",
    "        \n",
    "        plt.tight_layout()  # Adjust the layout to leave space for the histogram\n",
    "        plt.savefig(os.path.join(dir_name, f\"{image1_name}_RANSAC.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    return matches1_transformed.shape[-1], mse_before, mse12, mse21, tre_before, tre12, tre21, mse_before_image, mse12_image, mse21_image, ssim_before, ssim12_image, ssim21_image, \\\n",
    "        matches1_inliers_transformed.shape[-1], mse12_RANSAC, mse21_RANSAC, tre12_RANSAC, tre21_RANSAC, mse12_image_RANSAC, mse21_image_RANSAC, ssim12_image_RANSAC, ssim21_image_RANSAC\n",
    "\n",
    "def Affine_plot(name, dir_name, image1_name, image1, image2, \n",
    "                       points1, points2, desc1, desc2, heatmap1, heatmap2, plot=True):\n",
    "    # input both matches from RANSAC and non-RANSAC to affine_transform\n",
    "    # because we have to create affine transform matrix from matches_RANSAC\n",
    "    # and apply it to matches1 and matches2\n",
    "    # display them on the image with different colors\n",
    "    print('Affine_plot')\n",
    "\n",
    "    # Part 1\n",
    "    # match the points between the two images\n",
    "    tracker = PointTracker(5, nn_thresh=0.7)\n",
    "    matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=0.7)\n",
    "    print(f'Number of matches: {matches.shape}')\n",
    "\n",
    "    # take the elements from points1 and points2 using the matches as indices\n",
    "    matches1 = points1[:2, matches[0, :].astype(int)]\n",
    "    matches2 = points2[:2, matches[1, :].astype(int)]\n",
    "\n",
    "    # MSE and TRE before transformation\n",
    "    mse_before = np.mean((matches1 - matches2)**2)\n",
    "    tre_before = np.mean(np.sqrt(np.sum((matches1 - matches2)**2, axis=0)))\n",
    "    \n",
    "    # create affine transform matrix from points1 to points2\n",
    "    # and apply it to points1\n",
    "    affine_transform1 = cv2.estimateAffinePartial2D(matches1.T, matches2.T)\n",
    "    print(f'Affine transform matrix from points1 to points2:\\n{affine_transform1[0]}')\n",
    "    matches1_transformed = cv2.transform(matches1.T[None, :, :], affine_transform1[0])\n",
    "    matches1_transformed = matches1_transformed[0].T\n",
    "\n",
    "    mse12 = np.mean((matches1_transformed - matches2)**2)\n",
    "    tre12 = np.mean(np.sqrt(np.sum((matches1_transformed - matches2)**2, axis=0)))\n",
    "\n",
    "    # create affine transform matrix from points2 to points1\n",
    "    # and apply it to points2\n",
    "    affine_transform2 = cv2.estimateAffinePartial2D(matches2[:2, :].T, matches1[:2, :].T)\n",
    "    print(f'Affine transform matrix from points2 to points1:\\n{affine_transform2[0]}')\n",
    "    matches2_transformed = cv2.transform(matches2.T[None, :, :], affine_transform2[0])\n",
    "    matches2_transformed = matches2_transformed[0].T\n",
    "\n",
    "    mse21 = np.mean((matches2_transformed - matches1)**2)\n",
    "    tre21 = np.mean(np.sqrt(np.sum((matches2_transformed - matches1)**2, axis=0)))\n",
    "\n",
    "    # transform image 1 and 2 using the affine transform matrix\n",
    "    image1_transformed = cv2.warpAffine(image1, affine_transform1[0], (512, 512))\n",
    "    image2_transformed = cv2.warpAffine(image2, affine_transform2[0], (512, 512))\n",
    "\n",
    "    mse_before_image = np.mean((image1 - image2)**2)\n",
    "    # calculate the MSE between image1_transformed and image2\n",
    "    mse12_image = np.mean((image1_transformed - image2)**2)\n",
    "    # calculate the MSE between image2_transformed and image1\n",
    "    mse21_image = np.mean((image2_transformed - image1)**2)\n",
    "\n",
    "    ssim_before = ssim(image1, image2, data_range=1)\n",
    "    # calculate SSIM between image1_transformed and image2\n",
    "    ssim12 = ssim(image1_transformed, image2, data_range=1)\n",
    "    # calculate SSIM between image2_transformed and image1\n",
    "    ssim21 = ssim(image2_transformed, image1, data_range=1)\n",
    "\n",
    "    # Part 3 - Plotting\n",
    "    if plot:\n",
    "        # Create a subplot with 2 rows and 2 columns\n",
    "        fig, axes = plt.subplot_mosaic(\"AAFE;BDCG\", figsize=(20, 10))\n",
    "\n",
    "        overlaid1 = overlay_points(image1.copy(), matches1, radius=2)\n",
    "        overlaid2 = overlay_points(image2.copy(), matches2, radius=2)\n",
    "        overlaid1_transformed = overlay_points(image1_transformed.copy(), matches1_transformed, radius=2)\n",
    "        overlaid2_transformed = overlay_points(image2_transformed.copy(), matches2_transformed, radius=2)\n",
    "        \n",
    "        # Row 1: Two images side-by-side with overlaid points and lines\n",
    "        # show also MSE and SSIM between the two images\n",
    "        axes[\"A\"].imshow(draw_lines(overlaid1, overlaid2, matches1, matches2, matches[2, :]))\n",
    "        axes[\"A\"].set_title(f\"Pair {image1_name} with matched points. MSE (x100): {100*mse_before_image:.4f} SSIM (x10): {10*ssim_before:.4f}\")\n",
    "        axes[\"A\"].axis('off')\n",
    "\n",
    "        axes[\"B\"].imshow(overlaid1_transformed, cmap='gray')\n",
    "        axes[\"B\"].set_title(f\"Image A transformed to B\")\n",
    "        axes[\"B\"].axis('off')\n",
    "\n",
    "        axes[\"C\"].imshow(overlaid2_transformed, cmap='gray')\n",
    "        axes[\"C\"].set_title(f\"Image B transformed to A\")\n",
    "        axes[\"C\"].axis('off')\n",
    "\n",
    "        # New subplot for histograms of 'match_score' array\n",
    "        '''axes[\"D\"].hist(match_score, bins=20, color='blue', alpha=0.7)\n",
    "        axes[\"D\"].set_title(f\"Match Score Histogram, {len(match_score)} matches\")\n",
    "        axes[\"D\"].set_xlabel(\"Value\")\n",
    "        axes[\"D\"].set_ylabel(\"Frequency\")'''\n",
    "\n",
    "        # New subplot for the transformed points\n",
    "        # Blue: from original locations from image 2/1 to the affine-transformed locations\n",
    "        # Red: from affine-transformed locations of points from image 2/1 to \n",
    "        # the locations they supposed to be in image 1/2\n",
    "        img2 = draw_lines_one_image(overlaid2, matches1_transformed, matches1, line_color=(0, 0, 155))\n",
    "        img2 = draw_lines_one_image(img2, matches2, matches1_transformed, line_color=(255, 0, 0))\n",
    "        axes[\"F\"].imshow(img2)\n",
    "        axes[\"F\"].set_title(f\"Image B with points A transformed to B. MSE: {mse12:.4f}\")\n",
    "        axes[\"F\"].axis('off')\n",
    "        \n",
    "        img1 = draw_lines_one_image(overlaid1, matches2_transformed, matches2, line_color=(0, 0, 155))\n",
    "        img1 = draw_lines_one_image(img1, matches1, matches2_transformed, line_color=(255, 0, 0))\n",
    "        axes[\"E\"].imshow(img1)\n",
    "        axes[\"E\"].set_title(f\"Image A with points B transformed to A. MSE: {mse21:.4f}\")\n",
    "        axes[\"E\"].axis('off')        \n",
    "\n",
    "        # Display the checkerboard image 1 transformed to 2\n",
    "        checkerboard = create_checkerboard(overlaid1_transformed, overlaid2)\n",
    "        axes[\"D\"].imshow(checkerboard, cmap='gray')\n",
    "        axes[\"D\"].set_title(f\"Checkerboard A to B: {mse12_image:.4f}\")\n",
    "        axes[\"D\"].axis('off')\n",
    "\n",
    "        # Display the checkerboard image 2 transformed to 1\n",
    "        checkerboard = create_checkerboard(overlaid2_transformed, overlaid1)\n",
    "        axes[\"G\"].imshow(checkerboard, cmap='gray')\n",
    "        axes[\"G\"].set_title(f\"Checkerboard B to A: {mse21_image:.4f}\")\n",
    "        axes[\"G\"].axis('off')\n",
    "        \n",
    "        plt.tight_layout()  # Adjust the layout to leave space for the histogram\n",
    "        # create a folder named as yyyy-mm-dd_hh in output_images folder      \n",
    "        plt.savefig(os.path.join(dir_name, f\"{image1_name}_normal.png\"))\n",
    "        \n",
    "        # save images to output folder\n",
    "        '''cv2.imwrite(f\"../Dataset/output_images/transformed_images/{image1_name}_{image2_name}_{name}_1.png\", image1_transformed*255)\n",
    "        cv2.imwrite(f\"../Dataset/output_images/transformed_images/{image1_name}_{image2_name}_{name}_2.png\", image2_transformed*255)'''\n",
    "        plt.close()\n",
    "\n",
    "    return matches1_transformed.shape[-1], mse_before, mse12, mse21, tre_before, tre12, tre21, mse_before_image, mse12_image, mse21_image, ssim_before, ssim12, ssim21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     imageID  translate1  translate2    rotate     shear     scale  \\\n",
      "0  img_0.png           7         -18 -9.624466  1.161701  1.097009   \n",
      "1  img_1.png         -14           2 -0.458117 -3.123924  1.035084   \n",
      "2  img_2.png         -10           8  0.672812 -4.200006  1.015481   \n",
      "3  img_3.png         -20           7  8.142925  2.318126  1.086933   \n",
      "4  img_4.png         -14         -17  7.588277  1.685383  1.044198   \n",
      "\n",
      "                                             imgName  \n",
      "0  Dataset/Dataset-processed/15-12-2559/2011248/L...  \n",
      "1  Dataset/Dataset-processed/15-12-2559/2011248/L...  \n",
      "2  Dataset/Dataset-processed/06-10-2560/c2/011025...  \n",
      "3  Dataset/Dataset-processed/30-12-2559/2477598/b...  \n",
      "4  Dataset/Dataset-processed/30-12-2559/2477598/b...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "img_dir_train = \"Dataset/synthetic_eye_dataset_train\"\n",
    "img_dir_test = \"Dataset/synthetic_eye_dataset_test\"\n",
    "\n",
    "# read csv file to get the image names\n",
    "df_test = pd.read_csv('dataset_eye_synth_test.csv')\n",
    "df_test.columns = ['imageID', 'translate1', 'translate2', 'rotate', 'shear', 'scale', \\\n",
    "              'imgName']\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "def load_image(img_path, img_size=(512, 512)):\n",
    "    grayim = cv2.imread(img_path, 0)\n",
    "    interp = cv2.INTER_AREA\n",
    "    grayim = cv2.resize(grayim, (img_size[1], img_size[0]), interpolation=interp)\n",
    "\n",
    "    # normalize image to range 0 to 1\n",
    "    image = (grayim/np.max(grayim)).astype('float32')\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row 0: Dataset/synthetic_eye_dataset_test/img_0.png, Dataset/synthetic_eye_dataset_test/img_0_transformed.png\n",
      "\n",
      "Row 1: Dataset/synthetic_eye_dataset_test/img_1.png, Dataset/synthetic_eye_dataset_test/img_1_transformed.png\n",
      "\n",
      "Row 2: Dataset/synthetic_eye_dataset_test/img_2.png, Dataset/synthetic_eye_dataset_test/img_2_transformed.png\n",
      "\n",
      "Row 3: Dataset/synthetic_eye_dataset_test/img_3.png, Dataset/synthetic_eye_dataset_test/img_3_transformed.png\n",
      "\n",
      "Row 4: Dataset/synthetic_eye_dataset_test/img_4.png, Dataset/synthetic_eye_dataset_test/img_4_transformed.png\n",
      "\n",
      "Row 5: Dataset/synthetic_eye_dataset_test/img_5.png, Dataset/synthetic_eye_dataset_test/img_5_transformed.png\n",
      "\n",
      "Row 6: Dataset/synthetic_eye_dataset_test/img_6.png, Dataset/synthetic_eye_dataset_test/img_6_transformed.png\n",
      "\n",
      "Row 7: Dataset/synthetic_eye_dataset_test/img_7.png, Dataset/synthetic_eye_dataset_test/img_7_transformed.png\n",
      "\n",
      "Row 8: Dataset/synthetic_eye_dataset_test/img_8.png, Dataset/synthetic_eye_dataset_test/img_8_transformed.png\n",
      "\n",
      "Row 9: Dataset/synthetic_eye_dataset_test/img_9.png, Dataset/synthetic_eye_dataset_test/img_9_transformed.png\n",
      "\n",
      "Row 10: Dataset/synthetic_eye_dataset_test/img_10.png, Dataset/synthetic_eye_dataset_test/img_10_transformed.png\n",
      "\n",
      "Row 11: Dataset/synthetic_eye_dataset_test/img_11.png, Dataset/synthetic_eye_dataset_test/img_11_transformed.png\n",
      "\n",
      "Row 12: Dataset/synthetic_eye_dataset_test/img_12.png, Dataset/synthetic_eye_dataset_test/img_12_transformed.png\n",
      "\n",
      "Row 13: Dataset/synthetic_eye_dataset_test/img_13.png, Dataset/synthetic_eye_dataset_test/img_13_transformed.png\n",
      "\n",
      "Row 14: Dataset/synthetic_eye_dataset_test/img_14.png, Dataset/synthetic_eye_dataset_test/img_14_transformed.png\n",
      "\n",
      "Row 15: Dataset/synthetic_eye_dataset_test/img_15.png, Dataset/synthetic_eye_dataset_test/img_15_transformed.png\n",
      "\n",
      "Row 16: Dataset/synthetic_eye_dataset_test/img_16.png, Dataset/synthetic_eye_dataset_test/img_16_transformed.png\n",
      "\n",
      "Row 17: Dataset/synthetic_eye_dataset_test/img_17.png, Dataset/synthetic_eye_dataset_test/img_17_transformed.png\n",
      "\n",
      "Row 18: Dataset/synthetic_eye_dataset_test/img_18.png, Dataset/synthetic_eye_dataset_test/img_18_transformed.png\n",
      "\n",
      "Row 19: Dataset/synthetic_eye_dataset_test/img_19.png, Dataset/synthetic_eye_dataset_test/img_19_transformed.png\n",
      "\n",
      "Row 20: Dataset/synthetic_eye_dataset_test/img_20.png, Dataset/synthetic_eye_dataset_test/img_20_transformed.png\n",
      "\n",
      "Row 21: Dataset/synthetic_eye_dataset_test/img_21.png, Dataset/synthetic_eye_dataset_test/img_21_transformed.png\n",
      "\n",
      "Row 22: Dataset/synthetic_eye_dataset_test/img_22.png, Dataset/synthetic_eye_dataset_test/img_22_transformed.png\n",
      "\n",
      "Row 23: Dataset/synthetic_eye_dataset_test/img_23.png, Dataset/synthetic_eye_dataset_test/img_23_transformed.png\n",
      "\n",
      "Row 24: Dataset/synthetic_eye_dataset_test/img_24.png, Dataset/synthetic_eye_dataset_test/img_24_transformed.png\n",
      "\n",
      "Row 25: Dataset/synthetic_eye_dataset_test/img_25.png, Dataset/synthetic_eye_dataset_test/img_25_transformed.png\n",
      "\n",
      "Row 26: Dataset/synthetic_eye_dataset_test/img_26.png, Dataset/synthetic_eye_dataset_test/img_26_transformed.png\n",
      "\n",
      "Row 27: Dataset/synthetic_eye_dataset_test/img_27.png, Dataset/synthetic_eye_dataset_test/img_27_transformed.png\n",
      "\n",
      "Row 28: Dataset/synthetic_eye_dataset_test/img_28.png, Dataset/synthetic_eye_dataset_test/img_28_transformed.png\n",
      "\n",
      "Row 29: Dataset/synthetic_eye_dataset_test/img_29.png, Dataset/synthetic_eye_dataset_test/img_29_transformed.png\n",
      "\n",
      "Row 30: Dataset/synthetic_eye_dataset_test/img_30.png, Dataset/synthetic_eye_dataset_test/img_30_transformed.png\n",
      "\n",
      "Row 31: Dataset/synthetic_eye_dataset_test/img_31.png, Dataset/synthetic_eye_dataset_test/img_31_transformed.png\n",
      "\n",
      "Row 32: Dataset/synthetic_eye_dataset_test/img_32.png, Dataset/synthetic_eye_dataset_test/img_32_transformed.png\n",
      "\n",
      "Row 33: Dataset/synthetic_eye_dataset_test/img_33.png, Dataset/synthetic_eye_dataset_test/img_33_transformed.png\n",
      "\n",
      "Row 34: Dataset/synthetic_eye_dataset_test/img_34.png, Dataset/synthetic_eye_dataset_test/img_34_transformed.png\n",
      "\n",
      "Row 35: Dataset/synthetic_eye_dataset_test/img_35.png, Dataset/synthetic_eye_dataset_test/img_35_transformed.png\n",
      "\n",
      "Row 36: Dataset/synthetic_eye_dataset_test/img_36.png, Dataset/synthetic_eye_dataset_test/img_36_transformed.png\n",
      "\n",
      "Row 37: Dataset/synthetic_eye_dataset_test/img_37.png, Dataset/synthetic_eye_dataset_test/img_37_transformed.png\n",
      "\n",
      "Row 38: Dataset/synthetic_eye_dataset_test/img_38.png, Dataset/synthetic_eye_dataset_test/img_38_transformed.png\n",
      "\n",
      "Row 39: Dataset/synthetic_eye_dataset_test/img_39.png, Dataset/synthetic_eye_dataset_test/img_39_transformed.png\n",
      "\n",
      "Row 40: Dataset/synthetic_eye_dataset_test/img_40.png, Dataset/synthetic_eye_dataset_test/img_40_transformed.png\n",
      "\n",
      "Row 41: Dataset/synthetic_eye_dataset_test/img_41.png, Dataset/synthetic_eye_dataset_test/img_41_transformed.png\n",
      "\n",
      "Row 42: Dataset/synthetic_eye_dataset_test/img_42.png, Dataset/synthetic_eye_dataset_test/img_42_transformed.png\n",
      "\n",
      "Row 43: Dataset/synthetic_eye_dataset_test/img_43.png, Dataset/synthetic_eye_dataset_test/img_43_transformed.png\n",
      "\n",
      "Row 44: Dataset/synthetic_eye_dataset_test/img_44.png, Dataset/synthetic_eye_dataset_test/img_44_transformed.png\n",
      "\n",
      "Row 45: Dataset/synthetic_eye_dataset_test/img_45.png, Dataset/synthetic_eye_dataset_test/img_45_transformed.png\n",
      "\n",
      "Row 46: Dataset/synthetic_eye_dataset_test/img_46.png, Dataset/synthetic_eye_dataset_test/img_46_transformed.png\n",
      "\n",
      "Row 47: Dataset/synthetic_eye_dataset_test/img_47.png, Dataset/synthetic_eye_dataset_test/img_47_transformed.png\n",
      "\n",
      "Row 48: Dataset/synthetic_eye_dataset_test/img_48.png, Dataset/synthetic_eye_dataset_test/img_48_transformed.png\n",
      "\n",
      "Row 49: Dataset/synthetic_eye_dataset_test/img_49.png, Dataset/synthetic_eye_dataset_test/img_49_transformed.png\n",
      "\n",
      "Row 50: Dataset/synthetic_eye_dataset_test/img_50.png, Dataset/synthetic_eye_dataset_test/img_50_transformed.png\n",
      "\n",
      "Non-RANSAC:\n",
      "Landmark points:\n",
      "Average MSE before transformation: 579.2591\n",
      "Average MSE for image 1 transformed to image 2: 97.4777\n",
      "Average MSE for image 2 transformed to image 1: 94.4945\n",
      "Average MSE both ways: 95.9861\n",
      "Average TRE before transformation: 30.1185\n",
      "Average TRE for image 1 transformed to image 2: 8.9848\n",
      "Average TRE for image 2 transformed to image 1: 8.7739\n",
      "Average TRE both ways: 8.8794\n",
      "\n",
      "Image similarity:\n",
      "Average MSE before transformation: 0.0166\n",
      "Average MSE for image 1 transformed to image 2: 0.0064\n",
      "Average MSE for image 2 transformed to image 1: 0.0128\n",
      "Average MSE both ways: 0.0096\n",
      "Average SSIM before transformation: 0.5840\n",
      "Average SSIM for image 1 transformed to image 2: 0.7458\n",
      "Average SSIM for image 2 transformed to image 1: 0.6661\n",
      "Average SSIM both ways: 0.7060\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# main function for testing 2 methods and calculate the MSE, TRE, and SSIM\n",
    "def main_test(weights_path='superpoint_v1.pth', cuda=True, print_output=False):\n",
    "    # print if cuda is available\n",
    "    if cuda:\n",
    "        print('CUDA is available')\n",
    "    else:\n",
    "        print('CUDA is not available')\n",
    "\n",
    "    # Initialize SuperPointFrontend\n",
    "    superpoint = SuperPointFrontend(weights_path, nms_dist=4,\n",
    "                          conf_thresh=0.015,\n",
    "                          nn_thresh=0.7, cuda=cuda)\n",
    "    \n",
    "    # list to store image metrics (MSE, SSIM - before and after)\n",
    "    mse_image_before_list = []\n",
    "    mse_after12_list = []\n",
    "    mse_after21_list = []\n",
    "    ssim_before_list = []\n",
    "    ssim_after12_list = []\n",
    "    ssim_after21_list = []\n",
    "    \n",
    "    # create a list to store the MSE, TRE values\n",
    "    mse_before_list = []\n",
    "    mse12_list = []\n",
    "    mse21_list = []\n",
    "    tre_before_list = []\n",
    "    tre12_list = []\n",
    "    tre21_list = []\n",
    "\n",
    "    # create a dataframe to store the metrics\n",
    "    df_metrics = pd.DataFrame(columns=['ID', 'Image 1', 'Image 2', 'MSE before', \\\n",
    "            'MSE 1 to 2', 'MSE 2 to 1', 'TRE before', 'TRE 1 to 2', 'TRE 2 to 1', 'MSE before image', 'MSE 1 to 2 image', \\\n",
    "            'MSE 2 to 1 image', 'SSIM before', 'SSIM 1 to 2 image', 'SSIM 2 to 1 image', 'number of points', \\\n",
    "            'MSE 1 to 2 RANSAC', 'MSE 2 to 1 RANSAC', 'TRE 1 to 2 RANSAC', 'TRE 2 to 1 RANSAC', 'MSE 1 to 2 image RANSAC', \\\n",
    "            'MSE 2 to 1 image RANSAC', 'SSIM 1 to 2 image RANSAC', 'SSIM 2 to 1 image RANSAC', 'number of points RANSAC'])\n",
    "    \n",
    "    # Save the current sys.stdout\n",
    "    original_stdout = sys.stdout\n",
    "\n",
    "    # Create a folder named as yyyy-mm-dd_hh in output_images folder\n",
    "    dirname = f\"../results/{datetime.now().strftime('%Y-%m-%d_%H')+'_eye_synth_test'}\"\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "    # Define a file where you want to save the print statements\n",
    "    output_file = os.path.join(dirname, 'output.txt')\n",
    "    # if the file is already there, delete it\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    i = 0\n",
    "    # load images from df_test\n",
    "    for index, row in df_test.iterrows():\n",
    "        img1_path = os.path.join(img_dir_test, row['imageID'])\n",
    "        img2_path = os.path.join(img_dir_test, f\"{row['imageID'][:-4]}_transformed.png\")\n",
    "        # heatmap1_path = os.path.join(img_dir_test, f\"{row['imageID'][:-4]}_heatmap.png\")\n",
    "        # heatmap2_path = os.path.join(img_dir_test, f\"{row['imageID'][:-4]}_transformed_heatmap.png\")\n",
    "        print(f'\\nRow {i}: {img1_path}, {img2_path}')\n",
    "        image1 = load_image(img1_path)\n",
    "        image2 = load_image(img2_path)\n",
    "        # heatmap1 = load_image(heatmap1_path)\n",
    "        # heatmap2 = load_image(heatmap2_path)\n",
    "\n",
    "        # Process the first image\n",
    "        points1, desc1, heatmap1 = superpoint.run(image1)\n",
    "        # Process the second image\n",
    "        points2, desc2, heatmap2 = superpoint.run(image2)\n",
    "\n",
    "        if print_output:\n",
    "            # Affine Transform, Plotting in one function\n",
    "            number_points, mse_before, mse12, mse21, tre_before, tre12, tre21, mse_before_image, mse12_image, mse21_image, ssim_before, ssim12_image, ssim21_image = Affine_plot(\n",
    "                    'eye_synth', dirname, i, image1, image2, points1, points2, desc1, desc2, heatmap1, heatmap2, plot=True)\n",
    "\n",
    "        else:\n",
    "            try: # need to do this because the warning cannot be suppressed, fix clipping makes the image all black\n",
    "                # Open the file in append mode to save the print statements\n",
    "                with open(output_file, 'a') as file:\n",
    "                    # Redirect sys.stdout to the file\n",
    "                    sys.stdout = file\n",
    "                    print(f'Row {i}: {row[\"imageID\"]}, {row[\"imgName\"]}')\n",
    "\n",
    "                    # Redirect warnings to the file\n",
    "                    # warnings.filterwarnings('always')  # Capture all warnings\n",
    "                    warnings_file = open(output_file, 'a')\n",
    "                    warnings.showwarning = lambda message, category, filename, lineno, file=warnings_file, line=None: \\\n",
    "                        file.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
    "                    \n",
    "                    with capture_output():\n",
    "                        number_points, mse_before, mse12, mse21, tre_before, tre12, tre21, mse_before_image, mse12_image, mse21_image, ssim_before, ssim12_image, ssim21_image = Affine_plot(\n",
    "                                'eye_synth', dirname, i, image1, image2, points1, points2, desc1, desc2, heatmap1, heatmap2, plot=True)\n",
    "                    \n",
    "                    # After the code execution, restore sys.stdout to its original value\n",
    "                    sys.stdout = original_stdout\n",
    "\n",
    "                    # Close the warnings file\n",
    "                    warnings_file.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle exceptions, if any\n",
    "                print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        # calculate statistics\n",
    "        mse_before_list.append(mse_before)\n",
    "        tre_before_list.append(tre_before)\n",
    "        mse12_list.append(mse12)\n",
    "        mse21_list.append(mse21)\n",
    "        tre12_list.append(tre12)\n",
    "        tre21_list.append(tre21)\n",
    "        mse_image_before_list.append(mse_before_image)\n",
    "        ssim_before_list.append(ssim_before)\n",
    "        mse_after12_list.append(mse12_image)\n",
    "        mse_after21_list.append(mse21_image)\n",
    "        ssim_after12_list.append(ssim12_image)\n",
    "        ssim_after21_list.append(ssim21_image)\n",
    "\n",
    "        # append the metrics to the dataframe\n",
    "        df_metrics = pd.concat([df_metrics, pd.DataFrame(data={'ID': i, 'Image 1': row[\"imageID\"], 'Image 2': row[\"imgName\"], \\\n",
    "            'MSE before': mse_before, 'MSE 1 to 2': mse12, 'MSE 2 to 1': mse21, 'TRE before': tre_before, \\\n",
    "            'TRE 1 to 2': tre12, 'TRE 2 to 1': tre21, 'MSE before image': np.mean((image1 - image2)**2), 'MSE 1 to 2 image': mse12_image, \\\n",
    "            'MSE 2 to 1 image': mse21_image, 'SSIM before': ssim_before, 'SSIM 1 to 2 image': ssim12_image, 'SSIM 2 to 1 image': ssim21_image, \\\n",
    "            'number of points': number_points}, index=[i])])\n",
    "\n",
    "        i += 1\n",
    "        if i > 50:\n",
    "            break\n",
    "\n",
    "    # print the average MSE\n",
    "    print('\\nNon-RANSAC:')\n",
    "    print('Landmark points:')\n",
    "    print(f'Average MSE before transformation: {np.mean(mse_before_list):.4f}')\n",
    "    print(f'Average MSE for image 1 transformed to image 2: {np.mean(mse12_list):.4f}')\n",
    "    print(f'Average MSE for image 2 transformed to image 1: {np.mean(mse21_list):.4f}')\n",
    "    # print the average MSE both ways\n",
    "    print(f'Average MSE both ways: {np.mean(mse12_list + mse21_list):.4f}')\n",
    "    # print TRE before transformation\n",
    "    print(f'Average TRE before transformation: {np.mean(tre_before_list):.4f}')\n",
    "    # print TRE after transformation\n",
    "    print(f'Average TRE for image 1 transformed to image 2: {np.mean(tre12_list):.4f}')\n",
    "    print(f'Average TRE for image 2 transformed to image 1: {np.mean(tre21_list):.4f}')\n",
    "    # print the average TRE both ways\n",
    "    print(f'Average TRE both ways: {np.mean(tre12_list + tre21_list):.4f}')\n",
    "\n",
    "    print('\\nImage similarity:')\n",
    "    print(f'Average MSE before transformation: {np.mean(mse_image_before_list):.4f}')\n",
    "    print(f'Average MSE for image 1 transformed to image 2: {np.mean(mse_after12_list):.4f}')\n",
    "    print(f'Average MSE for image 2 transformed to image 1: {np.mean(mse_after21_list):.4f}')\n",
    "    # print the average MSE both ways\n",
    "    print(f'Average MSE both ways: {np.mean(mse_after12_list + mse_after21_list):.4f}')\n",
    "    # print image similarity before transformation\n",
    "    print(f'Average SSIM before transformation: {np.mean(ssim_before_list):.4f}')\n",
    "    # print image similarity after transformation\n",
    "    print(f'Average SSIM for image 1 transformed to image 2: {np.mean(ssim_after12_list):.4f}')\n",
    "    print(f'Average SSIM for image 2 transformed to image 1: {np.mean(ssim_after21_list):.4f}')\n",
    "    # print the average SSIM both ways\n",
    "    print(f'Average SSIM both ways: {np.mean(ssim_after12_list + ssim_after21_list):.4f}')\n",
    "\n",
    "    # create a new dataframe to store the recently printed metrics\n",
    "    '''df_metrics = pd.DataFrame(columns=['Image 1', 'Image 2', 'MSE before', \\\n",
    "            'MSE 1 to 2', 'MSE 2 to 1', 'TRE before', 'TRE 1 to 2', 'TRE 2 to 1', 'MSE 1 to 2 image', \\\n",
    "            'MSE 2 to 1 image', 'SSIM 1 to 2 image', 'SSIM 2 to 1 image', 'number of points', \\\n",
    "            'MSE 1 to 2 RANSAC', 'MSE 2 to 1 RANSAC', 'TRE 1 to 2 RANSAC', 'TRE 2 to 1 RANSAC', 'MSE 1 to 2 image RANSAC', \\\n",
    "            'MSE 2 to 1 image RANSAC', 'SSIM 1 to 2 image RANSAC', 'SSIM 2 to 1 image RANSAC', 'number of points RANSAC'])'''\n",
    "\n",
    "    df_metrics.to_csv('metrics_eye_synth_test.csv', index=False)\n",
    "    print(\"Done!\")\n",
    "    return df_metrics\n",
    "\n",
    "df_results = main_test(print_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Image 1</th>\n",
       "      <th>Image 2</th>\n",
       "      <th>MSE before</th>\n",
       "      <th>MSE 1 to 2</th>\n",
       "      <th>MSE 2 to 1</th>\n",
       "      <th>TRE before</th>\n",
       "      <th>TRE 1 to 2</th>\n",
       "      <th>TRE 2 to 1</th>\n",
       "      <th>MSE before image</th>\n",
       "      <th>...</th>\n",
       "      <th>number of points</th>\n",
       "      <th>MSE 1 to 2 RANSAC</th>\n",
       "      <th>MSE 2 to 1 RANSAC</th>\n",
       "      <th>TRE 1 to 2 RANSAC</th>\n",
       "      <th>TRE 2 to 1 RANSAC</th>\n",
       "      <th>MSE 1 to 2 image RANSAC</th>\n",
       "      <th>MSE 2 to 1 image RANSAC</th>\n",
       "      <th>SSIM 1 to 2 image RANSAC</th>\n",
       "      <th>SSIM 2 to 1 image RANSAC</th>\n",
       "      <th>number of points RANSAC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>img_0.png</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/L...</td>\n",
       "      <td>636.951104</td>\n",
       "      <td>8.249670</td>\n",
       "      <td>7.127057</td>\n",
       "      <td>33.971938</td>\n",
       "      <td>3.332139</td>\n",
       "      <td>3.105964</td>\n",
       "      <td>0.032340</td>\n",
       "      <td>...</td>\n",
       "      <td>317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>img_1.png</td>\n",
       "      <td>Dataset/Dataset-processed/15-12-2559/2011248/L...</td>\n",
       "      <td>253.938988</td>\n",
       "      <td>91.170492</td>\n",
       "      <td>77.194437</td>\n",
       "      <td>21.784760</td>\n",
       "      <td>10.574028</td>\n",
       "      <td>9.731946</td>\n",
       "      <td>0.028745</td>\n",
       "      <td>...</td>\n",
       "      <td>336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>img_2.png</td>\n",
       "      <td>Dataset/Dataset-processed/06-10-2560/c2/011025...</td>\n",
       "      <td>351.516447</td>\n",
       "      <td>385.963940</td>\n",
       "      <td>359.306701</td>\n",
       "      <td>24.796496</td>\n",
       "      <td>19.467310</td>\n",
       "      <td>18.639313</td>\n",
       "      <td>0.007106</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>img_3.png</td>\n",
       "      <td>Dataset/Dataset-processed/30-12-2559/2477598/b...</td>\n",
       "      <td>620.820724</td>\n",
       "      <td>39.147796</td>\n",
       "      <td>33.017301</td>\n",
       "      <td>33.625942</td>\n",
       "      <td>6.707538</td>\n",
       "      <td>6.165530</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>...</td>\n",
       "      <td>304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>img_4.png</td>\n",
       "      <td>Dataset/Dataset-processed/30-12-2559/2477598/b...</td>\n",
       "      <td>37.998744</td>\n",
       "      <td>21.794903</td>\n",
       "      <td>19.641560</td>\n",
       "      <td>7.468068</td>\n",
       "      <td>5.245580</td>\n",
       "      <td>4.979430</td>\n",
       "      <td>0.004732</td>\n",
       "      <td>...</td>\n",
       "      <td>398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID    Image 1                                            Image 2  \\\n",
       "0  0  img_0.png  Dataset/Dataset-processed/15-12-2559/2011248/L...   \n",
       "1  1  img_1.png  Dataset/Dataset-processed/15-12-2559/2011248/L...   \n",
       "2  2  img_2.png  Dataset/Dataset-processed/06-10-2560/c2/011025...   \n",
       "3  3  img_3.png  Dataset/Dataset-processed/30-12-2559/2477598/b...   \n",
       "4  4  img_4.png  Dataset/Dataset-processed/30-12-2559/2477598/b...   \n",
       "\n",
       "   MSE before  MSE 1 to 2  MSE 2 to 1  TRE before  TRE 1 to 2  TRE 2 to 1  \\\n",
       "0  636.951104    8.249670    7.127057   33.971938    3.332139    3.105964   \n",
       "1  253.938988   91.170492   77.194437   21.784760   10.574028    9.731946   \n",
       "2  351.516447  385.963940  359.306701   24.796496   19.467310   18.639313   \n",
       "3  620.820724   39.147796   33.017301   33.625942    6.707538    6.165530   \n",
       "4   37.998744   21.794903   19.641560    7.468068    5.245580    4.979430   \n",
       "\n",
       "   MSE before image  ...  number of points  MSE 1 to 2 RANSAC  \\\n",
       "0          0.032340  ...               317                NaN   \n",
       "1          0.028745  ...               336                NaN   \n",
       "2          0.007106  ...               152                NaN   \n",
       "3          0.009231  ...               304                NaN   \n",
       "4          0.004732  ...               398                NaN   \n",
       "\n",
       "   MSE 2 to 1 RANSAC  TRE 1 to 2 RANSAC  TRE 2 to 1 RANSAC  \\\n",
       "0                NaN                NaN                NaN   \n",
       "1                NaN                NaN                NaN   \n",
       "2                NaN                NaN                NaN   \n",
       "3                NaN                NaN                NaN   \n",
       "4                NaN                NaN                NaN   \n",
       "\n",
       "  MSE 1 to 2 image RANSAC MSE 2 to 1 image RANSAC SSIM 1 to 2 image RANSAC  \\\n",
       "0                     NaN                     NaN                      NaN   \n",
       "1                     NaN                     NaN                      NaN   \n",
       "2                     NaN                     NaN                      NaN   \n",
       "3                     NaN                     NaN                      NaN   \n",
       "4                     NaN                     NaN                      NaN   \n",
       "\n",
       "  SSIM 2 to 1 image RANSAC number of points RANSAC  \n",
       "0                      NaN                     NaN  \n",
       "1                      NaN                     NaN  \n",
       "2                      NaN                     NaN  \n",
       "3                      NaN                     NaN  \n",
       "4                      NaN                     NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_value = 0.7\n",
    "\n",
    "# plot the histogram of the metrics\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(df_results['MSE before'], bins=20, color='blue', alpha=alpha_value, label='before')\n",
    "plt.hist((df_results['MSE 1 to 2']+df_results['MSE 2 to 1'])/2, bins=20, color='red', alpha=alpha_value, label='non-RANSAC')\n",
    "plt.legend()\n",
    "plt.xlabel('MSE (points)')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(df_results['TRE before'], bins=20, color='blue', alpha=alpha_value, label='before')\n",
    "plt.hist((df_results['TRE 1 to 2']+df_results['TRE 2 to 1'])/2, bins=20, color='red', alpha=alpha_value, label='non-RANSAC')\n",
    "plt.legend()\n",
    "plt.xlabel('TRE (points)')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(df_results['MSE before image'], bins=20, color='blue', alpha=alpha_value, label='before')\n",
    "plt.hist((df_results['MSE 1 to 2 image']+df_results['MSE 2 to 1 image'])/2, bins=20, color='red', alpha=alpha_value, label='non-RANSAC')\n",
    "plt.legend()\n",
    "plt.xlabel('MSE (images)')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(df_results['SSIM before'], bins=20, color='blue', alpha=alpha_value, label='before')\n",
    "plt.hist((df_results['SSIM 1 to 2 image']+df_results['SSIM 2 to 1 image'])/2, bins=20, color='red', alpha=alpha_value, label='non-RANSAC')\n",
    "plt.legend()\n",
    "plt.xlabel('SSIM (images)')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
