{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Suppress the specific warning\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from utils.utils0 import *\n",
    "from utils.utils1 import *\n",
    "from utils.utils1 import ModelParams, DL_affine_plot, loss_extra\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3: # pragma: no cover\n",
    "  print('Warning: OpenCV 3 is not installed')\n",
    "\n",
    "image_size = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cases, model parameters\n",
    "- Supervised DL w/ groundtruth affine transformation parameters (MSE params, MSE, NCC images)\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Unsupervised DL (MSE, NCC images)\n",
    "    - Actual eye data\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Data\n",
    "    - only images\n",
    "    - only heatmaps\n",
    "    - images & heatmaps\n",
    "- Loss function\n",
    "    - MSE affine parameters\n",
    "    - MSE, NCC images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  dataset3_sup0_image1_heatmaps0_loss_image1\n",
      "Model code:  30101_0.001_0_2_1\n",
      "Model params:  {'dataset': 3, 'sup': 0, 'image': 1, 'heatmaps': 0, 'loss_image_case': 1, 'loss_image': NCC(), 'loss_affine': None, 'learning_rate': 0.001, 'decay_rate': 0.96, 'start_epoch': 0, 'num_epochs': 2, 'batch_size': 1, 'model_name': 'dataset3_sup0_image1_heatmaps0_loss_image1'}\n",
      "\n",
      "Model name:  dataset3_sup0_image1_heatmaps0_loss_image1\n",
      "Model code:  30101_0.001_0_2_1\n",
      "Dataset used:  Synthetic shape\n",
      "Supervised or unsupervised model:  Unsupervised\n",
      "Image type:  Image used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  1\n",
      "Loss function for image:  NCC()\n",
      "Loss function for affine:  None\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  2\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_params = ModelParams(sup=0, dataset=3, image=1, heatmaps=0, \n",
    "                           loss_image=1, num_epochs=2)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## SuperPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImgReg Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SPaffineNet import SP_AffineNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP ImgReg model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datagen import datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: 0, is_train: True, sup: False\n",
      "Training eye dataset\n",
      "Number of training data:  500\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 0, is_train: True, sup: True\n",
      "Training eye dataset\n",
      "Number of training data:  500\n",
      "skipping\n",
      "\n",
      "\n",
      "dataset: 0, is_train: False, sup: False\n",
      "Test eye dataset\n",
      "Number of testing data:  100\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 0, is_train: False, sup: True\n",
      "Test eye dataset\n",
      "Number of testing data:  100\n",
      "skipping\n",
      "\n",
      "\n",
      "dataset: 1, is_train: True, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 1, is_train: True, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "index, source_img.shape,       target_img.shape,            affine_params.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "3 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "4 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "5 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "\n",
      "\n",
      "dataset: 1, is_train: False, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 1, is_train: False, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "index, source_img.shape,       target_img.shape,            affine_params.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "3 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "4 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "5 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "\n",
      "\n",
      "dataset: 2, is_train: True, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 2, is_train: True, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "index, source_img.shape,       target_img.shape,            affine_params.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "3 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "4 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "5 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "\n",
      "\n",
      "dataset: 2, is_train: False, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "\n",
      "\n",
      "dataset: 2, is_train: False, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "index, source_img.shape,       target_img.shape,            affine_params.shape\n",
      "0 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "1 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "2 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "3 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "4 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "5 torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256]) torch.Size([1, 2, 3])\n",
      "\n",
      "\n",
      "dataset: 3, is_train: True, sup: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "1 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "\n",
      "\n",
      "dataset: 3, is_train: True, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "1 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "\n",
      "\n",
      "dataset: 3, is_train: False, sup: False\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "1 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "\n",
      "\n",
      "dataset: 3, is_train: False, sup: True\n",
      "index, source_img.shape,       target_img.shape\n",
      "0 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "1 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test datagen for all datasets and training and testing\n",
    "for dataset in range(4): # don't forget to change this back to 2\n",
    "    for is_train in [True, False]:\n",
    "        for sup in [False, True]:\n",
    "            print(f'dataset: {dataset}, is_train: {is_train}, sup: {sup}')\n",
    "            dataloader = datagen(dataset, is_train, sup)\n",
    "            \n",
    "            if sup==1 and dataset==0:\n",
    "                print('skipping')\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    print('index, source_img.shape,       target_img.shape')\n",
    "                    for i, (source_img, target_img) in enumerate(dataloader):\n",
    "                        print(i, source_img.shape, target_img.shape)\n",
    "                        if i == 2:\n",
    "                            break\n",
    "                except ValueError:\n",
    "                    print('index, source_img.shape,       target_img.shape,            affine_params.shape')\n",
    "                    for i, batch in enumerate(dataloader):\n",
    "                        print(i, batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "                        if i == 5:\n",
    "                            break\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  [torch.Size([1, 1, 28, 28]), torch.Size([1])]\n",
      "Test set:  [torch.Size([1, 1, 28, 28]), torch.Size([1])]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datagen(model_params.dataset, True, model_params.sup)\n",
    "test_dataset = datagen(model_params.dataset, False, model_params.sup)\n",
    "\n",
    "# Get sample batch\n",
    "print('Train set: ', [x.shape for x in next(iter(train_dataset))])\n",
    "print('Test set: ', [x.shape for x in next(iter(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset3_sup0_image1_heatmaps0_loss_image1\n",
      "\n",
      "Model name:  dataset3_sup0_image1_heatmaps0_loss_image1\n",
      "Model code:  30101_0.001_0_2_1\n",
      "Dataset used:  Synthetic shape\n",
      "Supervised or unsupervised model:  Unsupervised\n",
      "Image type:  Image used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  1\n",
      "Loss function for image:  NCC()\n",
      "Loss function for affine:  None\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  2\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print case\n",
    "print(model_params)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running new version (not run SP on source image)\n",
      "SP_AffineNet(\n",
      "  (affineNet): AffineNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv1s): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv2s): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv3s): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv4s): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (fc4): Linear(in_features=32, out_features=6, bias=True)\n",
      "    (aPooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (ReLU): LeakyReLU(negative_slope=0.01)\n",
      "    (Act1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "    (Act2): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "    (Act3): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "    (Act4): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "    (Act5): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "  )\n",
      ")\n",
      "Loaded model from trained_models/10102_0.001_0_20_1_20230930-091532.pth\n",
      "starting at epoch 20\n"
     ]
    }
   ],
   "source": [
    "model = SP_AffineNet(model_params).to(device)\n",
    "print(model)\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = optim.Adam(parameters, model_params.learning_rate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "model_path = 'trained_models/10102_0.001_0_20_1_20230930-091532.pth'\n",
    "\n",
    "# if a model is loaded, the training will continue from the epoch it was saved at\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model_params.start_epoch = int(model_path.split('/')[-1].split('_')[3])\n",
    "    print(f'Loaded model from {model_path}\\nstarting at epoch {model_params.start_epoch}')\n",
    "    if model_params.start_epoch >= model_params.num_epochs:\n",
    "            model_params.num_epochs += model_params.start_epoch\n",
    "except:\n",
    "    model_params.start_epoch = 0\n",
    "    print('No model loaded, starting from scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 1, 28, 28]) torch.Size([1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataset):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mprint\u001b[39m(i, data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dataset):\n",
    "    if i == 0:\n",
    "        print(i, data[0].shape, data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train(model, model_params, timestamp):\n",
    "    # Define loss function based on supervised or unsupervised learning\n",
    "    criterion = model_params.loss_image\n",
    "    # extra = loss_extra()\n",
    "\n",
    "    if model_params.sup:\n",
    "        criterion_affine = nn.MSELoss()\n",
    "        # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model_params.learning_rate)\n",
    "\n",
    "    # Create empty list to store epoch number, train loss and validation loss\n",
    "    epoch_loss_list = []\n",
    "    running_loss_list = []\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(model_params.start_epoch, model_params.num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_dataset, desc=f'Training Epoch {epoch+1}/{model_params.num_epochs}')\n",
    "        for i, data in enumerate(train_bar):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "                affine_params_true = None\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #         print(i, outputs[i].shape)\n",
    "            # 0 torch.Size([1, 1, 256, 256])\n",
    "            # 1 torch.Size([1, 2, 3])\n",
    "            # 2 (2, 4)\n",
    "            # 3 (2, 4)\n",
    "            # 4 (1, 4, 2)\n",
    "            # 5 (256, 9)\n",
    "            # 6 (256, 16)\n",
    "            # 7 (256, 256)\n",
    "            # 8 (256, 256)\n",
    "            transformed_source_affine = outputs[0] # image\n",
    "            affine_params_predicted = outputs[1] # affine parameters\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "\n",
    "            # print(f\"affine_params_true: {affine_params_true}\")\n",
    "            # print(f\"affine_params_predicted: {affine_params_predicted}\\n\")\n",
    "\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            loss = criterion(transformed_source_affine, target_image)\n",
    "            # loss += extra(affine_params_predicted)\n",
    "            if model_params.sup:\n",
    "                loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                # loss_points = criterion_points(points1_affine, points2)\n",
    "\n",
    "                loss += loss_affine\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Plot images if i < 5\n",
    "            if i < 5:\n",
    "                DL_affine_plot(f\"epoch{epoch+1}_train\", output_dir,\n",
    "                    f\"{i}\", \"_\", source_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    target_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                    transformed_source_affine[0, 0, :, :].detach().cpu().numpy(),\n",
    "                    points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                    affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_loss_list.append([epoch+((i+1)/len(train_dataset)), loss.item()])\n",
    "            train_bar.set_postfix({'loss': running_loss / (i+1)})\n",
    "        print(f'Training Epoch {epoch+1}/{model_params.num_epochs} loss: {running_loss / len(train_dataset)}')\n",
    "\n",
    "        # Validate model\n",
    "        validation_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_dataset, 0):\n",
    "                # Get images and affine parameters\n",
    "                if model_params.sup:\n",
    "                    source_image, target_image, affine_params_true = data\n",
    "                else:\n",
    "                    source_image, target_image = data\n",
    "                    affine_params_true = None\n",
    "                source_image = source_image.to(device)\n",
    "                target_image = target_image.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(source_image, target_image)\n",
    "                # for i in range(len(outputs)):\n",
    "                #     print(i, outputs[i].shape)\n",
    "                transformed_source_affine = outputs[0]\n",
    "                affine_params_predicted = outputs[1]\n",
    "                points1 = outputs[2]\n",
    "                points2 = outputs[3]\n",
    "                points1_affine = np.array(outputs[4])\n",
    "                try:\n",
    "                    points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "                except:\n",
    "                    pass\n",
    "                desc1 = outputs[5]\n",
    "                desc2 = outputs[6]\n",
    "                heatmap1 = outputs[7]\n",
    "                heatmap2 = outputs[8]\n",
    "\n",
    "                loss = criterion(transformed_source_affine, target_image)\n",
    "                # loss += extra(affine_params_predicted)\n",
    "                if model_params.sup:\n",
    "                    loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                    # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                    # loss_points = criterion_points(points1_affine, points2)\n",
    "                    loss += loss_affine\n",
    "\n",
    "                # Add to validation loss\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # Plot images if i < 5\n",
    "                if i < 5:\n",
    "                    DL_affine_plot(f\"epoch{epoch+1}_valid\", output_dir,\n",
    "                        f\"{i}\", \"_\", source_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                        target_image[0, 0, :, :].detach().cpu().numpy(), \n",
    "                        transformed_source_affine[0, 0, :, :].detach().cpu().numpy(),\n",
    "                        points1, points2, points1_affine, desc1, desc2, affine_params_true=affine_params_true,\n",
    "                        affine_params_predict=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "\n",
    "        # Print validation statistics\n",
    "        validation_loss /= len(test_dataset)\n",
    "        print(f'Validation Epoch {epoch+1}/{model_params.num_epochs} loss: {validation_loss}')\n",
    "\n",
    "        # Append epoch number, train loss and validation loss to epoch_loss_list\n",
    "        epoch_loss_list.append([epoch+1, running_loss / len(train_dataset), validation_loss])\n",
    "\n",
    "        \n",
    "        # Extract epoch number, train loss and validation loss from epoch_loss_list\n",
    "        epoch = [x[0] for x in epoch_loss_list]\n",
    "        train_loss = [x[1] for x in epoch_loss_list]\n",
    "        val_loss = [x[2] for x in epoch_loss_list]\n",
    "        step = [x[0] for x in running_loss_list]\n",
    "        running_train_loss = [x[1] for x in running_loss_list]\n",
    "\n",
    "        save_plot_name = f\"{output_dir}/loss_{model_params.get_model_code()}_epoch{model_params.num_epochs}_{timestamp}.png\"\n",
    "\n",
    "        # Plot train loss and validation loss against epoch number\n",
    "        plt.plot(step, running_train_loss, label='Running Train Loss', alpha=0.3)\n",
    "        plt.plot(epoch, train_loss, label='Train Loss')\n",
    "        plt.plot(epoch, val_loss, label='Validation Loss')\n",
    "        plt.title('Train and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_plot_name)\n",
    "        \n",
    "    plt.show()\n",
    "    print('Finished Training')\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "    # Return epoch_loss_list\n",
    "    return epoch_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/22:   0%|          | 0/60000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m timestamp \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m loss_list \u001b[39m=\u001b[39m train(model, model_params, timestamp)\n",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m target_image \u001b[39m=\u001b[39m target_image\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(source_image, target_image)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# for i in range(len(outputs)):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#         print(i, outputs[i].shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# 0 torch.Size([1, 1, 256, 256])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# 7 (256, 256)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# 8 (256, 256)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m transformed_source_affine \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m] \u001b[39m# image\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/spppt-1/utils/SPaffineNet.py:30\u001b[0m, in \u001b[0;36mSP_AffineNet.forward\u001b[0;34m(self, source_image, target_image)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, source_image, target_image):\n\u001b[1;32m     24\u001b[0m     \u001b[39m# source_image = source_image.to(device)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[39m# target_image = target_image.to(device)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m     \u001b[39m# print('source_image: ', source_image.shape)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[39m# print('target_image: ', target_image.shape)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     points1, desc1, heatmap1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuperpoint(source_image[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, :, :]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m---> 30\u001b[0m     points2, desc2, heatmap2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuperpoint(target_image[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, :, :]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_params\u001b[39m.\u001b[39mheatmaps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     33\u001b[0m         affine_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffineNet(source_image, target_image)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "loss_list = train(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output:\n",
      "[21, 4918184.778309577, 44482042.89400922]\n",
      "[22, 397508112.97214675, 78786919.4470046]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training output:\")\n",
    "for i in range(len(loss_list)):\n",
    "    print(loss_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/11101_0.001_20_22_1_20231017-000009.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"trained_models/\"\n",
    "model_name_to_save = model_save_path + f\"{model_params.get_model_code()}_{timestamp}.pth\"\n",
    "print(model_name_to_save)\n",
    "torch.save(model.state_dict(), model_name_to_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model (loading and inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results and export metrics to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'model_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m SPmodel \u001b[39m=\u001b[39m SP_AffineNet()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m parameters \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mparameters()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'model_params'"
     ]
    }
   ],
   "source": [
    "# model = SPmodel = SP_AffineNet().to(device)\n",
    "# print(model)\n",
    "\n",
    "# parameters = model.parameters()\n",
    "# optimizer = optim.Adam(parameters, model_params.learning_rate)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "\n",
    "# model.load_state_dict(torch.load(model_name_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing::   0%|          | 0/217 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing::  66%|██████▌   | 143/217 [01:26<00:44,  1.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template.ipynb Cell 31\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m             os\u001b[39m.\u001b[39mremove(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, file))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m metrics \u001b[39m=\u001b[39m test(model, model_params, timestamp)\n",
      "\u001b[1;32m/home/pkhamchuai/codes/spppt-1/DL_template.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m target_image \u001b[39m=\u001b[39m target_image\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(source_image, target_image)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# for i in range(len(outputs)):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m#     print(i, outputs[i].shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/spppt-1/DL_template.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m transformed_source_affine \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/spppt-1/utils/SPaffineNet.py:78\u001b[0m, in \u001b[0;36mSP_AffineNet.forward\u001b[0;34m(self, source_image, target_image)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, source_image, target_image):\n\u001b[1;32m     73\u001b[0m     \u001b[39m# source_image = source_image.to(device)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[39m# target_image = target_image.to(device)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m     \u001b[39m# print('source_image: ', source_image.shape)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39m# print('target_image: ', target_image.shape)\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     points1, desc1, heatmap1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuperpoint(source_image[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, :, :]\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m     79\u001b[0m     points2, desc2, heatmap2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuperpoint(target_image[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, :, :]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     81\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_params\u001b[39m.\u001b[39mheatmaps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/codes/spppt-1/utils/SuperPoint.py:183\u001b[0m, in \u001b[0;36mSuperPointFrontend.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    181\u001b[0m   inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m    182\u001b[0m \u001b[39m# Forward pass of network.\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet\u001b[39m.\u001b[39;49mforward(inp)\n\u001b[1;32m    184\u001b[0m semi, coarse_desc \u001b[39m=\u001b[39m outs[\u001b[39m0\u001b[39m], outs[\u001b[39m1\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[39m# Convert pytorch -> numpy.\u001b[39;00m\n",
      "File \u001b[0;32m~/codes/spppt-1/utils/SuperPoint.py:55\u001b[0m, in \u001b[0;36mSuperPointNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1a(x))\n\u001b[1;32m     54\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1b(x))\n\u001b[0;32m---> 55\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool(x)\n\u001b[1;32m     56\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2a(x))\n\u001b[1;32m     57\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2b(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/spppt/lib/python3.8/site-packages/torch/nn/modules/module.py:1495\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1495\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m             \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m             \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test(model, model_params, timestamp):\n",
    "    # Set model to training mode\n",
    "    model.eval()\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}_test\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Validate model\n",
    "    # validation_loss = 0.0\n",
    "\n",
    "    # create a csv file to store the metrics\n",
    "    csv_file = f\"{output_dir}/metrics.csv\"\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # matches1_transformed.shape[-1], mse_before, mse12, tre_before, tre12, \\\n",
    "        # mse12_image, ssim12_image, \n",
    "        writer.writerow([\"index\", \"mse_before\", \"mse12\", \"tre_before\", \"tre12\", \"mse12_image_before\", \"mse12_image\", \"ssim12_image_before\", \"ssim12_image\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        testbar = tqdm(test_dataset, desc=f'Testing:')\n",
    "        for i, data in enumerate(testbar, 0):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #     print(i, outputs[i].shape)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            if i < 50:\n",
    "                plot_ = True\n",
    "            else:\n",
    "                plot_ = False\n",
    "\n",
    "            results = DL_affine_plot(f\"{i+1}\", output_dir,\n",
    "                f\"{i}\", \"_\", source_image[0, 0, :, :].cpu().numpy(), target_image[0, 0, :, :].cpu().numpy(), \\\n",
    "                transformed_source_affine[0, 0, :, :].cpu().numpy(), \\\n",
    "                points1, points2, points1_affine, desc1, desc2, \\\n",
    "                    affine_params=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=plot_)\n",
    "\n",
    "            # calculate metrics\n",
    "            # matches1_transformed = results[0]\n",
    "            mse_before = results[1]\n",
    "            mse12 = results[2]\n",
    "            tre_before = results[3]\n",
    "            tre12 = results[4]\n",
    "            mse12_image_before = results[5]\n",
    "            mse12_image = results[6]\n",
    "            ssim12_image_before = results[7]\n",
    "            ssim12_image = results[8]\n",
    "\n",
    "            # write metrics to csv file\n",
    "            with open(csv_file, 'a', newline='') as file:\n",
    "                writer = csv.writer(file) # TODO: might need to export true & predicted affine parameters too\n",
    "                writer.writerow([i, mse_before, mse12, tre_before, tre12, mse12_image_before, mse12_image, ssim12_image_before, ssim12_image])\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "metrics = test(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spppt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
