{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import FundamentalMatrixTransform, AffineTransform\n",
    "\n",
    "# Suppress the specific warning\n",
    "import warnings\n",
    "import csv\n",
    "import sys\n",
    "from IPython.utils.capture import capture_output\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from utils.utils0 import *\n",
    "from utils.utils0 import transform_to_displacement_field\n",
    "from utils.utils1 import *\n",
    "from utils.utils1 import ModelParams, DL_affine_plot, loss_extra\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3: # pragma: no cover\n",
    "  print('Warning: OpenCV 3 is not installed')\n",
    "\n",
    "image_size = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cases, model parameters\n",
    "- Supervised DL w/ groundtruth affine transformation parameters (MSE params, MSE, NCC images)\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Unsupervised DL (MSE, NCC images)\n",
    "    - Actual eye data\n",
    "    - Synthetic eye\n",
    "    - Synthetic shape\n",
    "- Data\n",
    "    - only images\n",
    "    - only heatmaps\n",
    "    - images & heatmaps\n",
    "- Loss function\n",
    "    - MSE affine parameters\n",
    "    - MSE, NCC images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  dataset1_sup1_image1_heatmaps0_loss_image2\n",
      "Model code:  11102_0.001_0_1_1\n",
      "Model params:  {'dataset': 1, 'sup': 1, 'image': 1, 'heatmaps': 0, 'loss_image_case': 2, 'loss_image': <utils.utils1.MSE_SSIM object at 0x7fc9e87118b0>, 'loss_affine': <utils.utils1.loss_affine object at 0x7fc9e87116a0>, 'learning_rate': 0.001, 'decay_rate': 0.96, 'start_epoch': 0, 'num_epochs': 1, 'batch_size': 1, 'model_name': 'dataset1_sup1_image1_heatmaps0_loss_image2'}\n",
      "\n",
      "Model name:  dataset1_sup1_image1_heatmaps0_loss_image2\n",
      "Model code:  11102_0.001_0_1_1\n",
      "Dataset used:  Synthetic eye\n",
      "Supervised or unsupervised model:  Supervised\n",
      "Image type:  Image used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  2\n",
      "Loss function for image:  <utils.utils1.MSE_SSIM object at 0x7fc9e87118b0>\n",
      "Loss function for affine:  <utils.utils1.loss_affine object at 0x7fc9e87116a0>\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  1\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_params = ModelParams(sup=1, dataset=1, image=1, heatmaps=0, \n",
    "                           loss_image=2, num_epochs=1)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## SuperPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SuperPoint import SuperPointFrontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImgReg Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SPaffineNet import AffineNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP ImgReg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class SP_AffineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SP_AffineNet, self).__init__()\n",
    "        self.superpoint = SuperPointFrontend('utils/superpoint_v1.pth', nms_dist=4,\n",
    "                          conf_thresh=0.015, nn_thresh=0.7, cuda=True)\n",
    "        self.affineNet = AffineNet()\n",
    "        self.nn_thresh = 0.7\n",
    "\n",
    "    def forward(self, source_image, target_image):\n",
    "        # source_image = source_image.to(device)\n",
    "        # target_image = target_image.to(device)\n",
    "\n",
    "        # print('source_image: ', source_image.shape)\n",
    "        # print('target_image: ', target_image.shape)\n",
    "        points1, desc1, heatmap1 = self.superpoint(source_image[0, 0, :, :].cpu().numpy())\n",
    "        points2, desc2, heatmap2 = self.superpoint(target_image[0, 0, :, :].cpu().numpy())\n",
    "\n",
    "        if model_params.heatmaps == 0:\n",
    "            affine_params = self.affineNet(source_image, target_image)\n",
    "        elif model_params.heatmaps == 1:\n",
    "            affine_params = self.affineNet(source_image, target_image, heatmap1, heatmap2)\n",
    "\n",
    "        transformed_source_affine = tensor_affine_transform(source_image, affine_params)\n",
    "\n",
    "        # match the points between the two images\n",
    "        tracker = PointTracker(5, nn_thresh=0.7)\n",
    "        try:\n",
    "            matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=self.nn_thresh)\n",
    "        except:\n",
    "            print('No matches found')\n",
    "            # TODO: find a better way to do this\n",
    "            try:\n",
    "                while matches.shape[1] < 3 and self.nn_thresh > 0.1:\n",
    "                    self.nn_thresh = self.nn_thresh - 0.1\n",
    "                    matches = tracker.nn_match_two_way(desc1, desc2, nn_thresh=self.nn_thresh)\n",
    "            except:\n",
    "                return transformed_source_affine, affine_params, [], [], [], [], [], [], []\n",
    "\n",
    "        # take the elements from points1 and points2 using the matches as indices\n",
    "        matches1 = points1[:2, matches[0, :].astype(int)]\n",
    "        matches2 = points2[:2, matches[1, :].astype(int)]\n",
    "\n",
    "        # transform the points using the affine parameters\n",
    "        displacement_field = torch.zeros(source_image.size(0), source_image.size(1)).to(device)\n",
    "        DVF_affine = transform_to_displacement_field(\n",
    "                    displacement_field.view(1, 1, displacement_field.size(0), displacement_field.size(1)),\n",
    "                    affine_params, device=device)\n",
    "\n",
    "        matches1_transformed = ops.landmark_transformer2(matches1, DVF_affine.cpu())\n",
    "\n",
    "        return transformed_source_affine, affine_params, matches1, matches2, matches1_transformed, desc1, desc2, heatmap1, heatmap2\n",
    "\n",
    "        # return transformed_source_affine, affine_params\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datagen import datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  [torch.Size([1, 1, 256, 256]), torch.Size([1, 1, 256, 256]), torch.Size([1, 6])]\n",
      "Test set:  [torch.Size([1, 1, 256, 256]), torch.Size([1, 1, 256, 256]), torch.Size([1, 6])]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datagen(model_params.dataset, True, model_params.sup)\n",
    "test_dataset = datagen(model_params.dataset, False, model_params.sup)\n",
    "\n",
    "# Get sample batch\n",
    "print('Train set: ', [x.shape for x in next(iter(train_dataset))])\n",
    "print('Test set: ', [x.shape for x in next(iter(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1_sup1_image1_heatmaps0_loss_image2\n",
      "\n",
      "Model name:  dataset1_sup1_image1_heatmaps0_loss_image2\n",
      "Model code:  11102_0.001_0_1_1\n",
      "Dataset used:  Synthetic eye\n",
      "Supervised or unsupervised model:  Supervised\n",
      "Image type:  Image used\n",
      "Heatmaps used:  Heatmaps not used\n",
      "Loss function case:  2\n",
      "Loss function for image:  <utils.utils1.MSE_SSIM object at 0x7fc9e87118b0>\n",
      "Loss function for affine:  <utils.utils1.loss_affine object at 0x7fc9e87116a0>\n",
      "Learning rate:  0.001\n",
      "Decay rate:  0.96\n",
      "Start epoch:  0\n",
      "Number of epochs:  1\n",
      "Batch size:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print case\n",
    "print(model_params)\n",
    "model_params.print_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffineNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1s): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv2s): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv3s): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv4s): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=6, bias=True)\n",
      "  (aPooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (ReLU): LeakyReLU(negative_slope=0.01)\n",
      "  (Act1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (Act2): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "  (Act3): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "  (Act4): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "  (Act5): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      ")\n",
      "No model loaded, starting from scratch\n"
     ]
    }
   ],
   "source": [
    "model = AffineNet().to(device)\n",
    "print(model)\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = optim.Adam(parameters, model_params.learning_rate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "#model_path = 'trained_models/01102_0.001_20_1_20230930-091532.pth'\n",
    "\n",
    "# if a model is loaded, the training will continue from the epoch it was saved at\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model_params.start_epoch = int(model_path.split('/')[-1].split('_')[2])\n",
    "    print(f'Loaded model from {model_path}\\nstarting at epoch {model_params.start_epoch}')\n",
    "except:\n",
    "    model_params.start_epoch = 0\n",
    "    print('No model loaded, starting from scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train(model, model_params, timestamp):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Define loss function based on supervised or unsupervised learning\n",
    "    criterion = model_params.loss_image\n",
    "    extra = loss_extra()\n",
    "\n",
    "    if model_params.sup:\n",
    "        criterion_affine = nn.MSELoss()\n",
    "        # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model_params.learning_rate)\n",
    "\n",
    "    # Create empty list to store epoch number, train loss and validation loss\n",
    "    epoch_loss_list = []\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(model_params.start_epoch, model_params.num_epochs):\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_dataset, desc=f'Training Epoch {epoch+1}/{model_params.num_epochs}')\n",
    "        for i, data in enumerate(train_bar):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #         print(i, outputs[i].shape)\n",
    "            # 0 torch.Size([1, 1, 256, 256])\n",
    "            # 1 torch.Size([1, 2, 3])\n",
    "            # 2 (2, 4)\n",
    "            # 3 (2, 4)\n",
    "            # 4 (1, 4, 2)\n",
    "            # 5 (256, 9)\n",
    "            # 6 (256, 16)\n",
    "            # 7 (256, 256)\n",
    "            # 8 (256, 256)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            loss = criterion(transformed_source_affine, target_image)\n",
    "            loss += extra(affine_params_predicted)\n",
    "            if model_params.sup:\n",
    "                loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                # loss_points = criterion_points(points1_affine, points2)\n",
    "\n",
    "                loss += loss_affine\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            train_bar.set_postfix({'loss': running_loss / (i+1)})\n",
    "        print(f'Training Epoch {epoch+1}/{model_params.num_epochs} loss: {running_loss / len(train_dataset)}')\n",
    "\n",
    "        # Validate model\n",
    "        validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_dataset, 0):\n",
    "                # Get images and affine parameters\n",
    "                if model_params.sup:\n",
    "                    source_image, target_image, affine_params_true = data\n",
    "                else:\n",
    "                    source_image, target_image = data\n",
    "                source_image = source_image.to(device)\n",
    "                target_image = target_image.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(source_image, target_image)\n",
    "                # for i in range(len(outputs)):\n",
    "                #     print(i, outputs[i].shape)\n",
    "                transformed_source_affine = outputs[0]\n",
    "                affine_params_predicted = outputs[1]\n",
    "                points1 = outputs[2]\n",
    "                points2 = outputs[3]\n",
    "                points1_affine = np.array(outputs[4])\n",
    "                try:\n",
    "                    points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "                except:\n",
    "                    pass\n",
    "                desc1 = outputs[5]\n",
    "                desc2 = outputs[6]\n",
    "                heatmap1 = outputs[7]\n",
    "                heatmap2 = outputs[8]\n",
    "\n",
    "                loss = criterion(transformed_source_affine, target_image)\n",
    "                loss += extra(affine_params_predicted)\n",
    "                if model_params.sup:\n",
    "                    loss_affine = criterion_affine(affine_params_true.view(1, 2, 3), affine_params_predicted.cpu())\n",
    "                    # TODO: add loss for points1_affine and points2, Euclidean distance\n",
    "                    # loss_points = criterion_points(points1_affine, points2)\n",
    "\n",
    "                    loss += loss_affine\n",
    "\n",
    "                # Add to validation loss\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # Plot images if i < 5\n",
    "                \n",
    "                if i < 5:\n",
    "                    output_file = f\"{output_dir}/validation_epoch{epoch+1}.txt\"\n",
    "                    try: # need to do this because the warning cannot be suppressed, fix clipping makes the image all black\n",
    "                        # Open the file in append mode to save the print statements\n",
    "                        with open(output_file, 'a') as file:\n",
    "                            # Redirect sys.stdout to the file\n",
    "                            sys.stdout = file\n",
    "\n",
    "                            # Redirect warnings to the file\n",
    "                            # warnings.filterwarnings('always')  # Capture all warnings\n",
    "                            warnings_file = open(output_file, 'a')\n",
    "                            warnings.showwarning = lambda message, category, filename, lineno, file=warnings_file, line=None: file.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
    "                            \n",
    "                            # Plot images\n",
    "                            with capture_output():\n",
    "                                DL_affine_plot(f\"epoch{epoch+1}\", output_dir,\n",
    "                                    f\"{i}\", \"_\", source_image[0, 0, :, :].cpu().numpy(), target_image[0, 0, :, :].cpu().numpy(), transformed_source_affine[0, 0, :, :].cpu().numpy(),\n",
    "                                    points1, points2, points1_affine, desc1, desc2, \\\n",
    "                                        affine_params=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=True)\n",
    "                                \n",
    "                            # Reset sys.stdout to the console\n",
    "                            sys.stdout = sys.__stdout__\n",
    "                            warnings_file.close()\n",
    "                    except Exception as e:\n",
    "                        # Handle exceptions, if any\n",
    "                        print(f\"An error occurred: {e}\")\n",
    "\n",
    "        # Print validation statistics\n",
    "        validation_loss /= len(test_dataset)\n",
    "        print(f'Validation Epoch {epoch+1}/{model_params.num_epochs} loss: {validation_loss}')\n",
    "\n",
    "        # Append epoch number, train loss and validation loss to epoch_loss_list\n",
    "        epoch_loss_list.append([epoch+1, running_loss / len(train_dataset), validation_loss])\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "    # Extract epoch number, train loss and validation loss from epoch_loss_list\n",
    "    epoch = [x[0] for x in epoch_loss_list]\n",
    "    train_loss = [x[1] for x in epoch_loss_list]\n",
    "    val_loss = [x[2] for x in epoch_loss_list]\n",
    "\n",
    "    save_plot_name = f\"{output_dir}/loss_{model_params.get_model_code()}_epoch{model_params.num_epochs}_{timestamp}.png\"\n",
    "\n",
    "    # Plot train loss and validation loss against epoch number\n",
    "    plt.plot(epoch, train_loss, label='Train Loss')\n",
    "    plt.plot(epoch, val_loss, label='Validation Loss')\n",
    "    plt.title('Train and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(save_plot_name)\n",
    "    plt.show()\n",
    "\n",
    "    # Return epoch_loss_list\n",
    "    return epoch_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/1:   0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/1: 100%|██████████| 920/920 [02:29<00:00,  6.15it/s, loss=46.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/1 loss: 46.4614184529561\n",
      "Validation Epoch 1/1 loss: 46.56960289242081\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "loss_list = train(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training output:\")\n",
    "for i in range(len(loss_list)):\n",
    "    print(loss_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"trained_models/\"\n",
    "model_name_to_save = model_save_path + f\"{model_params.get_model_code()}_{timestamp}.pth\"\n",
    "print(model_name_to_save)\n",
    "torch.save(model.state_dict(), model_name_to_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model (loading and inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results and export metrics to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output:\n",
      "[1, 46.4614184529561, 46.56960289242081]\n",
      "trained_models/11102_0.001_0_1_1_20231002-111944.pth\n",
      "SP_AffineNet(\n",
      "  (affineNet): AffineNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv1s): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv2s): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv3s): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv4s): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (fc4): Linear(in_features=32, out_features=6, bias=True)\n",
      "    (aPooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (ReLU): LeakyReLU(negative_slope=0.01)\n",
      "    (Act1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "    (Act2): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "    (Act3): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "    (Act4): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "    (Act5): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SPmodel = SP_AffineNet().to(device)\n",
    "print(model)\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = optim.Adam(parameters, model_params.learning_rate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: model_params.decay_rate ** epoch)\n",
    "\n",
    "model.load_state_dict(torch.load(model_name_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing::  67%|██████▋   | 146/217 [05:48<02:49,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "def test(model, model_params, timestamp):\n",
    "    # Set model to training mode\n",
    "    model.eval()\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = f\"output/{model_params.get_model_code()}_{timestamp}_test\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Validate model\n",
    "    # validation_loss = 0.0\n",
    "\n",
    "    # create a csv file to store the metrics\n",
    "    csv_file = f\"{output_dir}/metrics.csv\"\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # matches1_transformed.shape[-1], mse_before, mse12, tre_before, tre12, \\\n",
    "        # mse12_image, ssim12_image, \n",
    "        writer.writerow([\"index\", \"mse_before\", \"mse12\", \"tre_before\", \"tre12\", \"mse12_image_before\", \"mse12_image\", \"ssim12_image_before\", \"ssim12_image\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        testbar = tqdm(test_dataset, desc=f'Testing:')\n",
    "        for i, data in enumerate(testbar, 0):\n",
    "            # Get images and affine parameters\n",
    "            if model_params.sup:\n",
    "                source_image, target_image, affine_params_true = data\n",
    "            else:\n",
    "                source_image, target_image = data\n",
    "            source_image = source_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(source_image, target_image)\n",
    "            # for i in range(len(outputs)):\n",
    "            #     print(i, outputs[i].shape)\n",
    "            transformed_source_affine = outputs[0]\n",
    "            affine_params_predicted = outputs[1]\n",
    "            points1 = outputs[2]\n",
    "            points2 = outputs[3]\n",
    "            points1_affine = np.array(outputs[4])\n",
    "            try:\n",
    "                points1_affine = points1_affine.reshape(points1_affine.shape[2], points1_affine.shape[1])\n",
    "            except:\n",
    "                pass\n",
    "            desc1 = outputs[5]\n",
    "            desc2 = outputs[6]\n",
    "            heatmap1 = outputs[7]\n",
    "            heatmap2 = outputs[8]\n",
    "\n",
    "            output_file = f\"{output_dir}/validation.txt\"\n",
    "            try: # need to do this because the warning cannot be suppressed, fix clipping makes the image all black\n",
    "                # Open the file in append mode to save the print statements\n",
    "                with open(output_file, 'a') as file:\n",
    "                    # Redirect sys.stdout to the file\n",
    "                    sys.stdout = file\n",
    "\n",
    "                    # Redirect warnings to the file\n",
    "                    # warnings.filterwarnings('always')  # Capture all warnings\n",
    "                    warnings_file = open(output_file, 'a')\n",
    "                    warnings.showwarning = lambda message, category, filename, lineno, file=warnings_file, line=None: \\\n",
    "                        file.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
    "\n",
    "                    # Plot images\n",
    "                    with capture_output():\n",
    "                        if i < 50:\n",
    "                            plot_ = True\n",
    "                        else:\n",
    "                            plot_ = False\n",
    "\n",
    "                        results = DL_affine_plot(f\"{i+1}\", output_dir,\n",
    "                            f\"{i}\", \"_\", source_image[0, 0, :, :].cpu().numpy(), target_image[0, 0, :, :].cpu().numpy(), \\\n",
    "                            transformed_source_affine[0, 0, :, :].cpu().numpy(), \\\n",
    "                            points1, points2, points1_affine, desc1, desc2, \\\n",
    "                                affine_params=affine_params_predicted, heatmap1=heatmap1, heatmap2=heatmap2, plot=plot_)\n",
    "\n",
    "                    # Reset sys.stdout to the console\n",
    "                    sys.stdout = sys.__stdout__\n",
    "                    warnings_file.close()\n",
    "            except Exception as e:\n",
    "                # Handle exceptions, if any\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "            # calculate metrics\n",
    "            # matches1_transformed = results[0]\n",
    "            mse_before = results[1]\n",
    "            mse12 = results[2]\n",
    "            tre_before = results[3]\n",
    "            tre12 = results[4]\n",
    "            mse12_image_before = results[5]\n",
    "            mse12_image = results[6]\n",
    "            ssim12_image_before = results[7]\n",
    "            ssim12_image = results[8]\n",
    "\n",
    "            # write metrics to csv file\n",
    "            with open(csv_file, 'a', newline='') as file:\n",
    "                writer = csv.writer(file) # TODO: might need to export true & predicted affine parameters too\n",
    "                writer.writerow([i, mse_before, mse12, tre_before, tre12, mse12_image_before, mse12_image, ssim12_image_before, ssim12_image])\n",
    "\n",
    "    # delete all txt files in output_dir\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "metrics = test(model, model_params, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spppt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
